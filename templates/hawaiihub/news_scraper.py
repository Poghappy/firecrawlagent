#!/usr/bin/env python3
"""HawaiiHub æ–°é—»çˆ¬å–æ¨¡æ¿

åŠŸèƒ½: çˆ¬å–å¤å¨å¤·æœ¬åœ°æ–°é—»å¹¶ä¿å­˜åˆ°æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python news_scraper.py
"""

import json
import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List

from dotenv import load_dotenv
from firecrawl import Firecrawl


# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/hawaiihub_scraper.log'),
        logging.StreamHandler()
    ]
)

# åˆå§‹åŒ– Firecrawl å®¢æˆ·ç«¯
firecrawl = Firecrawl(api_key=os.getenv("FIRECRAWL_API_KEY"))

# åŠ è½½æ•°æ®æºé…ç½®
with open("config/hawaiihub_sources.json", "r", encoding="utf-8") as f:
    SOURCES_CONFIG = json.load(f)

def scrape_news_source(source_id: str, config: Dict) -> List[Dict]:
    """çˆ¬å–å•ä¸ªæ–°é—»æº"""
    logging.info(f"å¼€å§‹çˆ¬å–: {config['name']}")

    articles = []

    try:
        # 1. ä½¿ç”¨ Map å‘ç°æ–‡ç« é“¾æ¥
        urls = firecrawl.map(
            url=config["url"],
            search="news article",
            limit=50
        )

        logging.info(f"  å‘ç° {len(urls.links)} ä¸ªé“¾æ¥")

        # 2. æ‰¹é‡çˆ¬å–æ–‡ç« ï¼ˆå–å‰20ä¸ªï¼‰
        if urls.links:
            result = firecrawl.batch_scrape(
                urls=urls.links[:20],
                formats=["markdown"],
                poll_interval=2
            )

            # 3. å¤„ç†ç»“æœ
            for doc in result.data:
                if not doc.error:
                    article = {
                        "source_id": source_id,
                        "source_name": config["name"],
                        "url": doc.url,
                        "title": doc.metadata.title if doc.metadata else "",
                        "content": doc.markdown,
                        "scraped_at": datetime.now().isoformat()
                    }
                    articles.append(article)

            logging.info(f"  æˆåŠŸçˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")

    except Exception as e:
        logging.error(f"  çˆ¬å–å¤±è´¥: {e}")

    return articles

def save_articles(articles: List[Dict]) -> None:
    """ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶"""
    if not articles:
        logging.warning("æ²¡æœ‰æ–‡ç« éœ€è¦ä¿å­˜")
        return

    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = Path("data/hawaiihub/news")
    data_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä¿å­˜ JSON
    json_file = data_dir / f"news_{timestamp}.json"
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

    logging.info(f"âœ… ä¿å­˜åˆ°: {json_file}")
    logging.info(f"   æ€»è®¡: {len(articles)} ç¯‡æ–‡ç« ")

def main():
    """ä¸»å‡½æ•°"""
    logging.info("ğŸ”¥ å¼€å§‹ HawaiiHub æ–°é—»é‡‡é›†")

    all_articles = []

    # éå†æ‰€æœ‰å¯ç”¨çš„æ–°é—»æº
    for source_id, config in SOURCES_CONFIG["news_sources"].items():
        if config.get("enabled", False):
            articles = scrape_news_source(source_id, config)
            all_articles.extend(articles)

    # ä¿å­˜ç»“æœ
    save_articles(all_articles)

    logging.info("âœ… æ–°é—»é‡‡é›†å®Œæˆ")

if __name__ == "__main__":
    main()
