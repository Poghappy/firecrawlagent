#!/usr/bin/env python3
"""HawaiiHub æ–°é—»é‡‡é›†ç³»ç»Ÿ - æ ¸å¿ƒé‡‡é›†å™¨

å®ç°æ–°é—»é‡‡é›†çš„æ ¸å¿ƒé€»è¾‘ï¼ŒåŒ…æ‹¬é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶å’Œæˆæœ¬æ§åˆ¶
"""

import logging
import time
from datetime import datetime
from typing import Dict, List, Optional

from firecrawl import FirecrawlApp
from firecrawl.exceptions import (
    AuthenticationError,
    RateLimitError,
    RequestTimeoutError,
)

from config import (
    DAILY_BUDGET,
    FIRECRAWL_API_KEY,
    FIRECRAWL_API_KEYS_BACKUP,
    SCRAPE_STRATEGY,
    get_enabled_sources,
)


# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class HawaiiHubNewsScraper:
    """HawaiiHub æ–°é—»é‡‡é›†å™¨"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        backup_keys: Optional[List[str]] = None,
        daily_budget: float = DAILY_BUDGET,
    ):
        """åˆå§‹åŒ–é‡‡é›†å™¨

        Args:
            api_key: Firecrawl API å¯†é’¥ï¼ˆå¯é€‰ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            backup_keys: å¤‡ç”¨ API å¯†é’¥åˆ—è¡¨
            daily_budget: æ¯æ—¥é¢„ç®—ï¼ˆç¾å…ƒï¼‰
        """
        self.api_key = api_key or FIRECRAWL_API_KEY
        self.backup_keys = backup_keys or [k for k in FIRECRAWL_API_KEYS_BACKUP if k]
        self.daily_budget = daily_budget

        # åˆå§‹åŒ– Firecrawl å®¢æˆ·ç«¯
        self.app = FirecrawlApp(api_key=self.api_key)

        # ç»Ÿè®¡ä¿¡æ¯
        self.request_count = 0
        self.success_count = 0
        self.failure_count = 0
        self.total_cost = 0.0
        self.cost_per_request = 0.01  # å‡è®¾æ¯æ¬¡è¯·æ±‚ $0.01

        logger.info("HawaiiHub æ–°é—»é‡‡é›†å™¨å·²åˆå§‹åŒ–")
        logger.info(f"æ¯æ—¥é¢„ç®—: ${self.daily_budget}")
        logger.info(f"å¤‡ç”¨å¯†é’¥æ•°: {len(self.backup_keys)}")

    def scrape_url(
        self, url: str, config: Optional[Dict] = None, max_retries: Optional[int] = None
    ) -> Optional[Dict]:
        """é‡‡é›†å•ä¸ª URL

        Args:
            url: è¦é‡‡é›†çš„ URL
            config: Scrape é…ç½®ï¼ˆå¯é€‰ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆå¯é€‰ï¼‰

        Returns:
            é‡‡é›†ç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å› None
        """
        max_retries = max_retries or SCRAPE_STRATEGY["max_retries"]
        config = config or {}

        for attempt in range(max_retries):
            try:
                # æ£€æŸ¥é¢„ç®—
                if self.total_cost >= self.daily_budget:
                    logger.error(f"è¶…å‡ºæ¯æ—¥é¢„ç®—: ${self.daily_budget}")
                    return None

                # æ‰§è¡Œé‡‡é›†
                logger.debug(f"é‡‡é›† URL: {url} (å°è¯• {attempt + 1}/{max_retries})")
                result = self.app.scrape(url=url, **config)

                # éªŒè¯ç»“æœ
                if not result or not hasattr(result, "markdown"):
                    raise ValueError("è¿”å›ç»“æœæ— æ•ˆ")

                # æ›´æ–°ç»Ÿè®¡
                self.request_count += 1
                self.success_count += 1
                self.total_cost += self.cost_per_request

                logger.info(f"âœ… æˆåŠŸé‡‡é›†: {url}")
                logger.debug(
                    f"ğŸ“Š ç»Ÿè®¡: è¯·æ±‚ {self.request_count}, "
                    f"æˆåŠŸ {self.success_count}, "
                    f"å¤±è´¥ {self.failure_count}, "
                    f"æˆæœ¬ ${self.total_cost:.2f}/{self.daily_budget}"
                )

                # è¿”å›ç»“æ„åŒ–æ•°æ®
                return {
                    "url": url,
                    "metadata": {
                        "source_url": result.metadata.source_url,
                        "title": result.metadata.title,
                        "description": result.metadata.description,
                        "status_code": result.metadata.status_code,
                        "cache_state": result.metadata.get("cacheState", "unknown"),
                    },
                    "content": {
                        "markdown": result.markdown,
                        "html": getattr(result, "html", None),
                        "links": getattr(result, "links", []),
                    },
                    "scraped_at": datetime.now().isoformat(),
                    "attempt": attempt + 1,
                }

            except AuthenticationError as e:
                logger.error(f"âŒ è®¤è¯å¤±è´¥: {e}")
                self.failure_count += 1
                return None  # å¯†é’¥é—®é¢˜ï¼Œä¸é‡è¯•

            except RateLimitError:
                if attempt < max_retries - 1:
                    wait_time = SCRAPE_STRATEGY["retry_delay_base"] ** attempt
                    logger.warning(
                        f"â³ é€Ÿç‡é™åˆ¶ï¼Œ{wait_time}ç§’åé‡è¯•... ({attempt + 1}/{max_retries})"
                    )
                    time.sleep(wait_time)
                else:
                    logger.error(f"âŒ è¾¾åˆ°é€Ÿç‡é™åˆ¶: {url}")
                    self.failure_count += 1
                    return None

            except RequestTimeoutError:
                if attempt < max_retries - 1:
                    wait_time = SCRAPE_STRATEGY["retry_delay_base"] ** attempt
                    logger.warning(
                        f"â³ è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... ({attempt + 1}/{max_retries})"
                    )
                    time.sleep(wait_time)
                else:
                    logger.error(f"âŒ è¶…æ—¶å¤±è´¥: {url}")
                    self.failure_count += 1
                    return None

            except Exception as e:
                logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {url} - {e}")
                self.failure_count += 1
                return None

        return None

    def batch_scrape_urls(
        self, urls: List[str], config: Optional[Dict] = None
    ) -> List[Dict]:
        """æ‰¹é‡é‡‡é›† URL åˆ—è¡¨

        Args:
            urls: URL åˆ—è¡¨
            config: Scrape é…ç½®ï¼ˆå¯é€‰ï¼‰

        Returns:
            é‡‡é›†ç»“æœåˆ—è¡¨
        """
        results = []
        batch_size = SCRAPE_STRATEGY["batch_size"]

        logger.info(f"ğŸ“¦ æ‰¹é‡é‡‡é›† {len(urls)} ä¸ª URLï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")

        for i in range(0, len(urls), batch_size):
            batch = urls[i : i + batch_size]
            batch_num = i // batch_size + 1

            logger.info(f"â³ å¤„ç†æ‰¹æ¬¡ {batch_num} ({len(batch)} ä¸ª URL)...")

            try:
                # ä½¿ç”¨ Firecrawl batch_scrape
                job = self.app.batch_scrape(
                    urls=batch,
                    **config or {},
                    poll_interval=2,
                    timeout=SCRAPE_STRATEGY["batch_timeout"],
                )

                # å¤„ç†ç»“æœ
                for doc in job.data:
                    results.append(
                        {
                            "url": doc.metadata.source_url,
                            "metadata": {
                                "title": doc.metadata.title,
                                "description": doc.metadata.description,
                                "status_code": doc.metadata.status_code,
                            },
                            "content": {"markdown": doc.markdown},
                            "scraped_at": datetime.now().isoformat(),
                            "batch_num": batch_num,
                        }
                    )

                self.request_count += len(batch)
                self.success_count += len(job.data)
                self.total_cost += len(batch) * self.cost_per_request

                logger.info(
                    f"   âœ… æ‰¹æ¬¡ {batch_num}: {len(job.data)}/{len(batch)} æˆåŠŸ"
                )

            except Exception as e:
                logger.error(f"   âŒ æ‰¹æ¬¡ {batch_num} å¤±è´¥: {e}")

                # é€ä¸ªé‡è¯•
                logger.info("   ğŸ”„ é€ä¸ªé‡è¯•...")
                for url in batch:
                    result = self.scrape_url(url, config)
                    if result:
                        results.append(result)

            # å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
            if i + batch_size < len(urls):
                delay = SCRAPE_STRATEGY["delay_between_batches"]
                logger.debug(f"   â¸ï¸  ç­‰å¾… {delay} ç§’...")
                time.sleep(delay)

        logger.info(
            f"âœ… æ‰¹é‡é‡‡é›†å®Œæˆ: {len(results)}/{len(urls)} æˆåŠŸ "
            f"(æˆæœ¬: ${self.total_cost:.2f})"
        )

        return results

    def scrape_news_source(self, source_config: Dict) -> Dict:
        """é‡‡é›†å•ä¸ªæ–°é—»æº

        Args:
            source_config: æ–°é—»æºé…ç½®

        Returns:
            é‡‡é›†ç»“æœå­—å…¸
        """
        source_id = source_config["id"]
        source_name = source_config["name"]
        source_url = source_config["url"]

        logger.info(f"ğŸ“° å¼€å§‹é‡‡é›†æ–°é—»æº: {source_name} ({source_id})")

        start_time = datetime.now()

        # 1. çˆ¬å–é¦–é¡µ
        logger.info(f"   ğŸ” çˆ¬å–é¦–é¡µ: {source_url}")
        homepage = self.scrape_url(source_url, source_config.get("scrape_config"))

        if not homepage:
            logger.error(f"   âŒ é¦–é¡µé‡‡é›†å¤±è´¥: {source_name}")
            return {
                "source_id": source_id,
                "source_name": source_name,
                "status": "failed",
                "error": "é¦–é¡µé‡‡é›†å¤±è´¥",
                "scraped_at": datetime.now().isoformat(),
            }

        # 2. æå–æ–‡ç« é“¾æ¥
        links = homepage.get("content", {}).get("links", [])
        article_links = [link for link in links if self._is_article_link(link)][:20]

        logger.info(f"   ğŸ”— å‘ç° {len(article_links)} ä¸ªæ–‡ç« é“¾æ¥")

        # 3. æ‰¹é‡é‡‡é›†æ–‡ç« 
        if article_links:
            articles = self.batch_scrape_urls(
                article_links, source_config.get("scrape_config")
            )
        else:
            articles = []

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        logger.info(
            f"âœ… å®Œæˆé‡‡é›†: {source_name} "
            f"(æ–‡ç« æ•°: {len(articles)}, è€—æ—¶: {duration:.1f}ç§’)"
        )

        return {
            "source_id": source_id,
            "source_name": source_name,
            "source_url": source_url,
            "status": "success",
            "homepage": homepage,
            "articles": articles,
            "stats": {
                "total_articles": len(articles),
                "duration_seconds": duration,
                "articles_per_minute": len(articles) / (duration / 60)
                if duration > 0
                else 0,
            },
            "scraped_at": datetime.now().isoformat(),
        }

    def scrape_all_sources(self, sources: Optional[List[Dict]] = None) -> List[Dict]:
        """é‡‡é›†æ‰€æœ‰æ–°é—»æº

        Args:
            sources: æ–°é—»æºåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„æºï¼‰

        Returns:
            æ‰€æœ‰é‡‡é›†ç»“æœåˆ—è¡¨
        """
        sources = sources or get_enabled_sources()

        logger.info(f"ğŸš€ å¼€å§‹é‡‡é›† {len(sources)} ä¸ªæ–°é—»æº")

        results = []
        for i, source in enumerate(sources, 1):
            logger.info(f"\n{'=' * 60}")
            logger.info(f"è¿›åº¦: {i}/{len(sources)}")
            logger.info(f"{'=' * 60}")

            result = self.scrape_news_source(source)
            results.append(result)

            # æ£€æŸ¥é¢„ç®—
            if self.total_cost >= self.daily_budget:
                logger.warning(f"âš ï¸  å·²è¾¾æ¯æ—¥é¢„ç®—: ${self.daily_budget}")
                break

        logger.info(f"\n{'=' * 60}")
        logger.info("âœ… æ‰€æœ‰é‡‡é›†å®Œæˆ")
        logger.info(f"{'=' * 60}")

        self._print_summary(results)

        return results

    def _is_article_link(self, link: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡ç« é“¾æ¥

        Args:
            link: URL é“¾æ¥

        Returns:
            æ˜¯å¦ä¸ºæ–‡ç« é“¾æ¥
        """
        # ç®€å•è§„åˆ™ï¼šåŒ…å«å¸¸è§æ–‡ç« è·¯å¾„
        article_patterns = [
            "/article/",
            "/news/",
            "/story/",
            "/post/",
            "category/",
            "/archives/",
        ]

        # æ’é™¤çš„æ¨¡å¼
        exclude_patterns = [
            "/video/",
            "/advertise/",
            "/subscribe/",
            "/contact/",
            "/about/",
            ".pdf",
            ".jpg",
            ".png",
        ]

        link_lower = link.lower()

        # æ£€æŸ¥æ’é™¤æ¨¡å¼
        for pattern in exclude_patterns:
            if pattern in link_lower:
                return False

        # æ£€æŸ¥æ–‡ç« æ¨¡å¼
        for pattern in article_patterns:
            if pattern in link_lower:
                return True

        return False

    def _print_summary(self, results: List[Dict]) -> None:
        """æ‰“å°é‡‡é›†æ€»ç»“

        Args:
            results: é‡‡é›†ç»“æœåˆ—è¡¨
        """
        total_sources = len(results)
        successful_sources = len([r for r in results if r["status"] == "success"])
        total_articles = sum(
            r.get("stats", {}).get("total_articles", 0) for r in results
        )

        logger.info("\nğŸ“Š é‡‡é›†ç»Ÿè®¡:")
        logger.info(f"   æ–°é—»æº: {successful_sources}/{total_sources} æˆåŠŸ")
        logger.info(f"   æ–‡ç« æ•°: {total_articles}")
        logger.info(f"   è¯·æ±‚æ•°: {self.request_count}")
        logger.info(
            f"   æˆåŠŸç‡: {self.success_count}/{self.request_count} ({self.success_count / self.request_count * 100:.1f}%)"
            if self.request_count > 0
            else "   æˆåŠŸç‡: N/A"
        )
        logger.info(f"   æ€»æˆæœ¬: ${self.total_cost:.2f}/{self.daily_budget}")
        logger.info(f"   å‰©ä½™é¢„ç®—: ${self.daily_budget - self.total_cost:.2f}")

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return {
            "request_count": self.request_count,
            "success_count": self.success_count,
            "failure_count": self.failure_count,
            "success_rate": self.success_count / self.request_count
            if self.request_count > 0
            else 0,
            "total_cost": self.total_cost,
            "budget": self.daily_budget,
            "remaining_budget": self.daily_budget - self.total_cost,
        }
