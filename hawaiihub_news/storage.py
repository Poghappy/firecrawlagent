#!/usr/bin/env python3
"""HawaiiHub æ–°é—»é‡‡é›†ç³»ç»Ÿ - æ•°æ®å­˜å‚¨æ¨¡å—

å®ç°æ•°æ®ä¿å­˜ã€å¯¼å‡ºå’Œç®¡ç†åŠŸèƒ½
"""

import csv
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List

from config import (
    DATA_PROCESSING,
    EXPORT_CONFIG,
    PROCESSED_DATA_DIR,
    RAW_DATA_DIR,
)


logger = logging.getLogger(__name__)


class NewsStorage:
    """æ–°é—»æ•°æ®å­˜å‚¨ç®¡ç†å™¨"""

    def __init__(
        self, raw_dir: str = RAW_DATA_DIR, processed_dir: str = PROCESSED_DATA_DIR
    ):
        """åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨

        Args:
            raw_dir: åŸå§‹æ•°æ®ç›®å½•
            processed_dir: å¤„ç†åæ•°æ®ç›®å½•
        """
        self.raw_dir = Path(raw_dir)
        self.processed_dir = Path(processed_dir)

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.raw_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)

        logger.info("æ•°æ®å­˜å‚¨ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        logger.info(f"  åŸå§‹æ•°æ®ç›®å½•: {self.raw_dir}")
        logger.info(f"  å¤„ç†æ•°æ®ç›®å½•: {self.processed_dir}")

    def save_raw_data(self, data: Dict | List, filename: str) -> Path:
        """ä¿å­˜åŸå§‹æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            filename: æ–‡ä»¶å

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        filepath = self.raw_dir / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=EXPORT_CONFIG["json_indent"])

        logger.info(f"âœ… åŸå§‹æ•°æ®å·²ä¿å­˜: {filepath}")
        return filepath

    def save_processed_data(self, data: Dict | List, filename: str) -> Path:
        """ä¿å­˜å¤„ç†åçš„æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            filename: æ–‡ä»¶å

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        filepath = self.processed_dir / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=EXPORT_CONFIG["json_indent"])

        logger.info(f"âœ… å¤„ç†æ•°æ®å·²ä¿å­˜: {filepath}")
        return filepath

    def export_to_json(self, data: List[Dict], filename: str) -> Path:
        """å¯¼å‡ºä¸º JSON æ–‡ä»¶

        Args:
            data: è¦å¯¼å‡ºçš„æ•°æ®åˆ—è¡¨
            filename: æ–‡ä»¶å

        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„
        """
        filepath = self.processed_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=EXPORT_CONFIG["json_indent"])

        logger.info(f"ğŸ“„ å¯¼å‡º JSON: {filepath} ({len(data)} æ¡è®°å½•)")
        return filepath

    def export_to_markdown(self, data: List[Dict], filename: str) -> Path:
        """å¯¼å‡ºä¸º Markdown æ–‡ä»¶

        Args:
            data: è¦å¯¼å‡ºçš„æ•°æ®åˆ—è¡¨
            filename: æ–‡ä»¶å

        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„
        """
        filepath = self.processed_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            # å†™å…¥æ ‡é¢˜
            f.write("# HawaiiHub æ–°é—»é‡‡é›†ç»“æœ\n\n")
            f.write(f"> å¯¼å‡ºæ—¶é—´: {datetime.now()}\n\n")
            f.write(f"> æ–‡ç« æ•°é‡: {len(data)}\n\n")
            f.write("---\n\n")

            # å†™å…¥æ¯ç¯‡æ–‡ç« 
            for i, article in enumerate(data, 1):
                f.write(f"## {i}. {article.get('title', 'Untitled')}\n\n")
                f.write(f"**æ¥æº**: {article.get('source_name', 'Unknown')}\n\n")
                f.write(f"**URL**: {article.get('url', 'N/A')}\n\n")
                f.write(f"**é‡‡é›†æ—¶é—´**: {article.get('scraped_at', 'N/A')}\n\n")

                # æ–‡ç« å†…å®¹
                content = article.get("content", "")
                if content:
                    f.write(f"{content}\n\n")

                f.write("---\n\n")

        logger.info(f"ğŸ“ å¯¼å‡º Markdown: {filepath} ({len(data)} æ¡è®°å½•)")
        return filepath

    def export_to_csv(self, data: List[Dict], filename: str) -> Path:
        """å¯¼å‡ºä¸º CSV æ–‡ä»¶

        Args:
            data: è¦å¯¼å‡ºçš„æ•°æ®åˆ—è¡¨
            filename: æ–‡ä»¶å

        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„
        """
        if not data:
            logger.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return None

        filepath = self.processed_dir / filename

        # æå–æ‰€æœ‰å­—æ®µ
        fieldnames = set()
        for item in data:
            fieldnames.update(item.keys())

        fieldnames = sorted(fieldnames)

        with open(
            filepath, "w", encoding=EXPORT_CONFIG["csv_encoding"], newline=""
        ) as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)

        logger.info(f"ğŸ“Š å¯¼å‡º CSV: {filepath} ({len(data)} æ¡è®°å½•)")
        return filepath

    def export_news_results(self, results: List[Dict], prefix: str = "news") -> Dict:
        """å¯¼å‡ºæ–°é—»é‡‡é›†ç»“æœåˆ°å¤šç§æ ¼å¼

        Args:
            results: é‡‡é›†ç»“æœåˆ—è¡¨
            prefix: æ–‡ä»¶åå‰ç¼€

        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        timestamp = datetime.now().strftime(EXPORT_CONFIG["timestamp_format"])

        # å¤„ç†æ•°æ®ï¼šæå–æ‰€æœ‰æ–‡ç« 
        all_articles = []
        for result in results:
            source_name = result.get("source_name", "Unknown")
            source_id = result.get("source_id", "unknown")

            for article in result.get("articles", []):
                all_articles.append(
                    {
                        "source_id": source_id,
                        "source_name": source_name,
                        "url": article.get("url"),
                        "title": article.get("metadata", {}).get("title"),
                        "description": article.get("metadata", {}).get("description"),
                        "content": article.get("content", {}).get("markdown"),
                        "scraped_at": article.get("scraped_at"),
                    }
                )

        logger.info(f"å‡†å¤‡å¯¼å‡º {len(all_articles)} ç¯‡æ–‡ç« ")

        exported_files = {}

        # å¯¼å‡º JSON
        if "json" in EXPORT_CONFIG["formats"]:
            filename = f"{prefix}_{timestamp}.json"
            exported_files["json"] = self.export_to_json(all_articles, filename)

        # å¯¼å‡º Markdown
        if "markdown" in EXPORT_CONFIG["formats"]:
            filename = f"{prefix}_{timestamp}.md"
            exported_files["markdown"] = self.export_to_markdown(all_articles, filename)

        # å¯¼å‡º CSV
        if "csv" in EXPORT_CONFIG["formats"]:
            filename = f"{prefix}_{timestamp}.csv"
            exported_files["csv"] = self.export_to_csv(all_articles, filename)

        logger.info(f"âœ… å¯¼å‡ºå®Œæˆ: {len(exported_files)} ä¸ªæ–‡ä»¶")
        for format_name, filepath in exported_files.items():
            logger.info(f"   {format_name.upper()}: {filepath}")

        return exported_files

    def clean_old_files(self, days_to_keep: int = 30) -> int:
        """æ¸…ç†æ—§æ•°æ®æ–‡ä»¶

        Args:
            days_to_keep: ä¿ç•™å¤©æ•°

        Returns:
            åˆ é™¤çš„æ–‡ä»¶æ•°é‡
        """
        from datetime import timedelta

        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        deleted_count = 0

        for directory in [self.raw_dir, self.processed_dir]:
            for filepath in directory.rglob("*"):
                if filepath.is_file():
                    file_time = datetime.fromtimestamp(filepath.stat().st_mtime)
                    if file_time < cutoff_date:
                        filepath.unlink()
                        deleted_count += 1
                        logger.debug(f"åˆ é™¤æ—§æ–‡ä»¶: {filepath}")

        logger.info(f"ğŸ§¹ æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªæ—§æ–‡ä»¶")
        return deleted_count


def process_article_content(content: str) -> str:
    """å¤„ç†æ–‡ç« å†…å®¹ï¼ˆæ¸…æ´—ã€è§„èŒƒåŒ–ï¼‰

    Args:
        content: åŸå§‹å†…å®¹

    Returns:
        å¤„ç†åçš„å†…å®¹
    """
    import re

    if not content:
        return ""

    # ç§»é™¤ç‰¹å®šæ¨¡å¼
    for pattern in DATA_PROCESSING["remove_patterns"]:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    # è§„èŒƒåŒ–ç©ºç™½
    content = re.sub(r"\n{3,}", "\n\n", content)
    content = re.sub(r" {2,}", " ", content)

    # ç§»é™¤é¦–å°¾ç©ºç™½
    content = content.strip()

    return content


def deduplicate_articles(articles: List[Dict]) -> List[Dict]:
    """æ–‡ç« å»é‡

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        å»é‡åçš„æ–‡ç« åˆ—è¡¨
    """
    if not DATA_PROCESSING["enable_deduplication"]:
        return articles

    dedup_field = DATA_PROCESSING["dedup_field"]
    seen = set()
    unique_articles = []

    for article in articles:
        key = article.get(dedup_field)
        if key and key not in seen:
            seen.add(key)
            unique_articles.append(article)

    removed_count = len(articles) - len(unique_articles)
    if removed_count > 0:
        logger.info(f"ğŸ”„ å»é‡: ç§»é™¤ {removed_count} ç¯‡é‡å¤æ–‡ç« ")

    return unique_articles
