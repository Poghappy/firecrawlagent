# ğŸ”¥ Firecrawl äº‘ API ä½¿ç”¨è§„èŒƒ

> **ä¸“ä¸º HawaiiHub å›¢é˜Ÿæ‰“é€ çš„ Firecrawl äº‘ API æœ€ä½³å®è·µ**
> **ç‰ˆæœ¬**: v1.0.0
> **æ›´æ–°**: 2025-10-27

---

## ğŸ“‹ ç›®å½•

1. [æ ¸å¿ƒåŸåˆ™](#æ ¸å¿ƒåŸåˆ™)
2. [API å¯†é’¥ç®¡ç†](#api-å¯†é’¥ç®¡ç†)
3. [è¯·æ±‚ä¼˜åŒ–ç­–ç•¥](#è¯·æ±‚ä¼˜åŒ–ç­–ç•¥)
4. [æˆæœ¬æ§åˆ¶](#æˆæœ¬æ§åˆ¶)
5. [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
6. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
7. [å®‰å…¨è§„èŒƒ](#å®‰å…¨è§„èŒƒ)
8. [ç›‘æ§ä¸æ—¥å¿—](#ç›‘æ§ä¸æ—¥å¿—)
9. [ä»£ç è§„èŒƒ](#ä»£ç è§„èŒƒ)
10. [åº”æ€¥é¢„æ¡ˆ](#åº”æ€¥é¢„æ¡ˆ)

---

## ğŸ¯ æ ¸å¿ƒåŸåˆ™

### 1. **æœ€å°åŒ–è¯·æ±‚åŸåˆ™**
```python
# âœ… å¥½çš„åšæ³• - ä½¿ç”¨ batch_scrape
urls = [url1, url2, url3]
results = app.batch_scrape(urls, formats=['markdown'])

# âŒ é¿å… - å¾ªç¯è°ƒç”¨
for url in urls:
    result = app.scrape(url)  # æµªè´¹ API é…é¢
```

### 2. **ç¼“å­˜ä¼˜å…ˆåŸåˆ™**
```python
# âœ… ä½¿ç”¨ maxAge å‚æ•°å¯ç”¨ç¼“å­˜
result = app.scrape(
    url="https://example.com",
    formats=['markdown'],
    maxAge=3600000  # 1 å°æ—¶ç¼“å­˜
)
```

### 3. **å†…å®¹è¿‡æ»¤åŸåˆ™**
```python
# âœ… åªæå–éœ€è¦çš„å†…å®¹
result = app.scrape(
    url="https://example.com",
    formats=['markdown'],
    onlyMainContent=True,  # è¿‡æ»¤å¹¿å‘Šã€å¯¼èˆªæ ç­‰
    removeTags=['script', 'style', 'iframe']
)
```

---

## ğŸ”‘ API å¯†é’¥ç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®

#### **å¼€å‘ç¯å¢ƒ** (`.env.development`)
```bash
# Firecrawl API é…ç½®
FIRECRAWL_API_KEY=fc-dev-xxxxxxxxxxxxxxxx
FIRECRAWL_API_URL=https://api.firecrawl.dev
FIRECRAWL_TIMEOUT=30000
FIRECRAWL_MAX_RETRIES=3

# åŠŸèƒ½å¼€å…³
FIRECRAWL_ENABLE_CACHE=true
FIRECRAWL_CACHE_TTL=3600
FIRECRAWL_ENABLE_DEBUG=true
```

#### **ç”Ÿäº§ç¯å¢ƒ** (`.env.production`)
```bash
# Firecrawl API é…ç½®
FIRECRAWL_API_KEY=fc-prod-xxxxxxxxxxxxxxxx
FIRECRAWL_API_URL=https://api.firecrawl.dev
FIRECRAWL_TIMEOUT=60000
FIRECRAWL_MAX_RETRIES=5

# åŠŸèƒ½å¼€å…³
FIRECRAWL_ENABLE_CACHE=true
FIRECRAWL_CACHE_TTL=7200
FIRECRAWL_ENABLE_DEBUG=false

# é€Ÿç‡é™åˆ¶
FIRECRAWL_RATE_LIMIT=100  # æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
FIRECRAWL_BURST_LIMIT=20  # çªå‘æµé‡é™åˆ¶
```

### å¯†é’¥è½®æ¢ç­–ç•¥

```python
# config/firecrawl.py
import os
from datetime import datetime, timedelta

class FirecrawlConfig:
    """Firecrawl é…ç½®ç®¡ç†"""

    # API å¯†é’¥ï¼ˆæ”¯æŒå¤šå¯†é’¥è½®æ¢ï¼‰
    API_KEYS = [
        os.getenv('FIRECRAWL_API_KEY_1'),
        os.getenv('FIRECRAWL_API_KEY_2'),  # å¤‡ç”¨å¯†é’¥
        os.getenv('FIRECRAWL_API_KEY_3'),  # å¤‡ç”¨å¯†é’¥
    ]

    # å½“å‰ä½¿ç”¨çš„å¯†é’¥ç´¢å¼•
    CURRENT_KEY_INDEX = 0

    # å¯†é’¥è½®æ¢æ—¶é—´ï¼ˆæ¯ 24 å°æ—¶ï¼‰
    KEY_ROTATION_INTERVAL = timedelta(hours=24)
    LAST_ROTATION = datetime.now()

    @classmethod
    def get_api_key(cls) -> str:
        """è·å–å½“å‰ API å¯†é’¥"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢
        if datetime.now() - cls.LAST_ROTATION > cls.KEY_ROTATION_INTERVAL:
            cls.rotate_key()

        return cls.API_KEYS[cls.CURRENT_KEY_INDEX]

    @classmethod
    def rotate_key(cls):
        """è½®æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥"""
        cls.CURRENT_KEY_INDEX = (cls.CURRENT_KEY_INDEX + 1) % len(cls.API_KEYS)
        cls.LAST_ROTATION = datetime.now()
        print(f"å·²è½®æ¢åˆ°å¯†é’¥ #{cls.CURRENT_KEY_INDEX + 1}")
```

### å¯†é’¥å®‰å…¨è§„èŒƒ

#### âœ… **å¿…é¡»éµå®ˆ**
1. **ç»ä¸**å°† API å¯†é’¥ç¡¬ç¼–ç åˆ°ä»£ç ä¸­
2. **ç»ä¸**å°† `.env` æ–‡ä»¶æäº¤åˆ° Git
3. **å¿…é¡»**ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–å¯†é’¥ç®¡ç†æœåŠ¡
4. **å¿…é¡»**å®šæœŸè½®æ¢å¯†é’¥ï¼ˆå»ºè®®æ¯æœˆï¼‰
5. **å¿…é¡»**ä¸ºä¸åŒç¯å¢ƒä½¿ç”¨ä¸åŒå¯†é’¥

#### âŒ **ä¸¥ç¦è¡Œä¸º**
```python
# âŒ ä¸¥ç¦ç¡¬ç¼–ç 
app = FirecrawlApp(api_key="fc-xxxxxxxxxxxxxxxx")

# âŒ ä¸¥ç¦æ‰“å°å¯†é’¥
print(f"API Key: {api_key}")

# âŒ ä¸¥ç¦åœ¨æ—¥å¿—ä¸­è®°å½•å¯†é’¥
logger.info(f"Using key: {api_key}")
```

#### âœ… **æ­£ç¡®åšæ³•**
```python
# âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡
import os
from firecrawl import FirecrawlApp

api_key = os.getenv('FIRECRAWL_API_KEY')
if not api_key:
    raise ValueError("æœªè®¾ç½® FIRECRAWL_API_KEY ç¯å¢ƒå˜é‡")

app = FirecrawlApp(api_key=api_key)

# âœ… æ—¥å¿—ä¸­éšè—å¯†é’¥
def mask_api_key(key: str) -> str:
    """éšè— API å¯†é’¥"""
    return f"{key[:8]}...{key[-4:]}" if len(key) > 12 else "***"

logger.info(f"ä½¿ç”¨ API å¯†é’¥: {mask_api_key(api_key)}")
```

---

## âš¡ è¯·æ±‚ä¼˜åŒ–ç­–ç•¥

### 1. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
from typing import List, Dict
import asyncio
from firecrawl import FirecrawlApp

class FirecrawlOptimizer:
    """Firecrawl è¯·æ±‚ä¼˜åŒ–å™¨"""

    def __init__(self, api_key: str, batch_size: int = 10):
        self.app = FirecrawlApp(api_key=api_key)
        self.batch_size = batch_size

    def batch_scrape_optimized(
        self,
        urls: List[str],
        formats: List[str] = ['markdown'],
        **kwargs
    ) -> List[Dict]:
        """
        ä¼˜åŒ–çš„æ‰¹é‡çˆ¬å–

        ç‰¹æ€§ï¼š
        1. è‡ªåŠ¨åˆ†æ‰¹å¤„ç†
        2. å¹¶å‘æ§åˆ¶
        3. å¤±è´¥é‡è¯•
        4. è¿›åº¦è¿½è¸ª
        """
        results = []
        total_batches = (len(urls) + self.batch_size - 1) // self.batch_size

        for i in range(0, len(urls), self.batch_size):
            batch = urls[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1

            print(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ª URL)")

            try:
                batch_results = self.app.batch_scrape(
                    urls=batch,
                    formats=formats,
                    **kwargs
                )
                results.extend(batch_results)

            except Exception as e:
                print(f"æ‰¹æ¬¡ {batch_num} å¤±è´¥: {e}")
                # å¤±è´¥é‡è¯•ï¼ˆé€ä¸ªå¤„ç†ï¼‰
                for url in batch:
                    try:
                        result = self.app.scrape(url, formats=formats, **kwargs)
                        results.append(result)
                    except Exception as url_error:
                        print(f"URL {url} å¤±è´¥: {url_error}")
                        results.append({'url': url, 'error': str(url_error)})

        return results
```

### 2. æ™ºèƒ½é‡è¯•ç­–ç•¥

```python
import time
from typing import Callable, Any
from functools import wraps

def retry_with_exponential_backoff(
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exponential_base: float = 2.0
):
    """
    æŒ‡æ•°é€€é¿é‡è¯•è£…é¥°å™¨

    å‚æ•°ï¼š
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        base_delay: åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
        max_delay: æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        exponential_base: æŒ‡æ•°åŸºæ•°
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            retries = 0
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)

                except Exception as e:
                    retries += 1

                    if retries >= max_retries:
                        raise

                    # è®¡ç®—å»¶è¿Ÿæ—¶é—´
                    delay = min(
                        base_delay * (exponential_base ** retries),
                        max_delay
                    )

                    print(f"è¯·æ±‚å¤±è´¥ï¼Œ{delay:.1f}ç§’åé‡è¯• ({retries}/{max_retries}): {e}")
                    time.sleep(delay)

            raise Exception(f"è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")

        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@retry_with_exponential_backoff(max_retries=5)
def scrape_with_retry(url: str):
    """å¸¦é‡è¯•çš„çˆ¬å–"""
    return app.scrape(url, formats=['markdown'])
```

### 3. å¹¶å‘æ§åˆ¶

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict

class ConcurrentScraper:
    """å¹¶å‘çˆ¬å–ç®¡ç†å™¨"""

    def __init__(
        self,
        api_key: str,
        max_workers: int = 5,
        rate_limit: int = 10  # æ¯ç§’æœ€å¤§è¯·æ±‚æ•°
    ):
        self.app = FirecrawlApp(api_key=api_key)
        self.max_workers = max_workers
        self.rate_limit = rate_limit
        self.semaphore = asyncio.Semaphore(max_workers)

    async def scrape_url(self, url: str, **kwargs) -> Dict:
        """å¼‚æ­¥çˆ¬å–å•ä¸ª URL"""
        async with self.semaphore:
            try:
                # é€Ÿç‡é™åˆ¶
                await asyncio.sleep(1 / self.rate_limit)

                # æ‰§è¡Œçˆ¬å–
                result = await asyncio.to_thread(
                    self.app.scrape,
                    url,
                    **kwargs
                )
                return {'url': url, 'success': True, 'data': result}

            except Exception as e:
                return {'url': url, 'success': False, 'error': str(e)}

    async def scrape_all(self, urls: List[str], **kwargs) -> List[Dict]:
        """å¼‚æ­¥çˆ¬å–æ‰€æœ‰ URL"""
        tasks = [self.scrape_url(url, **kwargs) for url in urls]
        return await asyncio.gather(*tasks)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    scraper = ConcurrentScraper(
        api_key=os.getenv('FIRECRAWL_API_KEY'),
        max_workers=5,
        rate_limit=10
    )

    urls = ["https://example.com/1", "https://example.com/2", ...]
    results = await scraper.scrape_all(urls, formats=['markdown'])

    print(f"æˆåŠŸ: {sum(1 for r in results if r['success'])}/{len(results)}")

# asyncio.run(main())
```

---

## ğŸ’° æˆæœ¬æ§åˆ¶

### 1. æˆæœ¬ç›‘æ§

```python
from datetime import datetime
from typing import Dict, List

class CostTracker:
    """API æˆæœ¬è¿½è¸ªå™¨"""

    # å®šä»·ï¼ˆæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    PRICING = {
        'scrape': 0.005,      # æ¯é¡µ $0.005
        'crawl': 0.004,       # æ¯é¡µ $0.004ï¼ˆæ‰¹é‡æŠ˜æ‰£ï¼‰
        'search': 0.01,       # æ¯æ¬¡æœç´¢ $0.01
        'extract': 0.008,     # æ¯æ¬¡æå– $0.008
    }

    def __init__(self):
        self.usage = {
            'scrape': 0,
            'crawl': 0,
            'search': 0,
            'extract': 0,
        }
        self.start_time = datetime.now()

    def track(self, operation: str, count: int = 1):
        """è®°å½• API ä½¿ç”¨"""
        if operation in self.usage:
            self.usage[operation] += count

    def get_cost(self) -> float:
        """è®¡ç®—æ€»æˆæœ¬"""
        total = sum(
            self.usage[op] * self.PRICING[op]
            for op in self.usage
        )
        return round(total, 2)

    def get_report(self) -> Dict:
        """ç”Ÿæˆæˆæœ¬æŠ¥å‘Š"""
        runtime = (datetime.now() - self.start_time).total_seconds()

        return {
            'total_cost': self.get_cost(),
            'usage': self.usage,
            'runtime_seconds': runtime,
            'cost_per_hour': round(self.get_cost() / (runtime / 3600), 2) if runtime > 0 else 0,
            'details': {
                op: {
                    'count': self.usage[op],
                    'cost': round(self.usage[op] * self.PRICING[op], 2)
                }
                for op in self.usage
            }
        }

# å…¨å±€æˆæœ¬è¿½è¸ªå™¨
cost_tracker = CostTracker()

# è£…é¥°å™¨
def track_cost(operation: str):
    """æˆæœ¬è¿½è¸ªè£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            result = func(*args, **kwargs)

            # æ ¹æ®ç»“æœè¿½è¸ªæˆæœ¬
            if isinstance(result, list):
                cost_tracker.track(operation, len(result))
            else:
                cost_tracker.track(operation, 1)

            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@track_cost('scrape')
def scrape_urls(urls: List[str]):
    return app.batch_scrape(urls)

# å®šæœŸæ‰“å°æˆæœ¬æŠ¥å‘Š
import atexit

def print_cost_report():
    report = cost_tracker.get_report()
    print("\n" + "=" * 50)
    print("ğŸ“Š Firecrawl API æˆæœ¬æŠ¥å‘Š")
    print("=" * 50)
    print(f"æ€»æˆæœ¬: ${report['total_cost']}")
    print(f"è¿è¡Œæ—¶é•¿: {report['runtime_seconds']:.0f} ç§’")
    print(f"æ¯å°æ—¶æˆæœ¬: ${report['cost_per_hour']}")
    print("\nè¯¦ç»†ä½¿ç”¨:")
    for op, details in report['details'].items():
        print(f"  {op}: {details['count']} æ¬¡ (${details['cost']})")
    print("=" * 50)

atexit.register(print_cost_report)
```

### 2. é¢„ç®—é™åˆ¶

```python
class BudgetLimiter:
    """é¢„ç®—é™åˆ¶å™¨"""

    def __init__(
        self,
        daily_budget: float = 10.0,   # æ¯æ—¥é¢„ç®— $10
        monthly_budget: float = 200.0  # æ¯æœˆé¢„ç®— $200
    ):
        self.daily_budget = daily_budget
        self.monthly_budget = monthly_budget
        self.daily_spent = 0.0
        self.monthly_spent = 0.0
        self.last_reset = datetime.now()

    def check_budget(self, estimated_cost: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…é¢„ç®—"""
        # é‡ç½®æ¯æ—¥é¢„ç®—
        if (datetime.now() - self.last_reset).days >= 1:
            self.daily_spent = 0.0
            self.last_reset = datetime.now()

        # æ£€æŸ¥é¢„ç®—
        if self.daily_spent + estimated_cost > self.daily_budget:
            raise BudgetExceededError(f"è¶…è¿‡æ¯æ—¥é¢„ç®— (${self.daily_budget})")

        if self.monthly_spent + estimated_cost > self.monthly_budget:
            raise BudgetExceededError(f"è¶…è¿‡æ¯æœˆé¢„ç®— (${self.monthly_budget})")

        return True

    def record_cost(self, cost: float):
        """è®°å½•æ¶ˆè´¹"""
        self.daily_spent += cost
        self.monthly_spent += cost

class BudgetExceededError(Exception):
    """é¢„ç®—è¶…æ”¯å¼‚å¸¸"""
    pass

# ä½¿ç”¨ç¤ºä¾‹
budget = BudgetLimiter(daily_budget=10.0)

def scrape_with_budget(urls: List[str]):
    """å¸¦é¢„ç®—æ§åˆ¶çš„çˆ¬å–"""
    estimated_cost = len(urls) * 0.005

    # æ£€æŸ¥é¢„ç®—
    budget.check_budget(estimated_cost)

    # æ‰§è¡Œçˆ¬å–
    results = app.batch_scrape(urls)

    # è®°å½•æˆæœ¬
    budget.record_cost(estimated_cost)

    return results
```

### 3. æˆæœ¬ä¼˜åŒ–å»ºè®®

#### **ç­–ç•¥ 1: æ™ºèƒ½ç¼“å­˜**
```python
import hashlib
import json
from typing import Optional

class SmartCache:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = "./cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def _get_cache_key(self, url: str, params: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        data = f"{url}:{json.dumps(params, sort_keys=True)}"
        return hashlib.md5(data.encode()).hexdigest()

    def get(self, url: str, params: dict, max_age: int = 3600) -> Optional[dict]:
        """è·å–ç¼“å­˜"""
        cache_key = self._get_cache_key(url, params)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")

        if not os.path.exists(cache_file):
            return None

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        mtime = os.path.getmtime(cache_file)
        if time.time() - mtime > max_age:
            os.remove(cache_file)
            return None

        with open(cache_file, 'r') as f:
            return json.load(f)

    def set(self, url: str, params: dict, data: dict):
        """è®¾ç½®ç¼“å­˜"""
        cache_key = self._get_cache_key(url, params)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")

        with open(cache_file, 'w') as f:
            json.dump(data, f)

# ä½¿ç”¨ç¼“å­˜
cache = SmartCache()

def scrape_with_cache(url: str, **params):
    """å¸¦ç¼“å­˜çš„çˆ¬å–"""
    # å°è¯•ä»ç¼“å­˜è·å–
    cached = cache.get(url, params, max_age=3600)
    if cached:
        print(f"âœ… å‘½ä¸­ç¼“å­˜: {url}")
        return cached

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨ API
    print(f"ğŸ”„ API è¯·æ±‚: {url}")
    result = app.scrape(url, **params)

    # ä¿å­˜åˆ°ç¼“å­˜
    cache.set(url, params, result)

    return result
```

#### **ç­–ç•¥ 2: å†…å®¹å»é‡**
```python
from typing import Set

class URLDeduplicator:
    """URL å»é‡å™¨"""

    def __init__(self):
        self.seen_urls: Set[str] = set()
        self.seen_content_hashes: Set[str] = set()

    def is_duplicate_url(self, url: str) -> bool:
        """æ£€æŸ¥ URL æ˜¯å¦é‡å¤"""
        normalized_url = self._normalize_url(url)
        if normalized_url in self.seen_urls:
            return True
        self.seen_urls.add(normalized_url)
        return False

    def is_duplicate_content(self, content: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤"""
        content_hash = hashlib.md5(content.encode()).hexdigest()
        if content_hash in self.seen_content_hashes:
            return True
        self.seen_content_hashes.add(content_hash)
        return False

    @staticmethod
    def _normalize_url(url: str) -> str:
        """è§„èŒƒåŒ– URL"""
        # ç§»é™¤æŸ¥è¯¢å‚æ•°ã€é”šç‚¹ç­‰
        from urllib.parse import urlparse
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

# ä½¿ç”¨å»é‡
deduplicator = URLDeduplicator()

def scrape_unique_urls(urls: List[str]):
    """åªçˆ¬å–ä¸é‡å¤çš„ URL"""
    unique_urls = [
        url for url in urls
        if not deduplicator.is_duplicate_url(url)
    ]

    print(f"å»é‡å: {len(unique_urls)}/{len(urls)} ä¸ª URL")
    return app.batch_scrape(unique_urls)
```

---

## âŒ é”™è¯¯å¤„ç†

### 1. å®Œæ•´çš„é”™è¯¯å¤„ç†ä½“ç³»

```python
from enum import Enum
from typing import Optional, Dict, Any

class FirecrawlErrorType(Enum):
    """é”™è¯¯ç±»å‹æšä¸¾"""
    AUTHENTICATION = "è®¤è¯å¤±è´¥"
    RATE_LIMIT = "é€Ÿç‡é™åˆ¶"
    TIMEOUT = "è¯·æ±‚è¶…æ—¶"
    INVALID_URL = "æ— æ•ˆ URL"
    NETWORK = "ç½‘ç»œé”™è¯¯"
    SERVER_ERROR = "æœåŠ¡å™¨é”™è¯¯"
    UNKNOWN = "æœªçŸ¥é”™è¯¯"

class FirecrawlError(Exception):
    """Firecrawl è‡ªå®šä¹‰å¼‚å¸¸"""

    def __init__(
        self,
        error_type: FirecrawlErrorType,
        message: str,
        original_error: Optional[Exception] = None,
        context: Optional[Dict[str, Any]] = None
    ):
        self.error_type = error_type
        self.message = message
        self.original_error = original_error
        self.context = context or {}

        super().__init__(self.message)

    def __str__(self):
        return f"[{self.error_type.value}] {self.message}"

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'error_type': self.error_type.name,
            'message': self.message,
            'context': self.context,
            'original_error': str(self.original_error) if self.original_error else None
        }

class ErrorHandler:
    """ç»Ÿä¸€é”™è¯¯å¤„ç†å™¨"""

    @staticmethod
    def handle_error(e: Exception, url: Optional[str] = None) -> FirecrawlError:
        """å¤„ç†å¹¶åˆ†ç±»é”™è¯¯"""
        error_msg = str(e).lower()
        context = {'url': url} if url else {}

        # è®¤è¯é”™è¯¯
        if 'unauthorized' in error_msg or 'api key' in error_msg:
            return FirecrawlError(
                FirecrawlErrorType.AUTHENTICATION,
                "API å¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ",
                e,
                context
            )

        # é€Ÿç‡é™åˆ¶
        if 'rate limit' in error_msg or '429' in error_msg:
            return FirecrawlError(
                FirecrawlErrorType.RATE_LIMIT,
                "è¶…è¿‡ API é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•",
                e,
                context
            )

        # è¶…æ—¶
        if 'timeout' in error_msg:
            return FirecrawlError(
                FirecrawlErrorType.TIMEOUT,
                "è¯·æ±‚è¶…æ—¶",
                e,
                context
            )

        # æ— æ•ˆ URL
        if 'invalid url' in error_msg:
            return FirecrawlError(
                FirecrawlErrorType.INVALID_URL,
                f"æ— æ•ˆçš„ URL: {url}",
                e,
                context
            )

        # ç½‘ç»œé”™è¯¯
        if 'connection' in error_msg or 'network' in error_msg:
            return FirecrawlError(
                FirecrawlErrorType.NETWORK,
                "ç½‘ç»œè¿æ¥å¤±è´¥",
                e,
                context
            )

        # æœåŠ¡å™¨é”™è¯¯
        if '500' in error_msg or '502' in error_msg or '503' in error_msg:
            return FirecrawlError(
                FirecrawlErrorType.SERVER_ERROR,
                "Firecrawl æœåŠ¡å™¨é”™è¯¯",
                e,
                context
            )

        # æœªçŸ¥é”™è¯¯
        return FirecrawlError(
            FirecrawlErrorType.UNKNOWN,
            f"æœªçŸ¥é”™è¯¯: {str(e)}",
            e,
            context
        )

# ä½¿ç”¨ç¤ºä¾‹
def safe_scrape(url: str, **kwargs) -> Dict:
    """å®‰å…¨çš„çˆ¬å–ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰"""
    try:
        return app.scrape(url, **kwargs)

    except Exception as e:
        # ç»Ÿä¸€é”™è¯¯å¤„ç†
        error = ErrorHandler.handle_error(e, url)

        # è®°å½•é”™è¯¯æ—¥å¿—
        logger.error(f"çˆ¬å–å¤±è´¥: {error}", extra=error.to_dict())

        # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
        if error.error_type == FirecrawlErrorType.RATE_LIMIT:
            time.sleep(60)  # ç­‰å¾… 1 åˆ†é’Ÿåé‡è¯•
            return safe_scrape(url, **kwargs)

        elif error.error_type == FirecrawlErrorType.TIMEOUT:
            # å¢åŠ è¶…æ—¶æ—¶é—´é‡è¯•
            kwargs['timeout'] = kwargs.get('timeout', 30) * 2
            return safe_scrape(url, **kwargs)

        # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
        raise error
```

### 2. é”™è¯¯æ—¥å¿—è®°å½•

```python
import logging
from logging.handlers import RotatingFileHandler

# é…ç½®æ—¥å¿—
def setup_logging():
    """é…ç½® Firecrawl æ—¥å¿—"""
    logger = logging.getLogger('firecrawl')
    logger.setLevel(logging.INFO)

    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆè‡ªåŠ¨è½®è½¬ï¼‰
    file_handler = RotatingFileHandler(
        'logs/firecrawl.log',
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5
    )
    file_handler.setLevel(logging.INFO)

    # é”™è¯¯æ—¥å¿—å•ç‹¬è®°å½•
    error_handler = RotatingFileHandler(
        'logs/firecrawl_errors.log',
        maxBytes=10*1024*1024,
        backupCount=5
    )
    error_handler.setLevel(logging.ERROR)

    # æ ¼å¼åŒ–
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    error_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(error_handler)

    return logger

logger = setup_logging()

# ä½¿ç”¨æ—¥å¿—
def logged_scrape(url: str, **kwargs):
    """å¸¦æ—¥å¿—çš„çˆ¬å–"""
    try:
        logger.info(f"å¼€å§‹çˆ¬å–: {url}")
        result = app.scrape(url, **kwargs)
        logger.info(f"çˆ¬å–æˆåŠŸ: {url}")
        return result

    except Exception as e:
        logger.error(
            f"çˆ¬å–å¤±è´¥: {url}",
            extra={
                'url': url,
                'error': str(e),
                'params': kwargs
            },
            exc_info=True
        )
        raise
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. è¿æ¥æ± ç®¡ç†

```python
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class OptimizedFirecrawlApp:
    """ä¼˜åŒ–çš„ Firecrawl å®¢æˆ·ç«¯"""

    def __init__(self, api_key: str):
        self.api_key = api_key

        # é…ç½®è¿æ¥æ± 
        self.session = self._create_session()
        self.app = FirecrawlApp(api_key=api_key)

    def _create_session(self):
        """åˆ›å»ºä¼˜åŒ–çš„ Session"""
        session = requests.Session()

        # é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )

        # HTTP é€‚é…å™¨
        adapter = HTTPAdapter(
            pool_connections=100,    # è¿æ¥æ± å¤§å°
            pool_maxsize=100,
            max_retries=retry_strategy,
            pool_block=False
        )

        session.mount("http://", adapter)
        session.mount("https://", adapter)

        return session
```

### 2. å“åº”å‹ç¼©

```python
# å¯ç”¨ gzip å‹ç¼©
result = app.scrape(
    url="https://example.com",
    headers={
        'Accept-Encoding': 'gzip, deflate',
    }
)
```

### 3. æ€§èƒ½ç›‘æ§

```python
import time
from contextlib import contextmanager

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.metrics = {
            'total_requests': 0,
            'total_time': 0.0,
            'avg_time': 0.0,
            'min_time': float('inf'),
            'max_time': 0.0,
        }

    @contextmanager
    def track(self, operation: str):
        """è¿½è¸ªæ“ä½œæ€§èƒ½"""
        start_time = time.time()

        try:
            yield
        finally:
            elapsed = time.time() - start_time
            self._record(operation, elapsed)

    def _record(self, operation: str, elapsed: float):
        """è®°å½•æ€§èƒ½æ•°æ®"""
        self.metrics['total_requests'] += 1
        self.metrics['total_time'] += elapsed
        self.metrics['avg_time'] = self.metrics['total_time'] / self.metrics['total_requests']
        self.metrics['min_time'] = min(self.metrics['min_time'], elapsed)
        self.metrics['max_time'] = max(self.metrics['max_time'], elapsed)

        logger.info(
            f"[{operation}] è€—æ—¶: {elapsed:.2f}s "
            f"(å¹³å‡: {self.metrics['avg_time']:.2f}s)"
        )

    def get_report(self) -> Dict:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        return {
            'total_requests': self.metrics['total_requests'],
            'total_time': round(self.metrics['total_time'], 2),
            'avg_time': round(self.metrics['avg_time'], 2),
            'min_time': round(self.metrics['min_time'], 2),
            'max_time': round(self.metrics['max_time'], 2),
            'requests_per_second': round(
                self.metrics['total_requests'] / self.metrics['total_time'], 2
            ) if self.metrics['total_time'] > 0 else 0
        }

# ä½¿ç”¨æ€§èƒ½ç›‘æ§
monitor = PerformanceMonitor()

def monitored_scrape(url: str):
    """å¸¦æ€§èƒ½ç›‘æ§çš„çˆ¬å–"""
    with monitor.track('scrape'):
        return app.scrape(url)
```

---

## ğŸ”’ å®‰å…¨è§„èŒƒ

### 1. è¯·æ±‚å¤´å®‰å…¨

```python
# æ¨èçš„è¯·æ±‚å¤´é…ç½®
SECURE_HEADERS = {
    'User-Agent': 'HawaiiHub/1.0 (Firecrawl Bot)',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'DNT': '1',  # Do Not Track
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

result = app.scrape(
    url="https://example.com",
    headers=SECURE_HEADERS
)
```

### 2. URL éªŒè¯

```python
from urllib.parse import urlparse
import re

class URLValidator:
    """URL éªŒè¯å™¨"""

    # å…è®¸çš„åè®®
    ALLOWED_SCHEMES = ['http', 'https']

    # ç¦æ­¢çš„åŸŸåï¼ˆå†…ç½‘ã€æ•æ„ŸåŸŸåï¼‰
    BLOCKED_DOMAINS = [
        'localhost',
        '127.0.0.1',
        '0.0.0.0',
        '::1',
        '192.168.',
        '10.',
        '172.16.',
    ]

    # å…è®¸çš„åŸŸåç™½åå•ï¼ˆå¯é€‰ï¼‰
    ALLOWED_DOMAINS = [
        'hawaiinewsnow.com',
        'staradvertiser.com',
        'civilbeat.org',
        # ... å…¶ä»–å¯ä¿¡åŸŸå
    ]

    @classmethod
    def validate(cls, url: str, strict: bool = False) -> bool:
        """
        éªŒè¯ URL

        å‚æ•°ï¼š
            url: å¾…éªŒè¯çš„ URL
            strict: ä¸¥æ ¼æ¨¡å¼ï¼ˆåªå…è®¸ç™½åå•åŸŸåï¼‰
        """
        try:
            parsed = urlparse(url)

            # æ£€æŸ¥åè®®
            if parsed.scheme not in cls.ALLOWED_SCHEMES:
                raise ValueError(f"ä¸å…è®¸çš„åè®®: {parsed.scheme}")

            # æ£€æŸ¥åŸŸå
            hostname = parsed.hostname or ''

            # ç¦æ­¢åˆ—è¡¨æ£€æŸ¥
            for blocked in cls.BLOCKED_DOMAINS:
                if hostname.startswith(blocked):
                    raise ValueError(f"ç¦æ­¢è®¿é—®çš„åŸŸå: {hostname}")

            # ä¸¥æ ¼æ¨¡å¼ï¼šç™½åå•æ£€æŸ¥
            if strict and not any(
                hostname.endswith(domain)
                for domain in cls.ALLOWED_DOMAINS
            ):
                raise ValueError(f"åŸŸåä¸åœ¨ç™½åå•ä¸­: {hostname}")

            return True

        except Exception as e:
            logger.warning(f"URL éªŒè¯å¤±è´¥: {url} - {e}")
            return False

# ä½¿ç”¨ URL éªŒè¯
def safe_scrape_validated(url: str, **kwargs):
    """éªŒè¯ URL åå†çˆ¬å–"""
    if not URLValidator.validate(url):
        raise ValueError(f"URL éªŒè¯å¤±è´¥: {url}")

    return app.scrape(url, **kwargs)
```

### 3. æ•°æ®è„±æ•

```python
import re

class DataSanitizer:
    """æ•°æ®è„±æ•å™¨"""

    # æ•æ„Ÿä¿¡æ¯æ­£åˆ™
    PATTERNS = {
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
        'credit_card': r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',
    }

    @classmethod
    def sanitize(cls, text: str, keep_structure: bool = True) -> str:
        """
        è„±æ•æ–‡æœ¬ä¸­çš„æ•æ„Ÿä¿¡æ¯

        å‚æ•°ï¼š
            text: åŸå§‹æ–‡æœ¬
            keep_structure: æ˜¯å¦ä¿ç•™ç»“æ„ï¼ˆå¦‚ abc@example.com -> ***@***.comï¼‰
        """
        for pattern_type, pattern in cls.PATTERNS.items():
            if keep_structure and pattern_type == 'email':
                # ä¿ç•™é‚®ç®±ç»“æ„
                text = re.sub(
                    pattern,
                    lambda m: cls._mask_email(m.group(0)),
                    text
                )
            else:
                # å®Œå…¨æ›¿æ¢
                text = re.sub(pattern, f'[{pattern_type.upper()}_REDACTED]', text)

        return text

    @staticmethod
    def _mask_email(email: str) -> str:
        """è„±æ•é‚®ç®±åœ°å€"""
        parts = email.split('@')
        if len(parts) != 2:
            return '[EMAIL_REDACTED]'

        username, domain = parts
        masked_username = username[0] + '***' if username else '***'

        domain_parts = domain.split('.')
        masked_domain = '.'.join([
            '***' if i < len(domain_parts) - 1 else part
            for i, part in enumerate(domain_parts)
        ])

        return f"{masked_username}@{masked_domain}"

# ä½¿ç”¨æ•°æ®è„±æ•
def scrape_and_sanitize(url: str):
    """çˆ¬å–å¹¶è„±æ•"""
    result = app.scrape(url, formats=['markdown'])
    result['markdown'] = DataSanitizer.sanitize(result['markdown'])
    return result
```

---

## ğŸ“Š ç›‘æ§ä¸æ—¥å¿—

### 1. å®æ—¶ç›‘æ§ä»ªè¡¨æ¿

```python
from collections import defaultdict
from datetime import datetime, timedelta

class DashboardMetrics:
    """ä»ªè¡¨æ¿æŒ‡æ ‡"""

    def __init__(self):
        self.metrics = defaultdict(lambda: {
            'count': 0,
            'success': 0,
            'failure': 0,
            'total_time': 0.0,
        })
        self.hourly_stats = defaultdict(int)

    def record(
        self,
        operation: str,
        success: bool,
        elapsed: float
    ):
        """è®°å½•æ“ä½œæŒ‡æ ‡"""
        m = self.metrics[operation]
        m['count'] += 1
        m['success' if success else 'failure'] += 1
        m['total_time'] += elapsed

        # å°æ—¶ç»Ÿè®¡
        hour_key = datetime.now().strftime('%Y-%m-%d %H:00')
        self.hourly_stats[hour_key] += 1

    def get_dashboard(self) -> Dict:
        """è·å–ä»ªè¡¨æ¿æ•°æ®"""
        dashboard = {}

        for operation, m in self.metrics.items():
            success_rate = (m['success'] / m['count'] * 100) if m['count'] > 0 else 0
            avg_time = m['total_time'] / m['count'] if m['count'] > 0 else 0

            dashboard[operation] = {
                'æ€»è¯·æ±‚': m['count'],
                'æˆåŠŸ': m['success'],
                'å¤±è´¥': m['failure'],
                'æˆåŠŸç‡': f"{success_rate:.1f}%",
                'å¹³å‡è€—æ—¶': f"{avg_time:.2f}s",
            }

        return dashboard

    def print_dashboard(self):
        """æ‰“å°ä»ªè¡¨æ¿"""
        print("\n" + "=" * 80)
        print("ğŸ“Š Firecrawl API å®æ—¶ç›‘æ§ä»ªè¡¨æ¿")
        print("=" * 80)

        dashboard = self.get_dashboard()
        for operation, stats in dashboard.items():
            print(f"\n{operation.upper()}:")
            for key, value in stats.items():
                print(f"  {key}: {value}")

        # å°æ—¶è¶‹åŠ¿
        print("\næ¯å°æ—¶è¯·æ±‚è¶‹åŠ¿:")
        for hour in sorted(self.hourly_stats.keys())[-24:]:  # æœ€è¿‘ 24 å°æ—¶
            bar = 'â–ˆ' * (self.hourly_stats[hour] // 10)
            print(f"  {hour}: {bar} ({self.hourly_stats[hour]})")

        print("=" * 80)

# å…¨å±€ä»ªè¡¨æ¿
dashboard = DashboardMetrics()

# å®šæ—¶æ‰“å°ï¼ˆæ¯ 5 åˆ†é’Ÿï¼‰
import threading

def print_dashboard_periodically():
    """å®šæ—¶æ‰“å°ä»ªè¡¨æ¿"""
    while True:
        time.sleep(300)  # 5 åˆ†é’Ÿ
        dashboard.print_dashboard()

dashboard_thread = threading.Thread(target=print_dashboard_periodically, daemon=True)
dashboard_thread.start()
```

### 2. å‘Šè­¦ç³»ç»Ÿ

```python
from enum import Enum

class AlertLevel(Enum):
    """å‘Šè­¦çº§åˆ«"""
    INFO = "ä¿¡æ¯"
    WARNING = "è­¦å‘Š"
    ERROR = "é”™è¯¯"
    CRITICAL = "ä¸¥é‡"

class AlertSystem:
    """å‘Šè­¦ç³»ç»Ÿ"""

    def __init__(
        self,
        webhook_url: Optional[str] = None,  # Slack/é’‰é’‰ Webhook
        email_config: Optional[Dict] = None
    ):
        self.webhook_url = webhook_url
        self.email_config = email_config
        self.alert_history = []

    def alert(
        self,
        level: AlertLevel,
        title: str,
        message: str,
        context: Optional[Dict] = None
    ):
        """å‘é€å‘Šè­¦"""
        alert = {
            'level': level,
            'title': title,
            'message': message,
            'context': context or {},
            'timestamp': datetime.now()
        }

        self.alert_history.append(alert)

        # æ ¹æ®çº§åˆ«å†³å®šæ˜¯å¦å‘é€é€šçŸ¥
        if level in [AlertLevel.ERROR, AlertLevel.CRITICAL]:
            self._send_notification(alert)

        # è®°å½•æ—¥å¿—
        logger.log(
            logging.ERROR if level == AlertLevel.ERROR else logging.WARNING,
            f"[{level.value}] {title}: {message}",
            extra=context
        )

    def _send_notification(self, alert: Dict):
        """å‘é€é€šçŸ¥ï¼ˆWebhook/é‚®ä»¶ï¼‰"""
        if self.webhook_url:
            try:
                import requests
                requests.post(self.webhook_url, json={
                    'text': f"[{alert['level'].value}] {alert['title']}\n{alert['message']}"
                })
            except Exception as e:
                logger.error(f"å‘é€å‘Šè­¦å¤±è´¥: {e}")

# ä½¿ç”¨å‘Šè­¦ç³»ç»Ÿ
alert_system = AlertSystem(
    webhook_url=os.getenv('ALERT_WEBHOOK_URL')
)

# ç¤ºä¾‹ï¼šæˆæœ¬å‘Šè­¦
def check_cost_alert():
    """æ£€æŸ¥æˆæœ¬å‘Šè­¦"""
    current_cost = cost_tracker.get_cost()

    if current_cost > 8.0:  # æ¥è¿‘æ¯æ—¥é¢„ç®— $10
        alert_system.alert(
            AlertLevel.WARNING,
            "API æˆæœ¬å‘Šè­¦",
            f"ä»Šæ—¥æˆæœ¬å·²è¾¾ ${current_cost:.2f}ï¼Œæ¥è¿‘é¢„ç®—ä¸Šé™",
            {'current_cost': current_cost, 'budget': 10.0}
        )

    if current_cost > 10.0:  # è¶…è¿‡é¢„ç®—
        alert_system.alert(
            AlertLevel.CRITICAL,
            "API æˆæœ¬è¶…æ”¯",
            f"ä»Šæ—¥æˆæœ¬ ${current_cost:.2f} å·²è¶…è¿‡é¢„ç®— $10.00",
            {'current_cost': current_cost, 'budget': 10.0}
        )
```

---

## ğŸ“ ä»£ç è§„èŒƒ

### 1. ç»Ÿä¸€çš„ API è°ƒç”¨å°è£…

```python
# utils/firecrawl_client.py
from typing import List, Dict, Optional, Union
from firecrawl import FirecrawlApp

class FirecrawlClient:
    """ç»Ÿä¸€çš„ Firecrawl å®¢æˆ·ç«¯"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv('FIRECRAWL_API_KEY')
        self.app = FirecrawlApp(api_key=self.api_key)

        # é›†æˆæ‰€æœ‰å·¥å…·
        self.cache = SmartCache()
        self.cost_tracker = CostTracker()
        self.monitor = PerformanceMonitor()
        self.alert = AlertSystem()

    def scrape(
        self,
        url: str,
        formats: List[str] = ['markdown'],
        use_cache: bool = True,
        **kwargs
    ) -> Dict:
        """
        çˆ¬å–å•ä¸ª URL

        å‚æ•°ï¼š
            url: ç›®æ ‡ URL
            formats: è¿”å›æ ¼å¼
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            **kwargs: å…¶ä»–å‚æ•°

        è¿”å›ï¼š
            çˆ¬å–ç»“æœå­—å…¸
        """
        # URL éªŒè¯
        if not URLValidator.validate(url):
            raise ValueError(f"æ— æ•ˆ URL: {url}")

        # å°è¯•ç¼“å­˜
        if use_cache:
            cached = self.cache.get(url, kwargs)
            if cached:
                return cached

        # æ‰§è¡Œçˆ¬å–ï¼ˆå¸¦ç›‘æ§ï¼‰
        with self.monitor.track('scrape'):
            try:
                result = self.app.scrape(
                    url=url,
                    formats=formats,
                    **kwargs
                )

                # è®°å½•æˆæœ¬
                self.cost_tracker.track('scrape', 1)

                # ä¿å­˜ç¼“å­˜
                if use_cache:
                    self.cache.set(url, kwargs, result)

                return result

            except Exception as e:
                # é”™è¯¯å¤„ç†
                error = ErrorHandler.handle_error(e, url)
                self.alert.alert(
                    AlertLevel.ERROR,
                    f"çˆ¬å–å¤±è´¥: {url}",
                    str(error)
                )
                raise error

    def batch_scrape(
        self,
        urls: List[str],
        formats: List[str] = ['markdown'],
        batch_size: int = 10,
        **kwargs
    ) -> List[Dict]:
        """
        æ‰¹é‡çˆ¬å–

        å‚æ•°ï¼š
            urls: URL åˆ—è¡¨
            formats: è¿”å›æ ¼å¼
            batch_size: æ‰¹æ¬¡å¤§å°
            **kwargs: å…¶ä»–å‚æ•°
        """
        # URL å»é‡
        deduplicator = URLDeduplicator()
        unique_urls = [
            url for url in urls
            if not deduplicator.is_duplicate_url(url)
        ]

        # åˆ†æ‰¹å¤„ç†
        optimizer = FirecrawlOptimizer(self.api_key, batch_size)
        results = optimizer.batch_scrape_optimized(
            unique_urls,
            formats=formats,
            **kwargs
        )

        # è®°å½•æˆæœ¬
        self.cost_tracker.track('scrape', len(results))

        return results

    def get_metrics(self) -> Dict:
        """è·å–æ‰€æœ‰æŒ‡æ ‡"""
        return {
            'cost': self.cost_tracker.get_report(),
            'performance': self.monitor.get_report(),
            'cache_stats': {
                # TODO: å®ç°ç¼“å­˜ç»Ÿè®¡
            }
        }

# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
firecrawl_client = FirecrawlClient()

# ä½¿ç”¨ç¤ºä¾‹
def scrape_hawaii_news():
    """çˆ¬å–å¤å¨å¤·æ–°é—»"""
    urls = [
        "https://www.hawaiinewsnow.com/",
        "https://www.staradvertiser.com/",
    ]

    results = firecrawl_client.batch_scrape(urls)

    # æ‰“å°æŒ‡æ ‡
    metrics = firecrawl_client.get_metrics()
    print(f"æˆæœ¬: ${metrics['cost']['total_cost']}")
    print(f"æ€§èƒ½: {metrics['performance']['avg_time']:.2f}s")

    return results
```

### 2. é…ç½®æ–‡ä»¶è§„èŒƒ

```python
# config/firecrawl_config.py
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class FirecrawlConfig:
    """Firecrawl é…ç½®"""

    # API é…ç½®
    api_key: str
    api_url: str = "https://api.firecrawl.dev"
    timeout: int = 60
    max_retries: int = 3

    # åŠŸèƒ½å¼€å…³
    enable_cache: bool = True
    enable_monitoring: bool = True
    enable_cost_tracking: bool = True

    # ç¼“å­˜é…ç½®
    cache_ttl: int = 3600
    cache_dir: str = "./cache"

    # æˆæœ¬é™åˆ¶
    daily_budget: float = 10.0
    monthly_budget: float = 200.0

    # æ€§èƒ½é…ç½®
    batch_size: int = 10
    max_workers: int = 5
    rate_limit: int = 10

    # å‘Šè­¦é…ç½®
    alert_webhook: Optional[str] = None
    alert_email: Optional[str] = None

    @classmethod
    def from_env(cls) -> 'FirecrawlConfig':
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            api_key=os.getenv('FIRECRAWL_API_KEY'),
            api_url=os.getenv('FIRECRAWL_API_URL', cls.api_url),
            timeout=int(os.getenv('FIRECRAWL_TIMEOUT', cls.timeout)),
            enable_cache=os.getenv('FIRECRAWL_ENABLE_CACHE', 'true').lower() == 'true',
            daily_budget=float(os.getenv('FIRECRAWL_DAILY_BUDGET', cls.daily_budget)),
            alert_webhook=os.getenv('FIRECRAWL_ALERT_WEBHOOK'),
        )

    def validate(self):
        """éªŒè¯é…ç½®"""
        if not self.api_key:
            raise ValueError("æœªè®¾ç½® FIRECRAWL_API_KEY")

        if self.daily_budget <= 0:
            raise ValueError("æ¯æ—¥é¢„ç®—å¿…é¡»å¤§äº 0")

        if self.batch_size < 1:
            raise ValueError("æ‰¹æ¬¡å¤§å°å¿…é¡»è‡³å°‘ä¸º 1")

        return True

# åŠ è½½é…ç½®
config = FirecrawlConfig.from_env()
config.validate()
```

---

## ğŸš¨ åº”æ€¥é¢„æ¡ˆ

### 1. API æ•…éšœåº”æ€¥

```python
class EmergencyFallback:
    """åº”æ€¥åå¤‡æ–¹æ¡ˆ"""

    def __init__(self):
        # å¤‡ç”¨ API å¯†é’¥
        self.backup_keys = [
            os.getenv('FIRECRAWL_API_KEY_BACKUP_1'),
            os.getenv('FIRECRAWL_API_KEY_BACKUP_2'),
        ]
        self.current_key_index = 0

        # é™çº§ç­–ç•¥
        self.degraded_mode = False

    def switch_to_backup_key(self):
        """åˆ‡æ¢åˆ°å¤‡ç”¨å¯†é’¥"""
        if self.current_key_index < len(self.backup_keys) - 1:
            self.current_key_index += 1
            new_key = self.backup_keys[self.current_key_index]
            print(f"âš ï¸ åˆ‡æ¢åˆ°å¤‡ç”¨å¯†é’¥ #{self.current_key_index + 1}")
            return FirecrawlApp(api_key=new_key)

        print("âŒ æ‰€æœ‰å¤‡ç”¨å¯†é’¥å·²è€—å°½ï¼Œå¯åŠ¨é™çº§æ¨¡å¼")
        self.degraded_mode = True
        return None

    def scrape_with_fallback(self, url: str, **kwargs):
        """å¸¦é™çº§çš„çˆ¬å–"""
        if self.degraded_mode:
            # é™çº§æ¨¡å¼ï¼šä½¿ç”¨ç®€å•çš„ requests
            return self._simple_scrape(url)

        try:
            return app.scrape(url, **kwargs)

        except Exception as e:
            # API æ•…éšœï¼Œå°è¯•åˆ‡æ¢å¯†é’¥
            backup_app = self.switch_to_backup_key()

            if backup_app:
                return backup_app.scrape(url, **kwargs)
            else:
                # é™çº§åˆ°ç®€å•çˆ¬å–
                return self._simple_scrape(url)

    @staticmethod
    def _simple_scrape(url: str) -> Dict:
        """ç®€å•çˆ¬å–ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        import requests
        from bs4 import BeautifulSoup

        response = requests.get(url, timeout=30)
        soup = BeautifulSoup(response.text, 'html.parser')

        # æå–ä¸»è¦å†…å®¹
        main_content = soup.find('main') or soup.find('article') or soup.body
        text = main_content.get_text(strip=True) if main_content else ""

        return {
            'url': url,
            'markdown': text,
            'method': 'fallback',
            'warning': 'ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼Œå†…å®¹å¯èƒ½ä¸å®Œæ•´'
        }

emergency = EmergencyFallback()
```

### 2. é€Ÿç‡é™åˆ¶åº”å¯¹

```python
class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨"""

    def __init__(self, max_requests: int = 100, time_window: int = 60):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = []

    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€è¯·æ±‚"""
        now = time.time()

        # æ¸…ç†è¿‡æœŸè¯·æ±‚
        self.requests = [
            req_time for req_time in self.requests
            if now - req_time < self.time_window
        ]

        # æ£€æŸ¥æ˜¯å¦è¶…é™
        if len(self.requests) >= self.max_requests:
            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
            oldest_request = min(self.requests)
            wait_time = self.time_window - (now - oldest_request)

            if wait_time > 0:
                print(f"â³ è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)

        # è®°å½•æœ¬æ¬¡è¯·æ±‚
        self.requests.append(time.time())

rate_limiter = RateLimiter(max_requests=100, time_window=60)

def rate_limited_scrape(url: str, **kwargs):
    """é€Ÿç‡é™åˆ¶çš„çˆ¬å–"""
    rate_limiter.wait_if_needed()
    return app.scrape(url, **kwargs)
```

---

## ğŸ“Œ å¿«é€Ÿå‚è€ƒ

### å¸¸ç”¨å‘½ä»¤

```bash
# å®‰è£…ä¾èµ–
pip install firecrawl-py requests python-dotenv

# è¿è¡Œæµ‹è¯•
python -m pytest tests/

# æŸ¥çœ‹æˆæœ¬æŠ¥å‘Š
python -m scripts.cost_report

# æ¸…ç†ç¼“å­˜
python -m scripts.clear_cache

# å¯¼å‡ºæ—¥å¿—
python -m scripts.export_logs --date 2025-10-27
```

### ç¯å¢ƒå˜é‡æ¸…å•

```bash
# å¿…éœ€
FIRECRAWL_API_KEY=fc-xxx

# å¯é€‰
FIRECRAWL_API_URL=https://api.firecrawl.dev
FIRECRAWL_TIMEOUT=60
FIRECRAWL_MAX_RETRIES=3
FIRECRAWL_ENABLE_CACHE=true
FIRECRAWL_CACHE_TTL=3600
FIRECRAWL_DAILY_BUDGET=10.0
FIRECRAWL_MONTHLY_BUDGET=200.0
FIRECRAWL_ALERT_WEBHOOK=https://hooks.slack.com/xxx
```

### æ€§èƒ½åŸºå‡†

| æ“ä½œ | å•æ¬¡è€—æ—¶ | æ‰¹é‡ (10ä¸ª) | æˆæœ¬ |
|------|---------|------------|------|
| Scrape | 1-3s | 10-15s | $0.005/é¡µ |
| Search | 2-5s | - | $0.01/æ¬¡ |
| Crawl | 30-60s | - | $0.004/é¡µ |

---

## âœ… æ£€æŸ¥æ¸…å•

### ä¸Šçº¿å‰æ£€æŸ¥

- [ ] API å¯†é’¥å·²é…ç½®ä¸”æœ‰æ•ˆ
- [ ] ç¯å¢ƒå˜é‡å®Œæ•´
- [ ] é”™è¯¯å¤„ç†å·²å®ç°
- [ ] æ—¥å¿—ç³»ç»Ÿå·²é…ç½®
- [ ] æˆæœ¬ç›‘æ§å·²å¯ç”¨
- [ ] ç¼“å­˜ç­–ç•¥å·²é…ç½®
- [ ] é€Ÿç‡é™åˆ¶å·²è®¾ç½®
- [ ] å‘Šè­¦ç³»ç»Ÿå·²æµ‹è¯•
- [ ] å¤‡ç”¨æ–¹æ¡ˆå·²å‡†å¤‡
- [ ] æ–‡æ¡£å·²æ›´æ–°

### æ—¥å¸¸æ£€æŸ¥

- [ ] æ£€æŸ¥ API é…é¢ä½¿ç”¨æƒ…å†µ
- [ ] æŸ¥çœ‹é”™è¯¯æ—¥å¿—
- [ ] æ£€æŸ¥æˆæœ¬æ˜¯å¦è¶…æ ‡
- [ ] æ¸…ç†è¿‡æœŸç¼“å­˜
- [ ] æ›´æ–°ç›‘æ§æ•°æ®
- [ ] å¤‡ä»½é‡è¦æ•°æ®

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [Firecrawl å®˜æ–¹æ–‡æ¡£](https://docs.firecrawl.dev)
- [Firecrawl ç”Ÿæ€ç³»ç»ŸæŒ‡å—](./FIRECRAWL_ECOSYSTEM_GUIDE.md)
- [HawaiiHub æŠ€æœ¯è§„èŒƒ](../AGENTS.md)

---

**ğŸ”¥ éµå®ˆè¿™äº›è§„èŒƒï¼Œè®© Firecrawl äº‘ API ä¸º HawaiiHub æä¾›å¼ºå¤§ä¸”ç¨³å®šçš„æ•°æ®é‡‡é›†èƒ½åŠ›ï¼ğŸŒ´**

---

_æœ€åæ›´æ–°: 2025-10-27_
_ç»´æŠ¤è€…: HawaiiHub AI Team_
_ç‰ˆæœ¬: v1.0.0_
