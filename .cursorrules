# Cursor AI è§„åˆ™ - FireShot é¡¹ç›®ï¼ˆFirecrawl ä¸“é¡¹ï¼‰

> **é¡¹ç›®**: FireShot - Firecrawl äº‘ API æœ€ä½³å®è·µå’Œ HawaiiHub æ•°æ®é‡‡é›† **æ›´æ–°æ—¶é—´**: 2025-10-27
> **ç»´æŠ¤è€…**: HawaiiHub AI Team

---

## ğŸŒ è¯­è¨€è¦æ±‚

- **æ‰€æœ‰è¾“å‡ºå¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡**ï¼ˆä»£ç æ³¨é‡Šã€æ–‡æ¡£ã€æ—¥å¿—ã€æç¤ºä¿¡æ¯ï¼‰
- è‹±æ–‡ä»…ç”¨äºï¼šä»£ç å˜é‡åã€å‡½æ•°åã€ç±»åã€åŒ…å
- æ–‡æ¡£ä¸­çš„æŠ€æœ¯æœ¯è¯­å¯ä¿ç•™è‹±æ–‡ï¼ˆå¦‚ Firecrawlã€MCPã€APIï¼‰

---

## ğŸ”¥ Firecrawl æ ¸å¿ƒåŸåˆ™

### 1. å·¥å…·é€‰æ‹©ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰

```python
# âœ… P0: ä¼˜å…ˆä½¿ç”¨ MCP å·¥å…·ï¼ˆæœ€ç®€å•ã€æœ€å¯é ï¼‰
result = mcp_firecrawl_firecrawl_scrape(
    url="https://example.com",
    formats=["markdown"],
    onlyMainContent=True  # âš ï¸ MCP å·¥å…·ä½¿ç”¨é©¼å³°å¼
)

# âœ… P1: Python SDK v2ï¼ˆéœ€è¦æ›´å¤šæ§åˆ¶æ—¶ï¼‰
from firecrawl import FirecrawlApp
app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
result = app.scrape(url, formats=["markdown"], only_main_content=True)  # âš ï¸ SDK ä½¿ç”¨ä¸‹åˆ’çº¿
# è®¿é—®ç»“æœï¼šresult.markdownï¼ˆå±æ€§ï¼‰ï¼Œä¸æ˜¯ result.get("markdown")

# âŒ é¿å…: ç›´æ¥ä½¿ç”¨ requests + BeautifulSoupï¼ˆä¸ç¨³å®šï¼‰
```

**é€‰æ‹©å†³ç­–æ ‘**ï¼š

- å¤æ‚é¡µé¢ï¼ˆå¤§é‡JSã€åŠ¨æ€åŠ è½½ï¼‰â†’ **MCP å·¥å…·**
- æ‰¹é‡çˆ¬å–ï¼ˆå·²çŸ¥ URL åˆ—è¡¨ï¼‰â†’ **Python SDK batch_scrape**
- æ•´ç«™çˆ¬å– â†’ **Python SDK crawl**
- æœç´¢ + çˆ¬å– â†’ **Python SDK search**

### 2. å¿…é¡»éµå®ˆçš„é…ç½®è§„èŒƒ

```python
# âœ… æ­£ç¡®ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡
import os
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("FIRECRAWL_API_KEY")

# âŒ é”™è¯¯ï¼šç¡¬ç¼–ç  API å¯†é’¥
api_key = "fc-xxxx"  # æ°¸è¿œä¸è¦è¿™æ ·åšï¼
```

**ç¯å¢ƒå˜é‡æ¸…å•**ï¼š

```bash
# å¿…éœ€
FIRECRAWL_API_KEY=fc-xxx

# æ¨èï¼ˆå¯†é’¥è½®æ¢ï¼‰
FIRECRAWL_API_KEY_BACKUP_1=fc-xxx
FIRECRAWL_API_KEY_BACKUP_2=fc-xxx
FIRECRAWL_API_KEY_BACKUP_3=fc-xxx

# å¯é€‰ï¼ˆæˆæœ¬æ§åˆ¶ï¼‰
FIRECRAWL_DAILY_BUDGET=10.0
FIRECRAWL_TIMEOUT=60
```

### 3. é”™è¯¯å¤„ç†æ¨¡å¼ï¼ˆå¼ºåˆ¶ï¼‰

```python
# âœ… æ­£ç¡®ï¼šå®Œæ•´çš„é”™è¯¯å¤„ç†
def safe_scrape(url: str, max_retries: int = 3) -> dict | None:
    """å®‰å…¨çˆ¬å–ï¼Œå¸¦é‡è¯•å’Œæ—¥å¿—"""
    for attempt in range(max_retries):
        try:
            result = app.scrape(url, formats=["markdown"], only_main_content=True)

            # éªŒè¯ç»“æœ
            if not result or not hasattr(result, "markdown"):
                raise ValueError("è¿”å›ç»“æœæ— æ•ˆ")

            logging.info(f"æˆåŠŸçˆ¬å–: {url}")
            return result

        except RequestTimeoutError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                logging.warning(f"è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... ({attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                logging.error(f"å¤±è´¥ï¼ˆ{max_retries}æ¬¡é‡è¯•åï¼‰: {url} - {e}")
                return None
        except Exception as e:
            logging.error(f"æœªçŸ¥é”™è¯¯: {url} - {e}")
            return None

# âŒ é”™è¯¯ï¼šæ²¡æœ‰é”™è¯¯å¤„ç†
result = app.scrape(url)  # å¯èƒ½å´©æºƒ
```

### 4. æ€§èƒ½ä¼˜åŒ–ï¼ˆå¿…é¡»ï¼‰

```python
# âœ… ä½¿ç”¨ç¼“å­˜ï¼ˆèŠ‚çœæˆæœ¬ 50%+ï¼‰
from functools import lru_cache
from datetime import datetime, timedelta

@lru_cache(maxsize=100)
def cached_scrape(url: str) -> str:
    """å¸¦ç¼“å­˜çš„çˆ¬å–ï¼ˆ1å°æ—¶æœ‰æ•ˆæœŸï¼‰"""
    cache_key = f"firecrawl:{url}"

    # æ£€æŸ¥ Redis ç¼“å­˜
    cached = redis_client.get(cache_key)
    if cached:
        return cached

    # çˆ¬å–æ–°æ•°æ®
    result = app.scrape(url, formats=["markdown"], only_main_content=True)
    content = result.markdown

    # å­˜å…¥ç¼“å­˜ï¼ˆ1å°æ—¶ï¼‰
    redis_client.setex(cache_key, 3600, content)
    return content

# âœ… æ‰¹é‡çˆ¬å–ï¼ˆæ›´é«˜æ•ˆï¼‰
urls = ["url1", "url2", "url3"]
results = app.batch_scrape(urls, formats=["markdown"])  # å¹¶å‘å¤„ç†

# âŒ é”™è¯¯ï¼šé€ä¸ªçˆ¬å–ï¼ˆæ…¢ + è´µï¼‰
for url in urls:
    result = app.scrape(url)  # ä¸²è¡Œå¤„ç†
```

---

## ğŸ Python ä»£ç è§„èŒƒ

### 1. ç±»å‹æ³¨è§£ï¼ˆå¼ºåˆ¶ï¼‰

```python
# âœ… æ­£ç¡®ï¼šæ‰€æœ‰å‡½æ•°å¿…é¡»æœ‰ç±»å‹æ³¨è§£
from typing import Optional, Dict, List

def scrape_news(
    url: str,
    formats: List[str] = ["markdown"],
    timeout: int = 60
) -> Optional[Dict[str, str]]:
    """
    çˆ¬å–æ–°é—»å†…å®¹

    Args:
        url: æ–°é—» URL
        formats: è¿”å›æ ¼å¼åˆ—è¡¨
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        åŒ…å« markdownã€html ç­‰å†…å®¹çš„å­—å…¸ï¼Œå¤±è´¥è¿”å› None
    """
    # ... å®ç°
    return {"markdown": content}

# âŒ é”™è¯¯ï¼šç¼ºå°‘ç±»å‹æ³¨è§£
def scrape_news(url, formats=["markdown"]):
    return {"markdown": content}
```

### 2. æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆå¼ºåˆ¶ï¼Œä¸­æ–‡ï¼‰

```python
# âœ… æ­£ç¡®ï¼šå®Œæ•´çš„ä¸­æ–‡ docstring
def analyze_articles(articles: List[Dict]) -> Dict[str, int]:
    """
    åˆ†ææ–‡ç« æ•°æ®ç»Ÿè®¡ä¿¡æ¯

    ç»Ÿè®¡å†…å®¹ï¼š
    - ä½œè€…åˆ†å¸ƒ
    - å‘å¸ƒæ—¶é—´åˆ†å¸ƒ
    - çƒ­é—¨å…³é”®è¯

    Args:
        articles: æ–‡ç« åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ç« åŒ…å« titleã€authorã€date ç­‰å­—æ®µ

    Returns:
        ç»Ÿè®¡ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
        - total_articles: æ€»æ–‡ç« æ•°
        - authors: ä½œè€…ç»Ÿè®¡ {ä½œè€…å: æ–‡ç« æ•°}
        - keywords: å…³é”®è¯ç»Ÿè®¡ {å…³é”®è¯: å‡ºç°æ¬¡æ•°}

    Example:
        >>> articles = [{"title": "æµ‹è¯•", "author": "å¼ ä¸‰", "date": "2025-10-27"}]
        >>> stats = analyze_articles(articles)
        >>> print(stats["total_articles"])
        1
    """
    # ... å®ç°
```

### 3. ä»£ç é£æ ¼ï¼ˆåŸºäº Ruffï¼‰

```python
# âœ… ä½¿ç”¨åŒå¼•å·ï¼ˆé¡¹ç›®æ ‡å‡†ï¼‰
message = "çˆ¬å–æˆåŠŸ"
url = "https://example.com"

# âœ… å¯¼å…¥é¡ºåºï¼šæ ‡å‡†åº“ â†’ ç¬¬ä¸‰æ–¹åº“ â†’ æœ¬åœ°æ¨¡å—
import json
import re
from datetime import datetime
from collections import defaultdict

from firecrawl import FirecrawlApp
from dotenv import load_dotenv

from .utils import parse_date
from .config import FIRECRAWL_API_KEY

# âœ… æ ¼å¼åŒ–ï¼šä½¿ç”¨ Black/Ruff
# æ¯è¡Œæœ€å¤š 88 å­—ç¬¦
# å‡½æ•°é—´ç©º 2 è¡Œ
# ç±»å®šä¹‰é—´ç©º 2 è¡Œ

# âœ… å‘½åè§„èŒƒ
class ArticleScraper:  # ç±»åï¼šå¤§é©¼å³°
    def __init__(self):
        self.api_key = ""  # å®ä¾‹å˜é‡ï¼šsnake_case

    def scrape_article(self) -> str:  # æ–¹æ³•åï¼šsnake_case
        return ""

FIRECRAWL_API_URL = "https://api.firecrawl.dev"  # å¸¸é‡ï¼šå¤§å†™+ä¸‹åˆ’çº¿
```

### 4. æµ‹è¯•è¦æ±‚ï¼ˆä½¿ç”¨ pytestï¼‰

```python
# âœ… æµ‹è¯•æ–‡ä»¶ï¼štests/test_scraper.py
import pytest
from unittest.mock import Mock, patch

def test_scrape_success():
    """æµ‹è¯•ï¼šæˆåŠŸçˆ¬å–æ–‡ç« """
    # Arrange
    mock_result = Mock()
    mock_result.markdown = "# æµ‹è¯•æ–‡ç« "

    # Act
    with patch("firecrawl.FirecrawlApp.scrape", return_value=mock_result):
        result = scrape_news("https://test.com")

    # Assert
    assert result is not None
    assert "markdown" in result
    assert result["markdown"] == "# æµ‹è¯•æ–‡ç« "

def test_scrape_timeout():
    """æµ‹è¯•ï¼šè¶…æ—¶é‡è¯•æœºåˆ¶"""
    with patch("firecrawl.FirecrawlApp.scrape", side_effect=RequestTimeoutError):
        result = safe_scrape("https://test.com", max_retries=2)

    assert result is None

# âŒ ä¸ä½¿ç”¨ unittest
# âŒ æµ‹è¯•å¿…é¡»æœ‰ docstring
```

---

## ğŸ“Š æ•°æ®å¤„ç†è§„èŒƒ

### 1. æ•°æ®ä¿å­˜æ ¼å¼

```python
# âœ… å¿…é¡»ä¿å­˜ 3 ç§æ ¼å¼ï¼ˆä¸‰è§’äº’éªŒï¼‰
def save_scraped_data(content: str, metadata: Dict) -> None:
    """ä¿å­˜çˆ¬å–çš„æ•°æ®åˆ°å¤šç§æ ¼å¼"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 1. åŸå§‹ Markdownï¼ˆäººç±»å¯è¯»ï¼‰
    with open(f"raw_{timestamp}.md", "w", encoding="utf-8") as f:
        f.write(f"# çˆ¬å–æ•°æ®\n\n")
        f.write(f"> æ—¶é—´: {datetime.now()}\n\n")
        f.write(content)

    # 2. ç»“æ„åŒ– JSONï¼ˆç¨‹åºå¤„ç†ï¼‰
    with open(f"data_{timestamp}.json", "w", encoding="utf-8") as f:
        json.dump({
            "content": content,
            "metadata": metadata,
            "scraped_at": datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)

    # 3. CSVï¼ˆåˆ†æå·¥å…·ï¼‰
    with open(f"data_{timestamp}.csv", "w", encoding="utf-8") as f:
        # ... CSV å¯¼å‡ºé€»è¾‘
```

### 2. æ•°æ®éªŒè¯

```python
# âœ… ä½¿ç”¨ Pydantic éªŒè¯æ•°æ®
from pydantic import BaseModel, HttpUrl, Field
from typing import Optional

class Article(BaseModel):
    """æ–‡ç« æ•°æ®æ¨¡å‹"""
    title: str = Field(..., min_length=1, max_length=200)
    url: HttpUrl
    author: str
    date: str  # ISO 8601 æ ¼å¼
    content: Optional[str] = None

    class Config:
        # å…è®¸é¢å¤–å­—æ®µ
        extra = "allow"

# ä½¿ç”¨
try:
    article = Article(
        title="æµ‹è¯•æ–‡ç« ",
        url="https://example.com",
        author="å¼ ä¸‰",
        date="2025-10-27"
    )
except ValidationError as e:
    logging.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
```

---

## ğŸ’° æˆæœ¬æ§åˆ¶è§„èŒƒ

### 1. è¯·æ±‚è®¡æ•°å’Œé¢„ç®—ç›‘æ§

```python
# âœ… å®ç°è¯·æ±‚è®¡æ•°å™¨
class FirecrawlClient:
    """Firecrawl å®¢æˆ·ç«¯ï¼ˆå¸¦æˆæœ¬æ§åˆ¶ï¼‰"""

    def __init__(self, api_key: str, daily_budget: float = 10.0):
        self.app = FirecrawlApp(api_key=api_key)
        self.daily_budget = daily_budget
        self.request_count = 0
        self.total_cost = 0.0

    def scrape(self, url: str, **kwargs) -> dict:
        """çˆ¬å–å¹¶è®°å½•æˆæœ¬"""
        # æ£€æŸ¥é¢„ç®—
        if self.total_cost >= self.daily_budget:
            raise BudgetExceededError(f"è¶…å‡ºæ¯æ—¥é¢„ç®—: ${self.daily_budget}")

        # æ‰§è¡Œçˆ¬å–
        result = self.app.scrape(url, **kwargs)

        # è®°å½•æˆæœ¬ï¼ˆå‡è®¾ $0.01/è¯·æ±‚ï¼‰
        cost = 0.01
        self.request_count += 1
        self.total_cost += cost

        logging.info(
            f"è¯·æ±‚ #{self.request_count} | "
            f"æˆæœ¬: ${cost:.4f} | "
            f"ç´¯è®¡: ${self.total_cost:.2f}/{self.daily_budget}"
        )

        return result
```

### 2. å¯†é’¥è½®æ¢

```python
# âœ… è‡ªåŠ¨å¯†é’¥è½®æ¢ï¼ˆçªç ´é€Ÿç‡é™åˆ¶ï¼‰
import itertools

class RotatingFirecrawlClient:
    """æ”¯æŒå¯†é’¥è½®æ¢çš„å®¢æˆ·ç«¯"""

    def __init__(self, api_keys: List[str]):
        self.api_keys = itertools.cycle(api_keys)  # å¾ªç¯ä½¿ç”¨
        self.current_key = next(self.api_keys)
        self.app = FirecrawlApp(api_key=self.current_key)

    def scrape(self, url: str, **kwargs) -> dict:
        """çˆ¬å–ï¼ˆå¤±è´¥è‡ªåŠ¨åˆ‡æ¢å¯†é’¥ï¼‰"""
        try:
            return self.app.scrape(url, **kwargs)
        except RateLimitError:
            # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥
            self.current_key = next(self.api_keys)
            self.app = FirecrawlApp(api_key=self.current_key)
            logging.info(f"åˆ‡æ¢å¯†é’¥: {self.current_key[:10]}...")
            return self.app.scrape(url, **kwargs)

# ä½¿ç”¨
client = RotatingFirecrawlClient([
    os.getenv("FIRECRAWL_API_KEY"),
    os.getenv("FIRECRAWL_API_KEY_BACKUP_1"),
    os.getenv("FIRECRAWL_API_KEY_BACKUP_2"),
    os.getenv("FIRECRAWL_API_KEY_BACKUP_3"),
])
```

---

## ğŸ—ï¸ é¡¹ç›®ç»“æ„è§„èŒƒ

```
FireShot/
â”œâ”€â”€ .env                          # ç¯å¢ƒå˜é‡ï¼ˆä¸æäº¤åˆ° Gitï¼‰
â”œâ”€â”€ .env.template                 # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”œâ”€â”€ .cursorrules                  # æœ¬æ–‡ä»¶
â”œâ”€â”€ .gitignore                    # Git å¿½ç•¥æ–‡ä»¶
â”‚
â”œâ”€â”€ requirements.txt              # Python ä¾èµ–
â”œâ”€â”€ pyproject.toml               # é¡¹ç›®é…ç½®ï¼ˆä½¿ç”¨ uv/poetryï¼‰
â”‚
â”œâ”€â”€ docs/                        # æ–‡æ¡£
â”‚   â”œâ”€â”€ FIRECRAWL_CLOUD_API_RULES.md
â”‚   â”œâ”€â”€ FIRECRAWL_CLOUD_SETUP_GUIDE.md
â”‚   â””â”€â”€ SETUP_COMPLETE.md
â”‚
â”œâ”€â”€ scripts/                     # å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ scrape_firecrawl_blog.py
â”‚   â”œâ”€â”€ analyze_firecrawl_blog.py
â”‚   â””â”€â”€ test_api_keys.py
â”‚
â”œâ”€â”€ src/                         # æºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper.py              # çˆ¬è™«æ ¸å¿ƒé€»è¾‘
â”‚   â”œâ”€â”€ parser.py               # æ•°æ®è§£æ
â”‚   â”œâ”€â”€ storage.py              # æ•°æ®å­˜å‚¨
â”‚   â””â”€â”€ utils.py                # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ tests/                       # æµ‹è¯•ä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_scraper.py
â”‚   â””â”€â”€ test_parser.py
â”‚
â”œâ”€â”€ data/                        # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/                    # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/              # å¤„ç†åæ•°æ®
â”‚   â””â”€â”€ cache/                  # ç¼“å­˜
â”‚
â””â”€â”€ logs/                        # æ—¥å¿—æ–‡ä»¶
    â”œâ”€â”€ firecrawl.log
    â””â”€â”€ errors.log
```

---

## ğŸ“ Git æäº¤è§„èŒƒ

### Conventional Commitsï¼ˆå¼ºåˆ¶ï¼‰

```bash
# æ ¼å¼ï¼š<ç±»å‹>(<èŒƒå›´>): <æè¿°>

# âœ… æ­£ç¡®çš„æäº¤æ¶ˆæ¯
git commit -m "feat(scraper): æ·»åŠ  Firecrawl MCP å·¥å…·æ”¯æŒ"
git commit -m "fix(parser): ä¿®å¤æ–‡ç« æ—¥æœŸè§£æé”™è¯¯"
git commit -m "docs(api): æ›´æ–° API å¯†é’¥é…ç½®æŒ‡å—"
git commit -m "refactor(storage): ä¼˜åŒ–æ•°æ®å­˜å‚¨æ ¼å¼"
git commit -m "perf(cache): å®ç° Redis ç¼“å­˜æœºåˆ¶ï¼ŒèŠ‚çœ 50% æˆæœ¬"

# âŒ é”™è¯¯çš„æäº¤æ¶ˆæ¯
git commit -m "æ›´æ–°ä»£ç "
git commit -m "fix bug"
```

**ç±»å‹æ¸…å•**ï¼š

- `feat`: æ–°åŠŸèƒ½
- `fix`: Bug ä¿®å¤
- `docs`: æ–‡æ¡£æ›´æ–°
- `refactor`: ä»£ç é‡æ„
- `perf`: æ€§èƒ½ä¼˜åŒ–
- `test`: æµ‹è¯•ç›¸å…³
- `chore`: æ„å»º/å·¥å…·é“¾
- `style`: ä»£ç æ ¼å¼ï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰

---

## ğŸ¯ HawaiiHub ä¸“é¡¹è§„èŒƒ

### 1. æ–°é—»çˆ¬å–æ¨¡æ¿

```python
# âœ… æ ‡å‡†çš„æ–°é—»çˆ¬å–æµç¨‹
from typing import List, Dict
import logging

HAWAII_NEWS_SOURCES = [
    "https://www.hawaiinewsnow.com/",
    "https://www.staradvertiser.com/",
    "https://www.civilbeat.org/",
]

def scrape_hawaii_news(sources: List[str]) -> List[Dict]:
    """
    çˆ¬å–å¤å¨å¤·æ–°é—»

    Args:
        sources: æ–°é—»æº URL åˆ—è¡¨

    Returns:
        æ–°é—»æ–‡ç« åˆ—è¡¨
    """
    articles = []

    for source in sources:
        try:
            # 1. çˆ¬å–é¦–é¡µ
            result = app.scrape(
                url=source,
                formats=["markdown"],
                only_main_content=True  # SDK v2 ä½¿ç”¨ä¸‹åˆ’çº¿
            )

            # 2. æå–æ–‡ç« é“¾æ¥
            links = extract_article_links(result.markdown)

            # 3. æ‰¹é‡çˆ¬å–æ–‡ç« å†…å®¹
            article_results = batch_scrape(links[:10])  # é™åˆ¶ 10 ç¯‡

            # 4. è§£æå’Œå­˜å‚¨
            for article in article_results:
                parsed = parse_news_article(article)
                articles.append(parsed)

            logging.info(f"æˆåŠŸçˆ¬å– {source}: {len(article_results)} ç¯‡æ–‡ç« ")

        except Exception as e:
            logging.error(f"çˆ¬å–å¤±è´¥ {source}: {e}")
            continue

    return articles
```

### 2. æ•°æ®æ¸…æ´—è§„èŒƒ

```python
# âœ… æ–°é—»æ•°æ®æ¸…æ´—
def clean_news_content(content: str) -> str:
    """
    æ¸…æ´—æ–°é—»å†…å®¹

    ç§»é™¤ï¼š
    - å¹¿å‘Š
    - å¯¼èˆªæ 
    - ç›¸å…³æ–‡ç« æ¨è
    - ç¤¾äº¤åª’ä½“æŒ‰é’®
    """
    # ä½¿ç”¨ Firecrawl çš„ only_main_content å·²ç»åšäº†åŸºç¡€æ¸…æ´—
    # è¿™é‡Œåªéœ€è¦é¢å¤–å¤„ç†

    # ç§»é™¤ç‰¹å®šæ¨¡å¼
    patterns_to_remove = [
        r"Subscribe to.*newsletter",
        r"Advertisement",
        r"Related Articles",
        r"Share on.*",
    ]

    for pattern in patterns_to_remove:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    # è§„èŒƒåŒ–ç©ºç™½
    content = re.sub(r"\n{3,}", "\n\n", content)

    return content.strip()
```

---

## ğŸ”§ å¼€å‘å·¥å…·é…ç½®

### 1. VSCode/Cursor è®¾ç½®

```json
// .vscode/settings.json
{
  "python.linting.enabled": true,
  "python.linting.ruffEnabled": true,
  "python.formatting.provider": "black",
  "python.testing.pytestEnabled": true,
  "editor.formatOnSave": true,
  "editor.codeActionsOnSave": {
    "source.organizeImports": true
  },
  "[python]": {
    "editor.rulers": [88],
    "editor.tabSize": 4
  }
}
```

### 2. Ruff é…ç½®

```toml
# pyproject.toml
[tool.ruff]
line-length = 88
target-version = "py311"

[tool.ruff.lint]
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "B",   # flake8-bugbear
    "C4",  # flake8-comprehensions
]

[tool.ruff.format]
quote-style = "double"  # å¼ºåˆ¶åŒå¼•å·
```

### 3. Pre-commit é’©å­

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.9
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
```

---

## ğŸ“š å¿…è¯»æ–‡æ¡£ä¼˜å…ˆçº§

### P0ï¼ˆç«‹å³é˜…è¯»ï¼‰

1. **SDK_CONFIGURATION_COMPLETE.md** - SDK é…ç½®æ€»ç»“ï¼ˆ2025-10-27 æœ€æ–°ï¼‰
2. **FIRECRAWL_CLOUD_SETUP_GUIDE.md** - 10 åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹
3. **FIRECRAWL_BLOG_SCRAPING_SUMMARY.md** - å®æˆ˜æ¡ˆä¾‹

### P1ï¼ˆæœ¬å‘¨é˜…è¯»ï¼‰

4. **FIRECRAWL_CLOUD_API_RULES.md** - å®Œæ•´ API è§„èŒƒ
5. **FIRECRAWL_ECOSYSTEM_GUIDE.md** - ç”Ÿæ€ç³»ç»Ÿæ¦‚è§ˆ
6. **Firecrawlæ›´æ–°æ—¥å¿—æ±‡æ€».md** - v1.0 â†’ v2.4.0 å®Œæ•´æ¼”è¿›

---

## ğŸ†• Firecrawl SDK v2 é‡è¦å˜åŒ–ï¼ˆ2025-10-27ï¼‰

### å‘½åçº¦å®šå˜åŒ–

```python
# âœ… SDK v2 æ­£ç¡®å†™æ³•ï¼ˆä¸‹åˆ’çº¿å‘½åï¼‰
result = app.scrape(
    url="https://example.com",
    formats=["markdown"],
    only_main_content=True,    # âœ… ä¸‹åˆ’çº¿
    max_age=172800000,          # âœ… ä¸‹åˆ’çº¿
    block_ads=True,             # âœ… ä¸‹åˆ’çº¿
    skip_tls_verification=True, # âœ… ä¸‹åˆ’çº¿
    remove_base64_images=True   # âœ… ä¸‹åˆ’çº¿
)

# âŒ é”™è¯¯å†™æ³•ï¼ˆé©¼å³°å¼ï¼Œv1 æ—§ç‰ˆæœ¬ï¼‰
result = app.scrape(
    url="https://example.com",
    formats=["markdown"],
    onlyMainContent=True,       # âŒ é©¼å³°å¼ä¼šæŠ¥é”™
    maxAge=172800000,           # âŒ é©¼å³°å¼ä¼šæŠ¥é”™
)
```

### è¿”å›å€¼ç±»å‹å˜åŒ–

```python
# âœ… SDK v2 æ­£ç¡®å†™æ³•ï¼ˆDocument å¯¹è±¡ï¼‰
result = app.scrape(url="https://example.com", formats=["markdown"])
content = result.markdown       # âœ… å±æ€§è®¿é—®
title = result.metadata.title   # âœ… å…ƒæ•°æ®è®¿é—®
url = result.url                # âœ… URL è®¿é—®

# âŒ é”™è¯¯å†™æ³•ï¼ˆå­—å…¸è®¿é—®ï¼Œv1 æ—§ç‰ˆæœ¬ï¼‰
content = result.get("markdown")      # âŒ ä¼šæŠ¥é”™ï¼š'Document' object has no attribute 'get'
content = result["markdown"]          # âŒ ä¼šæŠ¥é”™
```

### æ‰¹é‡çˆ¬å–è¿”å›å€¼

```python
# âœ… æ­£ç¡®å¤„ç† batch_scrape è¿”å›å€¼
results = app.batch_scrape(urls=["url1", "url2", "url3"], formats=["markdown"])

for item in results:
    if isinstance(item, tuple):
        success, result = item
        if success:
            print(result.markdown)
    else:
        print(item.markdown)
```

### v2 æ–°åŠŸèƒ½

1. **é»˜è®¤ç¼“å­˜**: `max_age=172800000`ï¼ˆ2å¤©ï¼‰å¯èŠ‚çœ 50%+ æˆæœ¬
2. **Summary æ ¼å¼**: `formats=["summary"]` ç›´æ¥è·å–æ‘˜è¦
3. **æœç´¢åˆ†ç±»**:
   - `categories=["github"]` - GitHub æœç´¢
   - `categories=["research"]` - å­¦æœ¯æœç´¢ï¼ˆarXivã€Natureã€IEEEã€PubMedï¼‰
   - `categories=["pdf"]` - PDF æ–‡æ¡£æœç´¢
4. **æ™ºèƒ½çˆ¬å–**: ä¼ å…¥ prompt å‚æ•°è‡ªåŠ¨æ¨å¯¼è·¯å¾„å’Œé™åˆ¶

---

## âš ï¸ ç¦æ­¢äº‹é¡¹

### ç»å¯¹ç¦æ­¢

1. âŒ **ç¡¬ç¼–ç  API å¯†é’¥**åˆ°ä»£ç ä¸­
2. âŒ **æäº¤ .env æ–‡ä»¶**åˆ° Git
3. âŒ **è·³è¿‡é”™è¯¯å¤„ç†**ï¼ˆæ‰€æœ‰å¤–éƒ¨è°ƒç”¨å¿…é¡» try-exceptï¼‰
4. âŒ **æ— é™é‡è¯•**ï¼ˆå¿…é¡»è®¾ç½® max_retriesï¼‰
5. âŒ **å¿½ç•¥æˆæœ¬ç›‘æ§**ï¼ˆå¿…é¡»è®°å½•æ¯æ¬¡ API è°ƒç”¨ï¼‰
6. âŒ **ä½¿ç”¨å•å¼•å·**ï¼ˆPython é¡¹ç›®ç»Ÿä¸€åŒå¼•å·ï¼‰
7. âŒ **ç¼ºå°‘ç±»å‹æ³¨è§£**ï¼ˆæ‰€æœ‰å‡½æ•°å¿…é¡»æœ‰å®Œæ•´ç±»å‹ï¼‰
8. âŒ **ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²**ï¼ˆæ‰€æœ‰å…¬å¼€å‡½æ•°/ç±»å¿…é¡»æœ‰ docstringï¼‰

### å¼ºçƒˆä¸æ¨è

1. âš ï¸ ä¸æ£€æŸ¥ç¼“å­˜ç›´æ¥çˆ¬å–ï¼ˆæµªè´¹æˆæœ¬ï¼‰
2. âš ï¸ ä¸²è¡Œå¤„ç†å¤§é‡ URLï¼ˆä½¿ç”¨ batch_scrapeï¼‰
3. âš ï¸ ä¸è®°å½•æ—¥å¿—ï¼ˆéš¾ä»¥è°ƒè¯•ï¼‰
4. âš ï¸ ä¸éªŒè¯æ•°æ®æ ¼å¼ï¼ˆä½¿ç”¨ Pydanticï¼‰

---

## ğŸ“ æœ€ä½³å®è·µæ€»ç»“

### çˆ¬å–ç­–ç•¥

1. **å…ˆ Mapï¼Œå Crawl/Scrape**

   ```python
   # 1. å‘ç°æ‰€æœ‰ URL
   urls = app.map(url="https://example.com")

   # 2. æ‰¹é‡çˆ¬å–
   results = app.batch_scrape(urls['links'][:100])
   ```

2. **ä½¿ç”¨ Search å‘ç°å†…å®¹**

   ```python
   # æœç´¢ + çˆ¬å–ä¸€æ­¥å®Œæˆ
   results = app.search(
       query="å¤å¨å¤· åäºº é¤å…",
       sources=[{"type": "web"}],
       limit=10,
       scrapeOptions={"formats": ["markdown"]}
   )
   ```

3. **åˆç†è®¾ç½®çˆ¬å–é¢‘ç‡**

   ```python
   import time

   for url in urls:
       result = scrape(url)
       time.sleep(1)  # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
   ```

### æ•°æ®ç®¡ç†

1. **åˆ†ç¦»åŸå§‹æ•°æ®å’Œå¤„ç†æ•°æ®**
2. **ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶**ï¼ˆæ–‡ä»¶ååŠ æ—¶é—´æˆ³ï¼‰
3. **å®šæœŸæ¸…ç†ç¼“å­˜**ï¼ˆé¿å…å­˜å‚¨çˆ†ç‚¸ï¼‰
4. **å¤‡ä»½é‡è¦æ•°æ®**ï¼ˆå¼‚åœ°å¤‡ä»½ï¼‰

### ç›‘æ§å‘Šè­¦

1. **æˆæœ¬ç›‘æ§**ï¼šæ¯æ—¥æ£€æŸ¥ API ä½¿ç”¨é‡
2. **é”™è¯¯å‘Šè­¦**ï¼šè¶…æ—¶ > 3 æ¬¡å‘é€ Slack é€šçŸ¥
3. **æ€§èƒ½è·Ÿè¸ª**ï¼šè®°å½•å¹³å‡å“åº”æ—¶é—´

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨æ¸…å•

æ–°æˆå‘˜å…¥èŒæ—¶ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œï¼š

```bash
# 1. å…‹éš†é¡¹ç›®
cd /Users/zhiledeng/Downloads/FireShot

# 2. å®‰è£…ä¾èµ–
pip3 install --break-system-packages firecrawl-py python-dotenv requests pydantic

# 3. é…ç½®ç¯å¢ƒå˜é‡
cp env.template .env
# ç¼–è¾‘ .envï¼Œå¡«å…¥ API å¯†é’¥

# 4. æµ‹è¯• API å¯†é’¥
python3 test_api_keys.py

# 5. è¿è¡Œç¤ºä¾‹
python3 quick_start.py

# 6. é˜…è¯»æ–‡æ¡£
cat FIRECRAWL_CLOUD_SETUP_GUIDE.md
cat SETUP_COMPLETE.md

# 7. å¼€å§‹å¼€å‘
# å‚è€ƒ scrape_firecrawl_blog.py å’Œ analyze_firecrawl_blog.py
```

---

## ğŸ“ è·å–å¸®åŠ©

### é‡åˆ°é—®é¢˜ï¼Ÿ

1. **æŸ¥çœ‹æ–‡æ¡£**: `docs/` ç›®å½•ä¸‹çš„æ‰€æœ‰ Markdown æ–‡ä»¶
2. **è¿è¡Œè¯Šæ–­**: `python3 test_api_keys.py`
3. **æŸ¥çœ‹æ—¥å¿—**: `tail -f logs/firecrawl.log`
4. **å®˜æ–¹èµ„æº**:
   - æ–‡æ¡£: https://docs.firecrawl.dev/
   - Discord: https://discord.gg/firecrawl
   - GitHub: https://github.com/mendableai/firecrawl

---

**ç‰ˆæœ¬**: v1.0.0 **æœ€åæ›´æ–°**: 2025-10-27 **ç»´æŠ¤è€…**: HawaiiHub AI Team **é€‚ç”¨é¡¹ç›®**: FireShot +
HawaiiHub
