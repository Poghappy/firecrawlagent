#!/usr/bin/env python3
"""Firecrawl Python SDK - Crawl çˆ¬å–ç¤ºä¾‹.

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Firecrawl SDK è¿›è¡Œæ·±åº¦çˆ¬å–ã€‚
"""

from __future__ import annotations

import json
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import NoReturn

from dotenv import load_dotenv
from firecrawl import FirecrawlApp


def crawl_example() -> None:
    """Crawl çˆ¬å–ç¤ºä¾‹."""
    print("=" * 60)
    print("  Firecrawl SDK - Crawl æ·±åº¦çˆ¬å–ç¤ºä¾‹")
    print("=" * 60)

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    # åˆå§‹åŒ– Firecrawl
    app = FirecrawlApp()

    # ç¤ºä¾‹ 1: ç®€å•çˆ¬å–ï¼ˆé˜»å¡å¼ï¼Œç­‰å¾…å®Œæˆï¼‰
    print("\nğŸ“ ç¤ºä¾‹ 1: ç®€å•çˆ¬å–ï¼ˆé™åˆ¶5ä¸ªé¡µé¢ï¼‰")
    print("-" * 60)

    job = app.crawl(
        url="https://docs.firecrawl.dev",
        limit=5,
        poll_interval=1,  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
        timeout=120,  # æœ€å¤šç­‰å¾…120ç§’
    )

    print("âœ… çˆ¬å–å®Œæˆ")
    print(f"ğŸ“Š çŠ¶æ€: {job.status}")
    print(f"ğŸ“„ é¡µé¢æ•°: {job.completed}/{job.total}")
    print("\nğŸ”— çˆ¬å–çš„é¡µé¢:")

    for i, doc in enumerate(job.data, 1):
        print(f"  {i}. {doc.metadata.source_url}")
        print(f"     æ ‡é¢˜: {doc.metadata.title}")
        print(f"     å†…å®¹: {len(doc.markdown)} å­—ç¬¦")

    # ç¤ºä¾‹ 2: éé˜»å¡çˆ¬å–ï¼ˆå¯åŠ¨åæ£€æŸ¥ï¼‰
    print("\nğŸ“ ç¤ºä¾‹ 2: éé˜»å¡çˆ¬å–ï¼ˆå¯åŠ¨åæ‰‹åŠ¨æ£€æŸ¥ï¼‰")
    print("-" * 60)

    # å¯åŠ¨ä»»åŠ¡
    started_job = app.start_crawl(
        url="https://docs.firecrawl.dev/introduction",
        limit=3,
        scrape_options={"formats": ["markdown"], "only_main_content": True},
    )

    print("âœ… ä»»åŠ¡å·²å¯åŠ¨")
    print(f"ğŸ†” ä»»åŠ¡ ID: {started_job.id}")
    print("â³ æ­£åœ¨çˆ¬å–...")

    # æ£€æŸ¥çŠ¶æ€
    while True:
        status = app.get_crawl_status(started_job.id)

        total = status.total if status.total > 0 else 1
        progress = (status.completed / total) * 100

        print(
            f"   è¿›åº¦: {status.completed}/{status.total} "
            f"({progress:.0f}%) - çŠ¶æ€: {status.status}"
        )

        if status.status in ["completed", "failed"]:
            break

        time.sleep(2)

    if status.status == "completed":
        print("\nâœ… çˆ¬å–å®Œæˆ")
        print(f"ğŸ“„ è·å–äº† {len(status.data)} ä¸ªé¡µé¢")
    else:
        print(f"\nâŒ çˆ¬å–å¤±è´¥: {status.status}")

    # ç¤ºä¾‹ 3: é«˜çº§çˆ¬å–é€‰é¡¹
    print("\nğŸ“ ç¤ºä¾‹ 3: é«˜çº§çˆ¬å–é€‰é¡¹ï¼ˆè·¯å¾„è¿‡æ»¤ã€æ·±åº¦æ§åˆ¶ï¼‰")
    print("-" * 60)

    job = app.crawl(
        url="https://docs.firecrawl.dev",
        limit=10,
        # è·¯å¾„è¿‡æ»¤
        include_paths=["/sdks/*", "/features/*"],
        exclude_paths=["/api-reference/*"],
        # åŸŸåæ§åˆ¶
        allow_subdomains=False,
        allow_external_links=False,
        # å»é‡
        ignore_query_parameters=True,
        # æ·±åº¦å’Œå¹¶å‘
        max_discovery_depth=2,
        max_concurrency=3,
        # å»¶è¿Ÿï¼ˆé¿å…é€Ÿç‡é™åˆ¶ï¼‰
        delay=500,  # 500æ¯«ç§’
        # Scrape é€‰é¡¹
        scrape_options={
            "formats": ["markdown"],
            "only_main_content": True,
            "remove_base64_images": True,
        },
        poll_interval=2,
        timeout=180,
    )

    print("âœ… çˆ¬å–å®Œæˆ")
    data_count = len(job.data)
    print(f"ğŸ“„ é¡µé¢æ•°: {data_count}")

    # åˆ†æç»“æœ
    total_chars = sum(len(doc.markdown) for doc in job.data)
    avg_chars = total_chars // data_count if data_count > 0 else 0

    print(f"ğŸ“Š æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print(f"ğŸ“Š å¹³å‡æ¯é¡µ: {avg_chars:,} å­—ç¬¦")

    # ä¿å­˜ç»“æœ
    output_dir = Path(__file__).parent.parent / "data" / "crawl_results"
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"crawl_{timestamp}.json"

    results = []
    for doc in job.data:
        results.append(
            {
                "url": doc.metadata.source_url,
                "title": doc.metadata.title,
                "content": doc.markdown,
                "content_length": len(doc.markdown),
                "scraped_at": datetime.now().isoformat(),
            }
        )

    output_path = Path(output_file)
    output_path.write_text(
        json.dumps(results, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    print("\n" + "=" * 60)
    print("  âœ… æ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
    print("=" * 60)


def run_example() -> NoReturn:
    """è¿è¡Œç¤ºä¾‹å¹¶å¤„ç†å¼‚å¸¸."""
    try:
        crawl_example()
        sys.exit(0)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except OSError as e:
        print(f"\nâŒ ç½‘ç»œé”™è¯¯: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    run_example()
