#!/usr/bin/env python3
"""Firecrawl Python SDK - Batch Scrape æ‰¹é‡é‡‡é›†ç¤ºä¾‹.

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Firecrawl SDK è¿›è¡Œæ‰¹é‡é‡‡é›†ã€‚
"""

from __future__ import annotations

import json
import sys
from datetime import datetime
from pathlib import Path
from typing import NoReturn

from dotenv import load_dotenv
from firecrawl import FirecrawlApp


def batch_scrape_example() -> None:
    """Batch Scrape æ‰¹é‡é‡‡é›†ç¤ºä¾‹."""
    print("=" * 60)
    print("  Firecrawl SDK - Batch Scrape æ‰¹é‡é‡‡é›†ç¤ºä¾‹")
    print("=" * 60)

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    # åˆå§‹åŒ– Firecrawl
    app = FirecrawlApp()

    # ç¤ºä¾‹ 1: ç®€å•æ‰¹é‡çˆ¬å–
    print("\nğŸ“ ç¤ºä¾‹ 1: æ‰¹é‡çˆ¬å–å¤šä¸ªé¡µé¢")
    print("-" * 60)

    urls = [
        "https://firecrawl.dev",
        "https://docs.firecrawl.dev",
        "https://docs.firecrawl.dev/introduction",
    ]

    print(f"ğŸ”— å¾…çˆ¬å– URL ({len(urls)} ä¸ª):")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")

    print("\nâ³ å¼€å§‹æ‰¹é‡çˆ¬å–...")

    job = app.batch_scrape(
        urls=urls,
        formats=["markdown"],
        poll_interval=1,
        timeout=60,
    )

    print("\nâœ… æ‰¹é‡çˆ¬å–å®Œæˆ")
    print(f"ğŸ“Š çŠ¶æ€: {job.status}")
    print(f"ğŸ“„ å®Œæˆ: {job.completed}/{job.total}")

    print("\nğŸ“‹ ç»“æœ:")
    for i, doc in enumerate(job.data, 1):
        print(f"\n  {i}. {doc.metadata.source_url}")
        print(f"     æ ‡é¢˜: {doc.metadata.title}")
        print(f"     å†…å®¹: {len(doc.markdown)} å­—ç¬¦")
        print(f"     é¢„è§ˆ: {doc.markdown[:100]}...")

    # ç¤ºä¾‹ 2: æ‰¹é‡çˆ¬å– Firecrawl ç›¸å…³é¡µé¢
    print("\nğŸ“ ç¤ºä¾‹ 2: æ‰¹é‡çˆ¬å– Firecrawl æ–‡æ¡£é¡µé¢")
    print("-" * 60)

    doc_urls = [
        "https://docs.firecrawl.dev/introduction",
        "https://docs.firecrawl.dev/features/scrape",
        "https://docs.firecrawl.dev/features/crawl",
        "https://docs.firecrawl.dev/features/map",
        "https://docs.firecrawl.dev/features/batch-scrape",
    ]

    print(f"ğŸ”— æ–‡æ¡£ URL ({len(doc_urls)} ä¸ª)")
    print("â³ æ‰¹é‡çˆ¬å–ä¸­...")

    job = app.batch_scrape(
        urls=doc_urls,
        formats=["markdown"],
        poll_interval=2,
        timeout=120,
    )

    print(f"\nâœ… çˆ¬å–å®Œæˆ: {job.completed}/{job.total}")

    # åˆ†æç»“æœ
    total_chars = sum(len(doc.markdown) for doc in job.data)
    data_count = len(job.data)
    avg_chars = total_chars // data_count if data_count > 0 else 0

    print("\nğŸ“Š ç»Ÿè®¡:")
    print(f"  æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print(f"  å¹³å‡æ¯é¡µ: {avg_chars:,} å­—ç¬¦")

    if job.data:
        max_len = max(len(doc.markdown) for doc in job.data)
        min_len = min(len(doc.markdown) for doc in job.data)
        print(f"  æœ€é•¿é¡µé¢: {max_len:,} å­—ç¬¦")
        print(f"  æœ€çŸ­é¡µé¢: {min_len:,} å­—ç¬¦")

    # ç¤ºä¾‹ 3: æ™ºèƒ½æ‰¹é‡å¤„ç†ï¼ˆåˆ†æ‰¹ + é”™è¯¯å¤„ç†ï¼‰
    print("\nğŸ“ ç¤ºä¾‹ 3: æ™ºèƒ½æ‰¹é‡å¤„ç†ï¼ˆå¤§åˆ—è¡¨åˆ†æ‰¹å¤„ç†ï¼‰")
    print("-" * 60)

    # æ¨¡æ‹Ÿå¤§é‡ URL
    large_url_list = [
        f"https://docs.firecrawl.dev/sdks/{sdk}"
        for sdk in ["overview", "python", "node"]
    ] + [
        f"https://docs.firecrawl.dev/features/{feature}"
        for feature in ["scrape", "crawl", "map", "batch-scrape", "extract"]
    ]

    print(f"ğŸ”— å¾…çˆ¬å–: {len(large_url_list)} ä¸ª URL")

    # åˆ†æ‰¹å¤„ç†
    batch_size = 3
    all_results = []

    for i in range(0, len(large_url_list), batch_size):
        batch = large_url_list[i : i + batch_size]
        batch_num = i // batch_size + 1

        print(f"\nâ³ å¤„ç†æ‰¹æ¬¡ {batch_num} ({len(batch)} ä¸ª URL)...")

        try:
            job = app.batch_scrape(
                urls=batch,
                formats=["markdown"],
                poll_interval=1,
                timeout=60,
            )

            all_results.extend(job.data)
            print(f"   âœ… æ‰¹æ¬¡ {batch_num}: {len(job.data)} ä¸ªé¡µé¢")

        except OSError as e:
            print(f"   âŒ æ‰¹æ¬¡ {batch_num} ç½‘ç»œé”™è¯¯: {e}")

            # é€ä¸ªé‡è¯•
            print("   ğŸ”„ é€ä¸ªé‡è¯•...")
            for url in batch:
                try:
                    result = app.scrape(url, formats=["markdown"])
                    all_results.append(result)
                    print(f"     âœ… {url}")
                except OSError as e2:
                    print(f"     âŒ {url}: {e2}")

    print("\nâœ… æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ")
    print(f"ğŸ“„ æˆåŠŸ: {len(all_results)}/{len(large_url_list)} ä¸ªé¡µé¢")

    # ä¿å­˜ç»“æœ
    output_dir = Path(__file__).parent.parent / "data" / "batch_results"
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"batch_{timestamp}.json"

    results = []
    for doc in all_results:
        results.append(
            {
                "url": doc.metadata.source_url,
                "title": doc.metadata.title,
                "content": doc.markdown,
                "content_length": len(doc.markdown),
                "scraped_at": datetime.now().isoformat(),
            }
        )

    output_path = Path(output_file)
    output_path.write_text(
        json.dumps(results, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    print("\n" + "=" * 60)
    print("  âœ… æ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
    print("=" * 60)


def run_example() -> NoReturn:
    """è¿è¡Œç¤ºä¾‹å¹¶å¤„ç†å¼‚å¸¸."""
    try:
        batch_scrape_example()
        sys.exit(0)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except OSError as e:
        print(f"\nâŒ ç½‘ç»œé”™è¯¯: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    run_example()
