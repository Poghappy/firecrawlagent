#!/usr/bin/env python3
"""
HawaiiHub æ•°æ®é‡‡é›†è„šæœ¬

åŸºäº HAWAIIHUB_DATA_SOURCES_CATALOG.md ä¸­å®šä¹‰çš„æ•°æ®æºè¿›è¡Œé‡‡é›†

ç”¨æ³•:
    python3 hawaiihub_scraper.py --category news --priority P0
    python3 hawaiihub_scraper.py --source "Hawaii News Now"
    python3 hawaiihub_scraper.py --all
"""

import json
import logging
import os
import sys
import time
from datetime import datetime
from pathlib import Path

from dotenv import load_dotenv

# å¯¼å…¥ Firecrawl SDK
try:
    from firecrawl import FirecrawlApp
except ImportError:
    print("é”™è¯¯: è¯·å…ˆå®‰è£… Firecrawl SDK")
    print("è¿è¡Œ: pip3 install --break-system-packages firecrawl-py")
    sys.exit(1)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/hawaiihub_scraper.log"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


# ==================== æ•°æ®æºé…ç½® ====================

NEWS_SOURCES = [
    {
        "name": "Hawaii News Now",
        "url": "https://www.hawaiinewsnow.com/",
        "category": "news",
        "priority": "P0",
        "frequency": "*/2 * * * *",  # æ¯2å°æ—¶
        "scrape_config": {
            "formats": ["markdown"],
            "only_main_content": True,
            "max_age": 7200000,  # 2å°æ—¶ç¼“å­˜
        },
    },
    {
        "name": "Honolulu Civil Beat",
        "url": "https://www.civilbeat.org/",
        "category": "news",
        "priority": "P0",
        "frequency": "0 */6 * * *",  # æ¯6å°æ—¶
        "scrape_config": {
            "formats": ["markdown"],
            "only_main_content": True,
            "max_age": 21600000,  # 6å°æ—¶ç¼“å­˜
        },
    },
    {
        "name": "Honolulu Star-Advertiser",
        "url": "https://www.staradvertiser.com/",
        "category": "news",
        "priority": "P0",
        "frequency": "0 */4 * * *",  # æ¯4å°æ—¶
        "scrape_config": {
            "formats": ["markdown"],
            "only_main_content": True,
            "max_age": 14400000,  # 4å°æ—¶ç¼“å­˜
        },
    },
    {
        "name": "KHON2",
        "url": "https://www.khon2.com/",
        "category": "news",
        "priority": "P0",
        "frequency": "0 */6 * * *",
        "scrape_config": {"formats": ["markdown"], "only_main_content": True},
    },
]

CHINESE_COMMUNITY_SOURCES = [
    {
        "name": "å¤å¨å¤·ä¸­å›½æ—¥æŠ¥",
        "url": "https://hawaiichinesedaily.com/",
        "category": "chinese_community",
        "priority": "P0",
        "frequency": "0 */4 * * *",  # æ¯4å°æ—¶
        "scrape_config": {"formats": ["markdown"], "only_main_content": True},
    },
    {
        "name": "Chinese Chamber of Commerce",
        "url": "https://www.chinesechamber.com/",
        "category": "chinese_community",
        "priority": "P0",
        "frequency": "0 9,18 * * 1,4",  # æ¯å‘¨ä¸€ã€å››
        "scrape_config": {"formats": ["markdown"], "only_main_content": True},
    },
]

RESTAURANT_SOURCES = [
    {
        "name": "Yelp Honolulu Hawaiian",
        "url": "https://www.yelp.com/search?cflt=hawaiian&find_loc=Honolulu%2C+HI",
        "category": "restaurant",
        "priority": "P1",
        "frequency": "0 10 * * *",  # æ¯å¤©10ç‚¹
        "scrape_config": {"formats": ["markdown"], "only_main_content": True},
    },
    {
        "name": "OpenTable Hawaii",
        "url": "https://www.opentable.com/metro/hawaii-restaurants",
        "category": "restaurant",
        "priority": "P1",
        "frequency": "0 11 * * *",  # æ¯å¤©11ç‚¹
        "scrape_config": {"formats": ["markdown"], "only_main_content": True},
    },
]

EVENT_SOURCES = [
    {
        "name": "Go Hawaii Official Events",
        "url": "https://www.gohawaii.com/trip-planning/events-festivals",
        "category": "events",
        "priority": "P1",
        "frequency": "0 8 * * 1",  # æ¯å‘¨ä¸€8ç‚¹
        "scrape_config": {"formats": ["markdown"], "only_main_content": True},
    },
    {
        "name": "Go Hawaii ä¸­æ–‡ç‰ˆ",
        "url": "https://www.gohawaii.cn/cn",
        "category": "events",
        "priority": "P1",
        "frequency": "0 9 * * 1",  # æ¯å‘¨ä¸€9ç‚¹
        "scrape_config": {"formats": ["markdown"], "only_main_content": True},
    },
]

BUSINESS_SOURCES = [
    {
        "name": "Yellow Pages Honolulu",
        "url": "https://www.yellowpages.com/honolulu-hi",
        "category": "business",
        "priority": "P2",
        "frequency": "0 2 1 * *",  # æ¯æœˆ1å·
        "scrape_config": {"formats": ["markdown"], "only_main_content": True},
    }
]

# åˆå¹¶æ‰€æœ‰æ•°æ®æº
ALL_SOURCES = (
    NEWS_SOURCES
    + CHINESE_COMMUNITY_SOURCES
    + RESTAURANT_SOURCES
    + EVENT_SOURCES
    + BUSINESS_SOURCES
)


# ==================== é‡‡é›†ç±» ====================


class HawaiiHubScraper:
    """HawaiiHub æ•°æ®é‡‡é›†å™¨"""

    def __init__(self, api_key: str | None = None):
        """
        åˆå§‹åŒ–é‡‡é›†å™¨

        Args:
            api_key: Firecrawl API å¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        self.api_key = api_key or os.getenv("FIRECRAWL_API_KEY")
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° FIRECRAWL_API_KEYï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡")

        self.app = FirecrawlApp(api_key=self.api_key)
        self.request_count = 0
        self.success_count = 0
        self.failed_count = 0
        self.total_cost = 0.0

        # åˆ›å»ºæ•°æ®ç›®å½•
        self.data_dir = Path("data/hawaiihub")
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def scrape_source(self, source: dict) -> dict | None:
        """
        é‡‡é›†å•ä¸ªæ•°æ®æº

        Args:
            source: æ•°æ®æºé…ç½®

        Returns:
            é‡‡é›†ç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å› None
        """
        name = source["name"]
        url = source["url"]
        config = source["scrape_config"]

        logger.info(f"å¼€å§‹é‡‡é›†: {name} ({url})")

        try:
            # æ‰§è¡Œé‡‡é›†
            result = self.app.scrape(url=url, **config)

            # æ›´æ–°ç»Ÿè®¡
            self.request_count += 1
            self.success_count += 1
            self.total_cost += 0.01  # å‡è®¾æ¯æ¬¡ $0.01

            # æ„å»ºç»“æœ
            scraped_data = {
                "source_name": name,
                "source_url": url,
                "category": source["category"],
                "priority": source["priority"],
                "content": result.markdown
                if hasattr(result, "markdown")
                else str(result),
                "metadata": result.metadata if hasattr(result, "metadata") else {},
                "scraped_at": datetime.now().isoformat(),
            }

            logger.info(
                f"é‡‡é›†æˆåŠŸ: {name} | "
                f"è¯·æ±‚ #{self.request_count} | "
                f"æˆæœ¬: ${self.total_cost:.2f}"
            )

            return scraped_data

        except Exception as e:
            self.request_count += 1
            self.failed_count += 1
            logger.error(f"é‡‡é›†å¤±è´¥: {name} - {e}")
            return None

    def scrape_by_category(self, category: str) -> list[dict]:
        """
        æŒ‰ç±»åˆ«é‡‡é›†

        Args:
            category: ç±»åˆ«åç§°ï¼ˆnews, chinese_community, restaurant, events, businessï¼‰

        Returns:
            é‡‡é›†ç»“æœåˆ—è¡¨
        """
        sources = [s for s in ALL_SOURCES if s["category"] == category]

        if not sources:
            logger.warning(f"æœªæ‰¾åˆ°ç±»åˆ«: {category}")
            return []

        logger.info(f"æŒ‰ç±»åˆ«é‡‡é›†: {category} ({len(sources)} ä¸ªæ•°æ®æº)")

        results = []
        for source in sources:
            result = self.scrape_source(source)
            if result:
                results.append(result)
            time.sleep(1)  # é¿å…é€Ÿç‡é™åˆ¶

        return results

    def scrape_by_priority(self, priority: str) -> list[dict]:
        """
        æŒ‰ä¼˜å…ˆçº§é‡‡é›†

        Args:
            priority: ä¼˜å…ˆçº§ï¼ˆP0, P1, P2ï¼‰

        Returns:
            é‡‡é›†ç»“æœåˆ—è¡¨
        """
        sources = [s for s in ALL_SOURCES if s["priority"] == priority]

        if not sources:
            logger.warning(f"æœªæ‰¾åˆ°ä¼˜å…ˆçº§: {priority}")
            return []

        logger.info(f"æŒ‰ä¼˜å…ˆçº§é‡‡é›†: {priority} ({len(sources)} ä¸ªæ•°æ®æº)")

        results = []
        for source in sources:
            result = self.scrape_source(source)
            if result:
                results.append(result)
            time.sleep(1)

        return results

    def scrape_by_name(self, name: str) -> dict | None:
        """
        æŒ‰åç§°é‡‡é›†å•ä¸ªæ•°æ®æº

        Args:
            name: æ•°æ®æºåç§°

        Returns:
            é‡‡é›†ç»“æœï¼Œå¤±è´¥è¿”å› None
        """
        sources = [s for s in ALL_SOURCES if s["name"] == name]

        if not sources:
            logger.warning(f"æœªæ‰¾åˆ°æ•°æ®æº: {name}")
            return None

        return self.scrape_source(sources[0])

    def scrape_all(self) -> list[dict]:
        """
        é‡‡é›†æ‰€æœ‰æ•°æ®æº

        Returns:
            é‡‡é›†ç»“æœåˆ—è¡¨
        """
        logger.info(f"é‡‡é›†æ‰€æœ‰æ•°æ®æº ({len(ALL_SOURCES)} ä¸ª)")

        results = []
        for source in ALL_SOURCES:
            result = self.scrape_source(source)
            if result:
                results.append(result)
            time.sleep(2)  # é¿å…é€Ÿç‡é™åˆ¶

        return results

    def save_results(self, results: list[dict], prefix: str = "scraped"):
        """
        ä¿å­˜é‡‡é›†ç»“æœ

        Args:
            results: é‡‡é›†ç»“æœåˆ—è¡¨
            prefix: æ–‡ä»¶åå‰ç¼€
        """
        if not results:
            logger.warning("æ— æ•°æ®å¯ä¿å­˜")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # 1. ä¿å­˜ JSON
        json_file = self.data_dir / f"{prefix}_{timestamp}.json"
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        logger.info(f"å·²ä¿å­˜ JSON: {json_file}")

        # 2. ä¿å­˜ Markdownï¼ˆæŒ‰ç±»åˆ«ï¼‰
        by_category = {}
        for item in results:
            category = item["category"]
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(item)

        for category, items in by_category.items():
            md_file = self.data_dir / f"{prefix}_{category}_{timestamp}.md"
            with open(md_file, "w", encoding="utf-8") as f:
                f.write(f"# {category.upper()} é‡‡é›†ç»“æœ\n\n")
                f.write(f"> é‡‡é›†æ—¶é—´: {datetime.now()}\n\n")
                f.write(f"> æ•°æ®æºæ•°é‡: {len(items)}\n\n")
                f.write("---\n\n")

                for item in items:
                    f.write(f"## {item['source_name']}\n\n")
                    f.write(f"- **URL**: {item['source_url']}\n")
                    f.write(f"- **ä¼˜å…ˆçº§**: {item['priority']}\n")
                    f.write(f"- **é‡‡é›†æ—¶é—´**: {item['scraped_at']}\n\n")
                    f.write("### å†…å®¹\n\n")
                    f.write(item["content"][:1000])  # æˆªå–å‰1000å­—ç¬¦
                    f.write("\n\n---\n\n")

            logger.info(f"å·²ä¿å­˜ Markdown: {md_file}")

    def print_statistics(self):
        """æ‰“å°é‡‡é›†ç»Ÿè®¡"""
        success_rate = (
            self.success_count / self.request_count * 100
            if self.request_count > 0
            else 0
        )

        print("\n" + "=" * 60)
        print("ğŸ“Š é‡‡é›†ç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»è¯·æ±‚æ•°: {self.request_count}")
        print(f"æˆåŠŸæ•°: {self.success_count}")
        print(f"å¤±è´¥æ•°: {self.failed_count}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"æ€»æˆæœ¬: ${self.total_cost:.2f}")
        print("=" * 60 + "\n")


# ==================== å‘½ä»¤è¡Œæ¥å£ ====================


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description="HawaiiHub æ•°æ®é‡‡é›†è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # é‡‡é›†æ‰€æœ‰æ–°é—»æº
  python3 hawaiihub_scraper.py --category news

  # é‡‡é›† P0 ä¼˜å…ˆçº§æ•°æ®æº
  python3 hawaiihub_scraper.py --priority P0

  # é‡‡é›†å•ä¸ªæ•°æ®æº
  python3 hawaiihub_scraper.py --source "Hawaii News Now"

  # é‡‡é›†æ‰€æœ‰æ•°æ®æº
  python3 hawaiihub_scraper.py --all

  # åˆ—å‡ºæ‰€æœ‰æ•°æ®æº
  python3 hawaiihub_scraper.py --list
        """,
    )

    parser.add_argument(
        "--category",
        choices=["news", "chinese_community", "restaurant", "events", "business"],
        help="æŒ‰ç±»åˆ«é‡‡é›†",
    )
    parser.add_argument("--priority", choices=["P0", "P1", "P2"], help="æŒ‰ä¼˜å…ˆçº§é‡‡é›†")
    parser.add_argument("--source", help="é‡‡é›†æŒ‡å®šåç§°çš„æ•°æ®æº")
    parser.add_argument("--all", action="store_true", help="é‡‡é›†æ‰€æœ‰æ•°æ®æº")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰æ•°æ®æº")

    args = parser.parse_args()

    # åˆ—å‡ºæ•°æ®æº
    if args.list:
        print("\nğŸ“‹ å¯ç”¨æ•°æ®æºåˆ—è¡¨:\n")
        for source in ALL_SOURCES:
            print(f"- {source['name']}")
            print(f"  ç±»åˆ«: {source['category']}")
            print(f"  ä¼˜å…ˆçº§: {source['priority']}")
            print(f"  URL: {source['url']}")
            print()
        return

    # åˆ›å»ºé‡‡é›†å™¨
    try:
        scraper = HawaiiHubScraper()
    except ValueError as e:
        logger.error(e)
        sys.exit(1)

    # æ‰§è¡Œé‡‡é›†
    results = []

    if args.category:
        results = scraper.scrape_by_category(args.category)
        prefix = f"category_{args.category}"
    elif args.priority:
        results = scraper.scrape_by_priority(args.priority)
        prefix = f"priority_{args.priority}"
    elif args.source:
        result = scraper.scrape_by_name(args.source)
        if result:
            results = [result]
        prefix = "single_source"
    elif args.all:
        results = scraper.scrape_all()
        prefix = "all_sources"
    else:
        parser.print_help()
        return

    # ä¿å­˜ç»“æœ
    if results:
        scraper.save_results(results, prefix=prefix)

    # æ‰“å°ç»Ÿè®¡
    scraper.print_statistics()


if __name__ == "__main__":
    main()
