#!/usr/bin/env python3
"""
HawaiiHub Firecrawl æ¨¡æ¿é…ç½®è„šæœ¬

åŠŸèƒ½ï¼š
1. å®‰è£…æ‰€éœ€ä¾èµ–
2. éªŒè¯ API å¯†é’¥
3. åˆ›å»ºé…ç½®æ–‡ä»¶
4. ç”Ÿæˆç¤ºä¾‹ä»£ç 
5. æµ‹è¯•è¿æ¥

ä½œè€…: HawaiiHub AI Team
æ—¥æœŸ: 2025-10-28
"""

import json
import logging
import os
import sys
from pathlib import Path


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class HawaiiHubTemplatesSetup:
    """HawaiiHub Firecrawl æ¨¡æ¿é…ç½®ç®¡ç†å™¨"""

    def __init__(self, project_root: Path | None = None):
        """åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨"""
        self.project_root = project_root or Path(__file__).parent.parent
        self.templates_dir = self.project_root / "templates" / "hawaiihub"
        self.config_dir = self.project_root / "config"
        self.data_dir = self.project_root / "data" / "hawaiihub"

        # åˆ›å»ºå¿…è¦ç›®å½•
        self.templates_dir.mkdir(parents=True, exist_ok=True)
        self.config_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def check_dependencies(self) -> bool:
        """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
        logging.info("ğŸ” æ£€æŸ¥ Python ä¾èµ–...")

        # (åŒ…å, å¯¼å…¥å)
        required_packages = [
            ("firecrawl-py", "firecrawl"),
            ("python-dotenv", "dotenv"),
            ("requests", "requests"),
            ("pydantic", "pydantic"),
        ]

        missing_packages = []

        for package_name, import_name in required_packages:
            try:
                __import__(import_name)
                logging.info(f"  âœ… {package_name}")
            except ImportError:
                logging.warning(f"  âŒ {package_name}")
                missing_packages.append(package_name)

        if missing_packages:
            logging.error(f"ç¼ºå°‘ä»¥ä¸‹ä¾èµ–: {', '.join(missing_packages)}")
            logging.info("è¯·è¿è¡Œ: pip3 install " + " ".join(missing_packages))
            return False

        logging.info("âœ… æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
        return True

    def verify_api_key(self) -> bool:
        """éªŒè¯ Firecrawl API å¯†é’¥"""
        logging.info("ğŸ”‘ éªŒè¯ Firecrawl API å¯†é’¥...")

        from dotenv import load_dotenv

        load_dotenv()

        api_key = os.getenv("FIRECRAWL_API_KEY")

        if not api_key:
            logging.error("âŒ æœªæ‰¾åˆ° FIRECRAWL_API_KEY")
            logging.info("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® FIRECRAWL_API_KEY=fc-xxx")
            return False

        if not api_key.startswith("fc-"):
            logging.error("âŒ API å¯†é’¥æ ¼å¼ä¸æ­£ç¡®ï¼ˆåº”ä»¥ 'fc-' å¼€å¤´ï¼‰")
            return False

        # æµ‹è¯• API è¿æ¥
        try:
            from firecrawl import Firecrawl

            firecrawl = Firecrawl(api_key=api_key)

            # ç®€å•æµ‹è¯•
            logging.info("  æµ‹è¯• API è¿æ¥...")
            result = firecrawl.scrape("https://firecrawl.dev", formats=["markdown"])

            if result and hasattr(result, "markdown"):
                logging.info("âœ… API å¯†é’¥æœ‰æ•ˆï¼Œè¿æ¥æˆåŠŸï¼")
                return True
            logging.error("âŒ API è¿”å›å¼‚å¸¸")
            return False

        except Exception as e:
            logging.error(f"âŒ API æµ‹è¯•å¤±è´¥: {e}")
            return False

    def create_config_files(self) -> None:
        """åˆ›å»ºé…ç½®æ–‡ä»¶"""
        logging.info("ğŸ“ åˆ›å»ºé…ç½®æ–‡ä»¶...")

        # 1. HawaiiHub æ•°æ®æºé…ç½®
        sources_config = {
            "news_sources": {
                "hawaiinewsnow": {
                    "url": "https://www.hawaiinewsnow.com/",
                    "name": "Hawaii News Now",
                    "type": "general",
                    "scrape_method": "crawl",
                    "cache_ttl": 3600,
                    "enabled": True,
                },
                "staradvertiser": {
                    "url": "https://www.staradvertiser.com/",
                    "name": "Honolulu Star-Advertiser",
                    "type": "general",
                    "scrape_method": "crawl",
                    "cache_ttl": 3600,
                    "enabled": True,
                },
                "civilbeat": {
                    "url": "https://www.civilbeat.org/",
                    "name": "Honolulu Civil Beat",
                    "type": "investigative",
                    "scrape_method": "crawl",
                    "cache_ttl": 7200,
                    "enabled": True,
                },
            },
            "restaurant_sources": {
                "yelp_chinese": {
                    "url": "https://www.yelp.com/search?find_desc=Chinese&find_loc=Honolulu,+HI",
                    "name": "Yelp - æª€é¦™å±±ä¸­é¤",
                    "type": "restaurant",
                    "scrape_method": "search",
                    "cache_ttl": 86400,
                    "enabled": True,
                },
                "yelp_japanese": {
                    "url": "https://www.yelp.com/search?find_desc=Japanese&find_loc=Honolulu,+HI",
                    "name": "Yelp - æª€é¦™å±±æ—¥æ–™",
                    "type": "restaurant",
                    "scrape_method": "search",
                    "cache_ttl": 86400,
                    "enabled": True,
                },
            },
            "housing_sources": {
                "craigslist_honolulu": {
                    "url": "https://honolulu.craigslist.org/search/apa",
                    "name": "Craigslist - æª€é¦™å±±ç§Ÿæˆ¿",
                    "type": "housing",
                    "scrape_method": "crawl",
                    "cache_ttl": 1800,
                    "enabled": True,
                }
            },
            "community_sources": {
                "kauai_chinese": {
                    "url": "https://www.kauaichineseassociation.org/",
                    "name": "Kauai Chinese Association",
                    "type": "community",
                    "scrape_method": "map",
                    "cache_ttl": 86400,
                    "enabled": True,
                }
            },
        }

        # ä¿å­˜æ•°æ®æºé…ç½®
        sources_file = self.config_dir / "hawaiihub_sources.json"
        with open(sources_file, "w", encoding="utf-8") as f:
            json.dump(sources_config, f, ensure_ascii=False, indent=2)
        logging.info(f"  âœ… æ•°æ®æºé…ç½®: {sources_file}")

        # 2. æ•°æ®Schemaé…ç½®
        schemas_config = {
            "news_article": {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "author": {"type": "string"},
                    "published_date": {"type": "string"},
                    "content": {"type": "string"},
                    "summary": {"type": "string"},
                    "category": {"type": "string"},
                    "tags": {"type": "array", "items": {"type": "string"}},
                },
                "required": ["title", "content"],
            },
            "restaurant": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "rating": {"type": "number"},
                    "price_range": {"type": "string"},
                    "address": {"type": "string"},
                    "phone": {"type": "string"},
                    "cuisine": {"type": "array", "items": {"type": "string"}},
                    "hours": {"type": "object"},
                    "reviews_count": {"type": "number"},
                    "website": {"type": "string"},
                },
                "required": ["name", "address"],
            },
            "housing_listing": {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "price": {"type": "number"},
                    "bedrooms": {"type": "number"},
                    "bathrooms": {"type": "number"},
                    "sqft": {"type": "number"},
                    "address": {"type": "string"},
                    "description": {"type": "string"},
                    "posted_date": {"type": "string"},
                    "contact": {"type": "string"},
                },
                "required": ["title", "price"],
            },
            "community_event": {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "date": {"type": "string"},
                    "time": {"type": "string"},
                    "location": {"type": "string"},
                    "organizer": {"type": "string"},
                    "description": {"type": "string"},
                    "registration_url": {"type": "string"},
                    "cost": {"type": "string"},
                },
                "required": ["title", "date"],
            },
        }

        # ä¿å­˜Schemaé…ç½®
        schemas_file = self.config_dir / "hawaiihub_schemas.json"
        with open(schemas_file, "w", encoding="utf-8") as f:
            json.dump(schemas_config, f, ensure_ascii=False, indent=2)
        logging.info(f"  âœ… Schema é…ç½®: {schemas_file}")

        logging.info("âœ… é…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ")

    def create_templates(self) -> None:
        """åˆ›å»ºä»£ç æ¨¡æ¿"""
        logging.info("ğŸ“„ åˆ›å»ºä»£ç æ¨¡æ¿...")

        # 1. æ–°é—»çˆ¬å–æ¨¡æ¿
        news_template = '''#!/usr/bin/env python3
"""
HawaiiHub æ–°é—»çˆ¬å–æ¨¡æ¿

åŠŸèƒ½: çˆ¬å–å¤å¨å¤·æœ¬åœ°æ–°é—»å¹¶ä¿å­˜åˆ°æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python news_scraper.py
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict

from firecrawl import Firecrawl
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/hawaiihub_scraper.log'),
        logging.StreamHandler()
    ]
)

# åˆå§‹åŒ– Firecrawl å®¢æˆ·ç«¯
firecrawl = Firecrawl(api_key=os.getenv("FIRECRAWL_API_KEY"))

# åŠ è½½æ•°æ®æºé…ç½®
with open("config/hawaiihub_sources.json", "r", encoding="utf-8") as f:
    SOURCES_CONFIG = json.load(f)

def scrape_news_source(source_id: str, config: Dict) -> List[Dict]:
    """çˆ¬å–å•ä¸ªæ–°é—»æº"""
    logging.info(f"å¼€å§‹çˆ¬å–: {config['name']}")

    articles = []

    try:
        # 1. ä½¿ç”¨ Map å‘ç°æ–‡ç« é“¾æ¥
        urls = firecrawl.map(
            url=config["url"],
            search="news article",
            limit=50
        )

        logging.info(f"  å‘ç° {len(urls.links)} ä¸ªé“¾æ¥")

        # 2. æ‰¹é‡çˆ¬å–æ–‡ç« ï¼ˆå–å‰20ä¸ªï¼‰
        if urls.links:
            result = firecrawl.batch_scrape(
                urls=urls.links[:20],
                formats=["markdown"],
                poll_interval=2
            )

            # 3. å¤„ç†ç»“æœ
            for doc in result.data:
                if not doc.error:
                    article = {
                        "source_id": source_id,
                        "source_name": config["name"],
                        "url": doc.url,
                        "title": doc.metadata.title if doc.metadata else "",
                        "content": doc.markdown,
                        "scraped_at": datetime.now().isoformat()
                    }
                    articles.append(article)

            logging.info(f"  æˆåŠŸçˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")

    except Exception as e:
        logging.error(f"  çˆ¬å–å¤±è´¥: {e}")

    return articles

def save_articles(articles: List[Dict]) -> None:
    """ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶"""
    if not articles:
        logging.warning("æ²¡æœ‰æ–‡ç« éœ€è¦ä¿å­˜")
        return

    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = Path("data/hawaiihub/news")
    data_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä¿å­˜ JSON
    json_file = data_dir / f"news_{timestamp}.json"
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

    logging.info(f"âœ… ä¿å­˜åˆ°: {json_file}")
    logging.info(f"   æ€»è®¡: {len(articles)} ç¯‡æ–‡ç« ")

def main():
    """ä¸»å‡½æ•°"""
    logging.info("ğŸ”¥ å¼€å§‹ HawaiiHub æ–°é—»é‡‡é›†")

    all_articles = []

    # éå†æ‰€æœ‰å¯ç”¨çš„æ–°é—»æº
    for source_id, config in SOURCES_CONFIG["news_sources"].items():
        if config.get("enabled", False):
            articles = scrape_news_source(source_id, config)
            all_articles.extend(articles)

    # ä¿å­˜ç»“æœ
    save_articles(all_articles)

    logging.info("âœ… æ–°é—»é‡‡é›†å®Œæˆ")

if __name__ == "__main__":
    main()
'''

        # ä¿å­˜æ–°é—»æ¨¡æ¿
        news_file = self.templates_dir / "news_scraper.py"
        with open(news_file, "w", encoding="utf-8") as f:
            f.write(news_template)
        logging.info(f"  âœ… æ–°é—»çˆ¬å–æ¨¡æ¿: {news_file}")

        # 2. é¤å…çˆ¬å–æ¨¡æ¿
        restaurant_template = '''#!/usr/bin/env python3
"""
HawaiiHub é¤å…ä¿¡æ¯çˆ¬å–æ¨¡æ¿

åŠŸèƒ½: çˆ¬å– Yelp ä¸Šçš„å¤å¨å¤·é¤å…ä¿¡æ¯

ä½¿ç”¨æ–¹æ³•:
    python restaurant_scraper.py
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict

from firecrawl import Firecrawl
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# åˆå§‹åŒ– Firecrawl å®¢æˆ·ç«¯
firecrawl = Firecrawl(api_key=os.getenv("FIRECRAWL_API_KEY"))

# åŠ è½½é…ç½®
with open("config/hawaiihub_sources.json", "r", encoding="utf-8") as f:
    SOURCES_CONFIG = json.load(f)

with open("config/hawaiihub_schemas.json", "r", encoding="utf-8") as f:
    SCHEMAS = json.load(f)

def scrape_restaurants(source_id: str, config: Dict) -> List[Dict]:
    """çˆ¬å–é¤å…ä¿¡æ¯"""
    logging.info(f"å¼€å§‹çˆ¬å–: {config['name']}")

    restaurants = []

    try:
        # ä½¿ç”¨ Extract åŠŸèƒ½æå–ç»“æ„åŒ–æ•°æ®
        result = firecrawl.scrape(
            url=config["url"],
            formats=[{
                "type": "json",
                "schema": SCHEMAS["restaurant"],
                "prompt": "Extract restaurant information"
            }]
        )

        if result.extract:
            restaurants = result.extract if isinstance(result.extract, list) else [result.extract]
            logging.info(f"  æå–äº† {len(restaurants)} å®¶é¤å…")

    except Exception as e:
        logging.error(f"  çˆ¬å–å¤±è´¥: {e}")

    return restaurants

def save_restaurants(restaurants: List[Dict]) -> None:
    """ä¿å­˜é¤å…æ•°æ®"""
    if not restaurants:
        logging.warning("æ²¡æœ‰é¤å…æ•°æ®éœ€è¦ä¿å­˜")
        return

    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = Path("data/hawaiihub/restaurants")
    data_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä¿å­˜ JSON
    json_file = data_dir / f"restaurants_{timestamp}.json"
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(restaurants, f, ensure_ascii=False, indent=2)

    logging.info(f"âœ… ä¿å­˜åˆ°: {json_file}")
    logging.info(f"   æ€»è®¡: {len(restaurants)} å®¶é¤å…")

def main():
    """ä¸»å‡½æ•°"""
    logging.info("ğŸœ å¼€å§‹ HawaiiHub é¤å…ä¿¡æ¯é‡‡é›†")

    all_restaurants = []

    # éå†æ‰€æœ‰å¯ç”¨çš„é¤å…æº
    for source_id, config in SOURCES_CONFIG["restaurant_sources"].items():
        if config.get("enabled", False):
            restaurants = scrape_restaurants(source_id, config)
            all_restaurants.extend(restaurants)

    # ä¿å­˜ç»“æœ
    save_restaurants(all_restaurants)

    logging.info("âœ… é¤å…ä¿¡æ¯é‡‡é›†å®Œæˆ")

if __name__ == "__main__":
    main()
'''

        # ä¿å­˜é¤å…æ¨¡æ¿
        restaurant_file = self.templates_dir / "restaurant_scraper.py"
        with open(restaurant_file, "w", encoding="utf-8") as f:
            f.write(restaurant_template)
        logging.info(f"  âœ… é¤å…çˆ¬å–æ¨¡æ¿: {restaurant_file}")

        logging.info("âœ… ä»£ç æ¨¡æ¿åˆ›å»ºå®Œæˆ")

    def create_quick_start_script(self) -> None:
        """åˆ›å»ºå¿«é€Ÿå¯åŠ¨è„šæœ¬"""
        logging.info("ğŸš€ åˆ›å»ºå¿«é€Ÿå¯åŠ¨è„šæœ¬...")

        quick_start = '''#!/usr/bin/env python3
"""
HawaiiHub Firecrawl å¿«é€Ÿå¯åŠ¨è„šæœ¬

å¿«é€Ÿæµ‹è¯•æ‰€æœ‰ Firecrawl æ ¸å¿ƒåŠŸèƒ½

ä½¿ç”¨æ–¹æ³•:
    python quick_start_hawaiihub.py
"""

import os
import logging
from firecrawl import Firecrawl
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s: %(message)s'
)

# åˆå§‹åŒ–å®¢æˆ·ç«¯
firecrawl = Firecrawl(api_key=os.getenv("FIRECRAWL_API_KEY"))

def test_scrape():
    """æµ‹è¯• Scrape åŠŸèƒ½"""
    logging.info("\\n" + "="*60)
    logging.info("1ï¸âƒ£  æµ‹è¯• SCRAPEï¼ˆå•é¡µçˆ¬å–ï¼‰")
    logging.info("="*60)

    try:
        doc = firecrawl.scrape(
            "https://www.hawaiinewsnow.com/",
            formats=["markdown"]
        )

        logging.info(f"âœ… æˆåŠŸçˆ¬å–")
        logging.info(f"   URL: {doc.url}")
        logging.info(f"   å†…å®¹é•¿åº¦: {len(doc.markdown)} å­—ç¬¦")
        logging.info(f"   å‰100å­—ç¬¦: {doc.markdown[:100]}...")

        return True

    except Exception as e:
        logging.error(f"âŒ Scrape å¤±è´¥: {e}")
        return False

def test_map():
    """æµ‹è¯• Map åŠŸèƒ½"""
    logging.info("\\n" + "="*60)
    logging.info("2ï¸âƒ£  æµ‹è¯• MAPï¼ˆç½‘ç«™åœ°å›¾ï¼‰")
    logging.info("="*60)

    try:
        urls = firecrawl.map(
            url="https://www.hawaiinewsnow.com/",
            limit=10
        )

        logging.info(f"âœ… æˆåŠŸå‘ç°é“¾æ¥")
        logging.info(f"   å‘ç° {len(urls.links)} ä¸ªé“¾æ¥:")
        for i, link in enumerate(urls.links[:5], 1):
            logging.info(f"   {i}. {link}")

        if len(urls.links) > 5:
            logging.info(f"   ... è¿˜æœ‰ {len(urls.links) - 5} ä¸ªé“¾æ¥")

        return True

    except Exception as e:
        logging.error(f"âŒ Map å¤±è´¥: {e}")
        return False

def test_search():
    """æµ‹è¯• Search åŠŸèƒ½"""
    logging.info("\\n" + "="*60)
    logging.info("3ï¸âƒ£  æµ‹è¯• SEARCHï¼ˆæœç´¢ï¼‰")
    logging.info("="*60)

    try:
        results = firecrawl.search(
            query="Hawaii news",
            limit=3
        )

        logging.info(f"âœ… æˆåŠŸæœç´¢")
        logging.info(f"   æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:")
        for i, result in enumerate(results, 1):
            logging.info(f"   {i}. {result.title}")
            logging.info(f"      {result.url}")

        return True

    except Exception as e:
        logging.error(f"âŒ Search å¤±è´¥: {e}")
        return False

def test_batch_scrape():
    """æµ‹è¯• Batch Scrape åŠŸèƒ½"""
    logging.info("\\n" + "="*60)
    logging.info("4ï¸âƒ£  æµ‹è¯• BATCH SCRAPEï¼ˆæ‰¹é‡çˆ¬å–ï¼‰")
    logging.info("="*60)

    urls = [
        "https://www.hawaiinewsnow.com/",
        "https://www.staradvertiser.com/"
    ]

    try:
        job = firecrawl.batch_scrape(
            urls=urls,
            formats=["markdown"],
            poll_interval=2
        )

        logging.info(f"âœ… æˆåŠŸæ‰¹é‡çˆ¬å–")
        logging.info(f"   çŠ¶æ€: {job.status}")
        logging.info(f"   å®Œæˆ: {job.completed}/{job.total}")

        for i, doc in enumerate(job.data, 1):
            if not doc.error:
                logging.info(f"   {i}. {doc.url} - {len(doc.markdown)} å­—ç¬¦")
            else:
                logging.info(f"   {i}. {doc.url} - å¤±è´¥: {doc.error}")

        return True

    except Exception as e:
        logging.error(f"âŒ Batch Scrape å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logging.info("\\n" + "ğŸ”¥"*30)
    logging.info("HawaiiHub Firecrawl å¿«é€Ÿå¯åŠ¨æµ‹è¯•")
    logging.info("ğŸ”¥"*30)

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("Scrape", test_scrape),
        ("Map", test_map),
        ("Search", test_search),
        ("Batch Scrape", test_batch_scrape)
    ]

    results = {}
    for name, test_func in tests:
        results[name] = test_func()

    # æ±‡æ€»ç»“æœ
    logging.info("\\n" + "="*60)
    logging.info("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    logging.info("="*60)

    for name, success in results.items():
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        logging.info(f"   {name}: {status}")

    # æ€»ç»“
    total = len(results)
    passed = sum(1 for r in results.values() if r)

    logging.info("\\n" + "="*60)
    if passed == total:
        logging.info(f"ğŸ‰ å®Œç¾ï¼æ‰€æœ‰ {total} ä¸ªæµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
        logging.info("   ä½ å·²ç»å‡†å¤‡å¥½ä½¿ç”¨ Firecrawl äº†ï¼")
    else:
        logging.warning(f"âš ï¸  {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
        logging.warning("   è¯·æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•")
    logging.info("="*60)

if __name__ == "__main__":
    main()
'''

        # ä¿å­˜å¿«é€Ÿå¯åŠ¨è„šæœ¬
        quick_start_file = self.project_root / "quick_start_hawaiihub.py"
        with open(quick_start_file, "w", encoding="utf-8") as f:
            f.write(quick_start)

        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(quick_start_file, 0o755)

        logging.info(f"  âœ… å¿«é€Ÿå¯åŠ¨è„šæœ¬: {quick_start_file}")
        logging.info("âœ… å¿«é€Ÿå¯åŠ¨è„šæœ¬åˆ›å»ºå®Œæˆ")

    def run_setup(self) -> bool:
        """è¿è¡Œå®Œæ•´é…ç½®æµç¨‹"""
        logging.info("\\n" + "ğŸ”¥" * 30)
        logging.info("HawaiiHub Firecrawl æ¨¡æ¿é…ç½®å·¥å…·")
        logging.info("ğŸ”¥" * 30 + "\\n")

        # 1. æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies():
            return False

        logging.info("")

        # 2. éªŒè¯ API å¯†é’¥
        if not self.verify_api_key():
            return False

        logging.info("")

        # 3. åˆ›å»ºé…ç½®æ–‡ä»¶
        self.create_config_files()

        logging.info("")

        # 4. åˆ›å»ºæ¨¡æ¿
        self.create_templates()

        logging.info("")

        # 5. åˆ›å»ºå¿«é€Ÿå¯åŠ¨è„šæœ¬
        self.create_quick_start_script()

        logging.info("")
        logging.info("=" * 60)
        logging.info("ğŸ‰ é…ç½®å®Œæˆï¼")
        logging.info("=" * 60)
        logging.info("")
        logging.info("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        logging.info(f"   â€¢ é…ç½®ç›®å½•: {self.config_dir}")
        logging.info(f"   â€¢ æ¨¡æ¿ç›®å½•: {self.templates_dir}")
        logging.info(f"   â€¢ æ•°æ®ç›®å½•: {self.data_dir}")
        logging.info("")
        logging.info("ğŸš€ ä¸‹ä¸€æ­¥:")
        logging.info("   1. è¿è¡Œå¿«é€Ÿå¯åŠ¨æµ‹è¯•:")
        logging.info("      python3 quick_start_hawaiihub.py")
        logging.info("")
        logging.info("   2. è¿è¡Œæ–°é—»çˆ¬å–æ¨¡æ¿:")
        logging.info("      python3 templates/hawaiihub/news_scraper.py")
        logging.info("")
        logging.info("   3. è¿è¡Œé¤å…çˆ¬å–æ¨¡æ¿:")
        logging.info("      python3 templates/hawaiihub/restaurant_scraper.py")
        logging.info("")
        logging.info("ğŸ“š æ–‡æ¡£:")
        logging.info("   â€¢ æ¨¡æ¿ç›®å½•: docs/FIRECRAWL_TEMPLATES_CATALOG.md")
        logging.info("   â€¢ å¿«é€Ÿå‚è€ƒ: QUICK_REFERENCE.md")
        logging.info("=" * 60)

        return True


def main() -> None:
    """ä¸»å…¥å£"""
    setup = HawaiiHubTemplatesSetup()
    success = setup.run_setup()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
