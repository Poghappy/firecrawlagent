# Firecrawl ä¸“é¡¹è§„åˆ™

> **æœ€åæ›´æ–°**: 2025-10-27
> **SDK ç‰ˆæœ¬**: v2.x
> **é€‚ç”¨åœºæ™¯**: Firecrawl äº‘ API ä½¿ç”¨

---

## ğŸ”¥ æ ¸å¿ƒåŸåˆ™

### 1. å·¥å…·é€‰æ‹©ä¼˜å…ˆçº§

```
P0: MCP å·¥å…·ï¼ˆæœ€ç®€å•ã€æœ€å¯é ï¼‰
â”œâ”€ å¤æ‚é¡µé¢ï¼ˆå¤§é‡ JSã€åŠ¨æ€åŠ è½½ï¼‰
â”œâ”€ å¿«é€ŸåŸå‹éªŒè¯
â””â”€ å•æ¬¡çˆ¬å–éœ€æ±‚

P1: Python SDK v2ï¼ˆéœ€è¦æ›´å¤šæ§åˆ¶ï¼‰
â”œâ”€ æ‰¹é‡çˆ¬å–ï¼ˆbatch_scrapeï¼‰
â”œâ”€ æ•´ç«™çˆ¬å–ï¼ˆcrawlï¼‰
â”œâ”€ æœç´¢+çˆ¬å–ï¼ˆsearchï¼‰
â””â”€ éœ€è¦é«˜çº§é…ç½®

P2: æ‰‹åŠ¨å®ç°ï¼ˆä¸æ¨èï¼‰
â””â”€ ä»…åœ¨ Firecrawl ä¸å¯ç”¨æ—¶è€ƒè™‘
```

---

## ğŸ†• SDK v2 é‡è¦å˜åŒ–

### å‘½åçº¦å®šï¼ˆé‡è¦ï¼ï¼‰

| åœºæ™¯           | v1 (æ—§)           | v2 (æ–°)             | è¯´æ˜        |
| -------------- | ----------------- | ------------------- | ----------- |
| **MCP å·¥å…·**   | `onlyMainContent` | `onlyMainContent`   | âœ… é©¼å³°å¼   |
| **Python SDK** | `onlyMainContent` | `only_main_content` | âœ… ä¸‹åˆ’çº¿   |
| **è¿”å›å€¼**     | `dict`            | `Document` å¯¹è±¡     | âš ï¸ ç±»å‹å˜åŒ– |

### æ­£ç¡®ç”¨æ³•ç¤ºä¾‹

```python
# âœ… MCP å·¥å…·ï¼ˆé©¼å³°å¼ï¼‰
result = mcp_firecrawl_scrape(
    url="https://example.com",
    formats=["markdown"],
    onlyMainContent=True  # é©¼å³°å¼
)

# âœ… Python SDK v2ï¼ˆä¸‹åˆ’çº¿ï¼‰
from firecrawl import FirecrawlApp
app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
result = app.scrape(
    url="https://example.com",
    formats=["markdown"],
    only_main_content=True  # ä¸‹åˆ’çº¿
)

# âœ… è®¿é—®ç»“æœï¼ˆå±æ€§ï¼Œä¸æ˜¯å­—å…¸ï¼‰
content = result.markdown  # æ­£ç¡®
title = result.metadata.title  # æ­£ç¡®

# âŒ é”™è¯¯å†™æ³•
content = result.get("markdown")  # ä¼šæŠ¥é”™
content = result["markdown"]  # ä¼šæŠ¥é”™
```

---

## ğŸ“Š å¸¸ç”¨ API æ–¹æ³•

### 1. scrape() - å•é¡µçˆ¬å–

```python
result = app.scrape(
    url="https://example.com",
    formats=["markdown"],           # å¯é€‰: markdown, html, rawHtml, links
    only_main_content=True,         # åªæå–ä¸»è¦å†…å®¹
    max_age=172800000,              # ç¼“å­˜ 2 å¤©ï¼ˆæ¯«ç§’ï¼‰
    block_ads=True,                 # å±è”½å¹¿å‘Š
    skip_tls_verification=False,    # TLS éªŒè¯
    remove_base64_images=True       # ç§»é™¤ base64 å›¾ç‰‡
)

# è®¿é—®ç»“æœ
markdown = result.markdown
html = result.html
url = result.url
title = result.metadata.title if result.metadata else None
```

### 2. batch_scrape() - æ‰¹é‡çˆ¬å–

```python
urls = ["url1", "url2", "url3"]
results = app.batch_scrape(
    urls=urls,
    formats=["markdown"],
    only_main_content=True
)

# å¤„ç†ç»“æœ
for item in results:
    if isinstance(item, tuple):
        success, result = item
        if success:
            print(result.markdown)
    else:
        print(item.markdown)
```

### 3. crawl() - æ•´ç«™çˆ¬å–

```python
# å¯åŠ¨çˆ¬å–
crawl_result = app.crawl(
    url="https://example.com",
    max_depth=3,                # æœ€å¤§æ·±åº¦
    limit=100,                  # æœ€å¤§é¡µé¢æ•°
    allow_backward_links=False, # å…è®¸å‘åé“¾æ¥
    allow_external_links=False, # å…è®¸å¤–éƒ¨é“¾æ¥
    formats=["markdown"]
)

# è·å– ID
crawl_id = crawl_result.id

# æ£€æŸ¥çŠ¶æ€
status = app.check_crawl_status(crawl_id)
```

### 4. search() - æœç´¢+çˆ¬å–

```python
results = app.search(
    query="å¤å¨å¤· åäºº é¤å…",
    sources=[{"type": "web"}],
    limit=10,
    scrape_options={
        "formats": ["markdown"],
        "only_main_content": True
    }
)
```

### 5. map() - è·å–ç«™ç‚¹åœ°å›¾

```python
site_map = app.map(
    url="https://example.com",
    search="news",  # å¯é€‰ï¼šæœç´¢ç‰¹å®šå†…å®¹
    limit=1000
)

# è·å–æ‰€æœ‰é“¾æ¥
urls = site_map.links
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. ä½¿ç”¨ç¼“å­˜ï¼ˆèŠ‚çœ 50%+ æˆæœ¬ï¼‰

```python
# âœ… æ¨èï¼šä½¿ç”¨ç¼“å­˜
result = app.scrape(
    url="https://example.com",
    formats=["markdown"],
    max_age=172800000  # 2 å¤©ç¼“å­˜ï¼ˆæ¯«ç§’ï¼‰
)

# ç¼“å­˜ç­–ç•¥
CACHE_TTL = {
    "news": 3600000,      # 1 å°æ—¶ï¼ˆæ–°é—»æ›´æ–°å¿«ï¼‰
    "products": 43200000, # 12 å°æ—¶ï¼ˆå•†å“ä¸­ç­‰ï¼‰
    "company": 86400000   # 24 å°æ—¶ï¼ˆå…¬å¸ä¿¡æ¯æ…¢ï¼‰
}
```

### 2. æ‰¹é‡å¤„ç†

```python
# âœ… æ¨èï¼šæ‰¹é‡çˆ¬å–
urls = ["url1", "url2", "url3", "url4", "url5"]
results = app.batch_scrape(urls, formats=["markdown"])

# âŒ é¿å…ï¼šé€ä¸ªçˆ¬å–
for url in urls:
    result = app.scrape(url)  # æ…¢ä¸”è´µ
```

### 3. åªæå–éœ€è¦çš„æ ¼å¼

```python
# âœ… æ¨èï¼šåªè¦ markdown
result = app.scrape(url, formats=["markdown"])

# âŒ é¿å…ï¼šè¦æ‰€æœ‰æ ¼å¼
result = app.scrape(url, formats=["markdown", "html", "rawHtml", "links"])
```

---

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†æ¨¡æ¿

### æ ‡å‡†é”™è¯¯å¤„ç†

```python
from firecrawl import FirecrawlApp
from firecrawl.exceptions import RequestTimeoutError, RateLimitError
import logging
import time

def safe_scrape(url: str, max_retries: int = 3) -> dict | None:
    """
    å®‰å…¨çˆ¬å–ï¼Œå¸¦é‡è¯•å’Œæ—¥å¿—

    Args:
        url: ç›®æ ‡ URL
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        çˆ¬å–ç»“æœï¼Œå¤±è´¥è¿”å› None
    """
    app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))

    for attempt in range(max_retries):
        try:
            result = app.scrape(
                url=url,
                formats=["markdown"],
                only_main_content=True
            )

            # éªŒè¯ç»“æœ
            if not result or not hasattr(result, "markdown"):
                raise ValueError("è¿”å›ç»“æœæ— æ•ˆ")

            logging.info(f"æˆåŠŸçˆ¬å–: {url}")
            return result

        except RequestTimeoutError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                logging.warning(
                    f"è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... "
                    f"({attempt+1}/{max_retries})"
                )
                time.sleep(wait_time)
            else:
                logging.error(
                    f"å¤±è´¥ï¼ˆ{max_retries}æ¬¡é‡è¯•åï¼‰: {url} - {e}"
                )
                return None

        except RateLimitError as e:
            logging.error(f"é€Ÿç‡é™åˆ¶: {url} - è€ƒè™‘åˆ‡æ¢å¯†é’¥")
            return None

        except Exception as e:
            logging.error(f"æœªçŸ¥é”™è¯¯: {url} - {e}")
            return None

    return None
```

---

## ğŸ’° æˆæœ¬æ§åˆ¶

### 1. è¯·æ±‚è®¡æ•°

```python
class FirecrawlClient:
    """å¸¦æˆæœ¬æ§åˆ¶çš„å®¢æˆ·ç«¯"""

    def __init__(self, api_key: str, daily_budget: float = 10.0):
        self.app = FirecrawlApp(api_key=api_key)
        self.daily_budget = daily_budget
        self.request_count = 0
        self.total_cost = 0.0

    def scrape(self, url: str, **kwargs) -> dict:
        # æ£€æŸ¥é¢„ç®—
        if self.total_cost >= self.daily_budget:
            raise BudgetExceededError(
                f"è¶…å‡ºæ¯æ—¥é¢„ç®—: ${self.daily_budget}"
            )

        # æ‰§è¡Œçˆ¬å–
        result = self.app.scrape(url, **kwargs)

        # è®°å½•æˆæœ¬ï¼ˆå‡è®¾ $0.01/è¯·æ±‚ï¼‰
        cost = 0.01
        self.request_count += 1
        self.total_cost += cost

        logging.info(
            f"è¯·æ±‚ #{self.request_count} | "
            f"æˆæœ¬: ${cost:.4f} | "
            f"ç´¯è®¡: ${self.total_cost:.2f}/{self.daily_budget}"
        )

        return result
```

### 2. å¯†é’¥è½®æ¢

```python
import itertools

class RotatingFirecrawlClient:
    """æ”¯æŒå¯†é’¥è½®æ¢çš„å®¢æˆ·ç«¯"""

    def __init__(self, api_keys: List[str]):
        self.api_keys = itertools.cycle(api_keys)
        self.current_key = next(self.api_keys)
        self.app = FirecrawlApp(api_key=self.current_key)

    def scrape(self, url: str, **kwargs) -> dict:
        try:
            return self.app.scrape(url, **kwargs)
        except RateLimitError:
            # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥
            self.current_key = next(self.api_keys)
            self.app = FirecrawlApp(api_key=self.current_key)
            logging.info(f"åˆ‡æ¢å¯†é’¥: {self.current_key[:10]}...")
            return self.app.scrape(url, **kwargs)

# ä½¿ç”¨
client = RotatingFirecrawlClient([
    os.getenv("FIRECRAWL_API_KEY"),
    os.getenv("FIRECRAWL_API_KEY_BACKUP_1"),
    os.getenv("FIRECRAWL_API_KEY_BACKUP_2"),
    os.getenv("FIRECRAWL_API_KEY_BACKUP_3"),
])
```

---

## ğŸ“‹ æœ€ä½³å®è·µæ¸…å•

### âœ… å¿…é¡»åš

- [ ] ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨ API å¯†é’¥
- [ ] å®ç°å®Œæ•´çš„é”™è¯¯å¤„ç†ï¼ˆtry-exceptï¼‰
- [ ] è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´ï¼ˆ60 ç§’ï¼‰
- [ ] å¯ç”¨ç¼“å­˜ï¼ˆmax_ageï¼‰
- [ ] è®°å½•æ‰€æœ‰ API è°ƒç”¨æ—¥å¿—
- [ ] éªŒè¯è¿”å›ç»“æœ
- [ ] ä½¿ç”¨ `only_main_content=True`

### âŒ ç¦æ­¢åš

- [ ] ç¡¬ç¼–ç  API å¯†é’¥
- [ ] è·³è¿‡é”™è¯¯å¤„ç†
- [ ] æ— é™é‡è¯•
- [ ] ä¸æ£€æŸ¥ç¼“å­˜ç›´æ¥çˆ¬å–
- [ ] ä¸²è¡Œå¤„ç†å¤§é‡ URL
- [ ] ä¸è®°å½•æˆæœ¬

---

## ğŸ”— å‚è€ƒèµ„æº

- å®˜æ–¹æ–‡æ¡£: https://docs.firecrawl.dev/
- SDK GitHub: https://github.com/mendableai/firecrawl-py
- MCP æœåŠ¡å™¨: https://github.com/firecrawl/mcp-server-firecrawl
- Discord ç¤¾åŒº: https://discord.gg/firecrawl

---

_æœ€åæ›´æ–°: 2025-10-27_
_SDK ç‰ˆæœ¬: v2.x_
_ç»´æŠ¤è€…: HawaiiHub AI Team_
