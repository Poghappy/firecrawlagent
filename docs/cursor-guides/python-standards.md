# Python ä»£ç è§„èŒƒ

> **æœ€åæ›´æ–°**: 2025-10-27
> **Python ç‰ˆæœ¬**: 3.11+
> **ä»£ç é£æ ¼**: Black + Ruff

---

## ğŸ æ ¸å¿ƒåŸåˆ™

1. **ç±»å‹æ³¨è§£å¿…é¡»**ï¼šæ‰€æœ‰å‡½æ•°éƒ½è¦æœ‰å®Œæ•´ç±»å‹æ³¨è§£
2. **æ–‡æ¡£å­—ç¬¦ä¸²å¿…é¡»**ï¼šæ‰€æœ‰å…¬å¼€å‡½æ•°/ç±»éƒ½è¦æœ‰ä¸­æ–‡ docstring
3. **åŒå¼•å·æ ‡å‡†**ï¼šç»Ÿä¸€ä½¿ç”¨åŒå¼•å·
4. **æ ¼å¼åŒ–è‡ªåŠ¨**ï¼šä½¿ç”¨ Black/Ruff è‡ªåŠ¨æ ¼å¼åŒ–
5. **æµ‹è¯•é©±åŠ¨**ï¼šä½¿ç”¨ pytestï¼Œæµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%

---

## âœ… ç±»å‹æ³¨è§£ï¼ˆå¼ºåˆ¶ï¼‰

### åŸºæœ¬è§„åˆ™

```python
# âœ… æ­£ç¡®ï¼šå®Œæ•´çš„ç±»å‹æ³¨è§£
from typing import Optional, Dict, List

def scrape_news(
    url: str,
    formats: List[str] = ["markdown"],
    timeout: int = 60
) -> Optional[Dict[str, str]]:
    """çˆ¬å–æ–°é—»å†…å®¹"""
    return {"markdown": content}

# âŒ é”™è¯¯ï¼šç¼ºå°‘ç±»å‹æ³¨è§£
def scrape_news(url, formats=["markdown"]):
    return {"markdown": content}
```

### å¸¸ç”¨ç±»å‹

| ç±»å‹          | ç”¨é€”   | ç¤ºä¾‹                     |
| ------------- | ------ | ------------------------ |
| `str`         | å­—ç¬¦ä¸² | `url: str`               |
| `int`         | æ•´æ•°   | `timeout: int`           |
| `float`       | æµ®ç‚¹æ•° | `cost: float`            |
| `bool`        | å¸ƒå°”å€¼ | `success: bool`          |
| `List[T]`     | åˆ—è¡¨   | `urls: List[str]`        |
| `Dict[K, V]`  | å­—å…¸   | `data: Dict[str, int]`   |
| `Optional[T]` | å¯é€‰   | `result: Optional[str]`  |
| `Union[A, B]` | è”åˆ   | `value: Union[str, int]` |
| `Tuple[A, B]` | å…ƒç»„   | `point: Tuple[int, int]` |

### Python 3.10+ æ–°è¯­æ³•

```python
# âœ… æ¨èï¼šä½¿ç”¨æ–°è¯­æ³•ï¼ˆPython 3.10+ï¼‰
def process(data: dict[str, int]) -> str | None:
    """ä½¿ç”¨æ–°è¯­æ³•"""
    pass

# âœ… ä¹Ÿå¯ä»¥ï¼šä½¿ç”¨æ—§è¯­æ³•ï¼ˆå…¼å®¹æ€§ï¼‰
from typing import Optional, Dict

def process(data: Dict[str, int]) -> Optional[str]:
    """ä½¿ç”¨æ—§è¯­æ³•"""
    pass
```

---

## ğŸ“– æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆå¼ºåˆ¶ï¼‰

### æ ‡å‡†æ ¼å¼

```python
def scrape_articles(
    sources: List[str],
    limit: int = 10,
    use_cache: bool = True
) -> List[Dict[str, str]]:
    """
    çˆ¬å–å¤šä¸ªæ–°é—»æºçš„æ–‡ç« 

    ä»æŒ‡å®šçš„æ–°é—»æºæ‰¹é‡çˆ¬å–æ–‡ç« å†…å®¹ï¼Œæ”¯æŒç¼“å­˜å’Œé™åˆ¶æ•°é‡ã€‚

    Args:
        sources: æ–°é—»æº URL åˆ—è¡¨
        limit: æ¯ä¸ªæºæœ€å¤šçˆ¬å–çš„æ–‡ç« æ•°ï¼Œé»˜è®¤ 10
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ True

    Returns:
        æ–‡ç« åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ç« åŒ…å«ï¼š
        - title: æ–‡ç« æ ‡é¢˜
        - url: æ–‡ç«  URL
        - content: æ–‡ç« å†…å®¹ï¼ˆMarkdownï¼‰
        - date: å‘å¸ƒæ—¥æœŸ

    Raises:
        ValueError: å½“ sources ä¸ºç©ºæ—¶
        RequestTimeoutError: å½“è¯·æ±‚è¶…æ—¶æ—¶

    Example:
        >>> sources = ["https://news.example.com"]
        >>> articles = scrape_articles(sources, limit=5)
        >>> print(len(articles))
        5

    Note:
        - ä½¿ç”¨ç¼“å­˜å¯èŠ‚çœ 50% æˆæœ¬
        - å»ºè®® limit â‰¤ 100
    """
    if not sources:
        raise ValueError("sources ä¸èƒ½ä¸ºç©º")

    # å®ç°...
```

### æœ€å°è¦æ±‚

```python
# âœ… æœ€å°æ–‡æ¡£ï¼ˆç®€å•å‡½æ•°ï¼‰
def clean_text(text: str) -> str:
    """
    æ¸…æ´—æ–‡æœ¬ï¼Œç§»é™¤å¤šä½™ç©ºç™½

    Args:
        text: åŸå§‹æ–‡æœ¬

    Returns:
        æ¸…æ´—åçš„æ–‡æœ¬
    """
    return text.strip()
```

---

## ğŸ¨ ä»£ç é£æ ¼

### 1. å¼•å·è§„åˆ™

```python
# âœ… æ­£ç¡®ï¼šä½¿ç”¨åŒå¼•å·
message = "çˆ¬å–æˆåŠŸ"
url = "https://example.com"
data = {"key": "value"}

# âŒ é”™è¯¯ï¼šä½¿ç”¨å•å¼•å·
message = 'çˆ¬å–æˆåŠŸ'
url = 'https://example.com'
data = {'key': 'value'}
```

### 2. å¯¼å…¥é¡ºåº

```python
# âœ… æ­£ç¡®çš„å¯¼å…¥é¡ºåº
# 1. æ ‡å‡†åº“
import json
import re
from datetime import datetime
from collections import defaultdict

# 2. ç¬¬ä¸‰æ–¹åº“
from firecrawl import FirecrawlApp
from dotenv import load_dotenv
import pandas as pd

# 3. æœ¬åœ°æ¨¡å—
from .utils import parse_date
from .config import FIRECRAWL_API_KEY
```

### 3. å‘½åè§„èŒƒ

| ç±»å‹ | è§„èŒƒ             | ç¤ºä¾‹                       |
| ---- | ---------------- | -------------------------- |
| å˜é‡ | snake_case       | `user_name`, `total_count` |
| å‡½æ•° | snake_case       | `scrape_article()`         |
| ç±»   | PascalCase       | `ArticleScraper`           |
| å¸¸é‡ | UPPER_SNAKE_CASE | `API_KEY`, `MAX_RETRIES`   |
| ç§æœ‰ | å‰ç¼€ `_`         | `_internal_method()`       |

```python
# âœ… æ­£ç¡®çš„å‘½å
class ArticleScraper:
    """æ–‡ç« çˆ¬è™«"""

    MAX_RETRIES = 3  # å¸¸é‡

    def __init__(self):
        self.api_key = ""  # å®ä¾‹å˜é‡
        self._cache = {}  # ç§æœ‰å˜é‡

    def scrape_article(self, url: str) -> str:  # å…¬å¼€æ–¹æ³•
        """çˆ¬å–æ–‡ç« """
        return self._fetch_content(url)

    def _fetch_content(self, url: str) -> str:  # ç§æœ‰æ–¹æ³•
        """è·å–å†…å®¹"""
        return ""

API_URL = "https://api.example.com"  # æ¨¡å—å¸¸é‡
```

### 4. è¡Œé•¿åº¦å’Œæ ¼å¼

```python
# âœ… æ­£ç¡®ï¼šæ¯è¡Œæœ€å¤š 88 å­—ç¬¦
result = app.scrape(
    url="https://example.com",
    formats=["markdown"],
    only_main_content=True
)

# âœ… æ­£ç¡®ï¼šé•¿å­—ç¬¦ä¸²æ¢è¡Œ
message = (
    "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ¶ˆæ¯ï¼Œ"
    "éœ€è¦æ¢è¡Œæ˜¾ç¤ºï¼Œ"
    "ä¿æŒå¯è¯»æ€§"
)

# âœ… æ­£ç¡®ï¼šé•¿åˆ—è¡¨æ¢è¡Œ
urls = [
    "https://example1.com",
    "https://example2.com",
    "https://example3.com",
]
```

---

## ğŸ§ª æµ‹è¯•è§„èŒƒ

### pytest æ ‡å‡†

```python
# âœ… æµ‹è¯•æ–‡ä»¶ï¼štests/test_scraper.py
import pytest
from unittest.mock import Mock, patch
from my_module import scrape_news

def test_scrape_success():
    """æµ‹è¯•ï¼šæˆåŠŸçˆ¬å–æ–‡ç« """
    # Arrange
    mock_result = Mock()
    mock_result.markdown = "# æµ‹è¯•æ–‡ç« "

    # Act
    with patch("firecrawl.FirecrawlApp.scrape", return_value=mock_result):
        result = scrape_news("https://test.com")

    # Assert
    assert result is not None
    assert "markdown" in result
    assert result["markdown"] == "# æµ‹è¯•æ–‡ç« "

def test_scrape_timeout():
    """æµ‹è¯•ï¼šè¶…æ—¶é‡è¯•æœºåˆ¶"""
    # Arrange & Act
    with patch("firecrawl.FirecrawlApp.scrape", side_effect=RequestTimeoutError):
        result = safe_scrape("https://test.com", max_retries=2)

    # Assert
    assert result is None

@pytest.mark.parametrize("url,expected", [
    ("https://test.com", True),
    ("invalid-url", False),
])
def test_validate_url(url: str, expected: bool):
    """æµ‹è¯•ï¼šURL éªŒè¯"""
    assert validate_url(url) == expected
```

### æµ‹è¯•è¦†ç›–ç‡

```bash
# è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=src --cov-report=term-missing

# è¦æ±‚ï¼šè¦†ç›–ç‡ â‰¥ 80%
```

---

## ğŸ”§ å·¥å…·é…ç½®

### Ruff é…ç½®

```toml
# pyproject.toml
[tool.ruff]
line-length = 88
target-version = "py311"

[tool.ruff.lint]
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "B",   # flake8-bugbear
    "C4",  # flake8-comprehensions
]

[tool.ruff.format]
quote-style = "double"  # å¼ºåˆ¶åŒå¼•å·
```

### mypy é…ç½®

```toml
# pyproject.toml
[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
```

---

## ğŸ“‹ ä»£ç æ£€æŸ¥æ¸…å•

### æäº¤å‰å¿…é¡»æ£€æŸ¥

- [ ] æ‰€æœ‰å‡½æ•°æœ‰ç±»å‹æ³¨è§£
- [ ] æ‰€æœ‰å…¬å¼€å‡½æ•°æœ‰æ–‡æ¡£å­—ç¬¦ä¸²
- [ ] ä½¿ç”¨åŒå¼•å·
- [ ] è¿è¡Œ `ruff format .`
- [ ] è¿è¡Œ `ruff check .`
- [ ] è¿è¡Œ `mypy --strict .`
- [ ] è¿è¡Œ `pytest`
- [ ] æµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%

---

## âš ï¸ å¸¸è§é”™è¯¯

### é”™è¯¯ 1: ç¼ºå°‘è¿”å›ç±»å‹

```python
# âŒ é”™è¯¯
def get_data():
    return {"key": "value"}

# âœ… æ­£ç¡®
def get_data() -> Dict[str, str]:
    return {"key": "value"}
```

### é”™è¯¯ 2: ä½¿ç”¨ `Any` ç±»å‹

```python
# âŒ é¿å…ï¼šä½¿ç”¨ Any
from typing import Any

def process(data: Any) -> Any:
    pass

# âœ… æ¨èï¼šå…·ä½“ç±»å‹
def process(data: Dict[str, int]) -> str:
    pass
```

### é”™è¯¯ 3: ç¼ºå°‘ docstring

```python
# âŒ é”™è¯¯ï¼šæ²¡æœ‰æ–‡æ¡£
def important_function(x, y):
    return x + y

# âœ… æ­£ç¡®ï¼šæœ‰æ–‡æ¡£
def important_function(x: int, y: int) -> int:
    """
    è®¡ç®—ä¸¤æ•°ä¹‹å’Œ

    Args:
        x: ç¬¬ä¸€ä¸ªæ•°
        y: ç¬¬äºŒä¸ªæ•°

    Returns:
        ä¸¤æ•°ä¹‹å’Œ
    """
    return x + y
```

---

_æœ€åæ›´æ–°: 2025-10-27_
_Python ç‰ˆæœ¬: 3.11+_
_ä»£ç é£æ ¼: Black + Ruff_
