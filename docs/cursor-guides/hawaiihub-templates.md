# HawaiiHub ä¸“é¡¹æ¨¡æ¿

> **æœ€åæ›´æ–°**: 2025-10-27
> **é€‚ç”¨åœºæ™¯**: å¤å¨å¤·æœ¬åœ°å†…å®¹é‡‡é›†

---

## ğŸŒ´ æ–°é—»çˆ¬å–æ¨¡æ¿

### å¤å¨å¤·æ–°é—»æº

```python
# ä¸»è¦æ–°é—»æº
HAWAII_NEWS_SOURCES = {
    "hawaiinewsnow": {
        "url": "https://www.hawaiinewsnow.com/",
        "name": "Hawaii News Now",
        "type": "general",
        "cache_ttl": 3600  # 1 å°æ—¶
    },
    "staradvertiser": {
        "url": "https://www.staradvertiser.com/",
        "name": "Honolulu Star-Advertiser",
        "type": "general",
        "cache_ttl": 3600
    },
    "civilbeat": {
        "url": "https://www.civilbeat.org/",
        "name": "Honolulu Civil Beat",
        "type": "investigative",
        "cache_ttl": 7200  # 2 å°æ—¶
    },
    "khon2": {
        "url": "https://www.khon2.com/",
        "name": "KHON2 News",
        "type": "general",
        "cache_ttl": 3600
    }
}

# åäººç¤¾åŒºæº
CHINESE_COMMUNITY_SOURCES = {
    "kauai_chinese": {
        "url": "https://www.kauaichineseassociation.org/news",
        "name": "Kauai Chinese Association",
        "type": "community",
        "cache_ttl": 86400  # 24 å°æ—¶
    }
}
```

### æ ‡å‡†çˆ¬å–æµç¨‹

```python
from typing import List, Dict
from firecrawl import FirecrawlApp
import logging

def scrape_hawaii_news(
    sources: Dict[str, Dict],
    articles_per_source: int = 10
) -> List[Dict[str, str]]:
    """
    çˆ¬å–å¤å¨å¤·æ–°é—»

    Args:
        sources: æ–°é—»æºé…ç½®å­—å…¸
        articles_per_source: æ¯ä¸ªæºçˆ¬å–çš„æ–‡ç« æ•°

    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
    all_articles = []

    for source_id, config in sources.items():
        try:
            logging.info(f"å¼€å§‹çˆ¬å–: {config['name']}")

            # 1. çˆ¬å–é¦–é¡µ
            homepage = app.scrape(
                url=config["url"],
                formats=["markdown"],
                only_main_content=True,
                max_age=config["cache_ttl"] * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            )

            # 2. æå–æ–‡ç« é“¾æ¥
            article_urls = extract_article_links(
                homepage.markdown,
                base_url=config["url"],
                limit=articles_per_source
            )

            logging.info(
                f"å‘ç° {len(article_urls)} ç¯‡æ–‡ç« : {config['name']}"
            )

            # 3. æ‰¹é‡çˆ¬å–æ–‡ç« 
            if article_urls:
                articles = app.batch_scrape(
                    urls=article_urls,
                    formats=["markdown"],
                    only_main_content=True
                )

                # 4. è§£æå’Œæ ‡è®°
                for article in articles:
                    if hasattr(article, "markdown"):
                        parsed = {
                            "source_id": source_id,
                            "source_name": config["name"],
                            "source_type": config["type"],
                            "url": article.url,
                            "title": article.metadata.title if article.metadata else "",
                            "content": article.markdown,
                            "scraped_at": datetime.now().isoformat()
                        }
                        all_articles.append(parsed)

            logging.info(
                f"æˆåŠŸçˆ¬å– {len(article_urls)} ç¯‡: {config['name']}"
            )

        except Exception as e:
            logging.error(f"çˆ¬å–å¤±è´¥ {config['name']}: {e}")
            continue

    return all_articles

# ä½¿ç”¨
articles = scrape_hawaii_news(HAWAII_NEWS_SOURCES, articles_per_source=10)
```

---

## ğŸª å•†å®¶ä¿¡æ¯çˆ¬å–

### Yelp å¤å¨å¤·é¤å…

```python
YELP_SEARCH_URLS = [
    # æª€é¦™å±±ä¸­é¤
    "https://www.yelp.com/search?find_desc=Chinese&find_loc=Honolulu,+HI",
    # æª€é¦™å±±æ—¥æ–™
    "https://www.yelp.com/search?find_desc=Japanese&find_loc=Honolulu,+HI",
    # æª€é¦™å±±éŸ©é¤
    "https://www.yelp.com/search?find_desc=Korean&find_loc=Honolulu,+HI",
]

def scrape_yelp_restaurants(
    search_urls: List[str],
    max_restaurants: int = 50
) -> List[Dict]:
    """
    çˆ¬å– Yelp é¤å…ä¿¡æ¯

    Args:
        search_urls: Yelp æœç´¢ URL åˆ—è¡¨
        max_restaurants: æ¯ä¸ªæœç´¢æœ€å¤šçˆ¬å–çš„é¤å…æ•°

    Returns:
        é¤å…ä¿¡æ¯åˆ—è¡¨
    """
    app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
    restaurants = []

    for url in search_urls:
        try:
            # çˆ¬å–æœç´¢ç»“æœé¡µ
            result = app.scrape(
                url=url,
                formats=["markdown"],
                only_main_content=True,
                max_age=604800000  # 7 å¤©ç¼“å­˜
            )

            # æå–é¤å…é“¾æ¥
            restaurant_urls = extract_yelp_restaurant_links(
                result.markdown,
                limit=max_restaurants
            )

            # æ‰¹é‡çˆ¬å–é¤å…è¯¦æƒ…
            details = app.batch_scrape(
                urls=restaurant_urls,
                formats=["markdown"]
            )

            # è§£æé¤å…ä¿¡æ¯
            for detail in details:
                if hasattr(detail, "markdown"):
                    info = parse_yelp_restaurant(detail.markdown)
                    if info:
                        restaurants.append(info)

        except Exception as e:
            logging.error(f"çˆ¬å– Yelp å¤±è´¥: {e}")

    return restaurants

def parse_yelp_restaurant(markdown: str) -> Dict | None:
    """è§£æ Yelp é¤å… Markdown"""
    # TODO: å®ç°è§£æé€»è¾‘
    # æå–ï¼šåç§°ã€è¯„åˆ†ã€åœ°å€ã€ç”µè¯ã€è¥ä¸šæ—¶é—´ã€è¯„è®ºç­‰
    return {
        "name": "",
        "rating": 0.0,
        "address": "",
        "phone": "",
        "hours": "",
        "price_range": "",
        "cuisine": []
    }
```

---

## ğŸ  ç§Ÿæˆ¿ä¿¡æ¯çˆ¬å–

### Craigslist å¤å¨å¤·ç§Ÿæˆ¿

```python
CRAIGSLIST_HOUSING_URLS = [
    # æª€é¦™å±±ç§Ÿæˆ¿
    "https://honolulu.craigslist.org/search/apa",
    # æ•´ä¸ªå¤å¨å¤·
    "https://honolulu.craigslist.org/search/oah/apa",
]

def scrape_housing_listings(
    urls: List[str],
    max_listings: int = 100
) -> List[Dict]:
    """
    çˆ¬å–ç§Ÿæˆ¿ä¿¡æ¯

    Args:
        urls: Craigslist URL åˆ—è¡¨
        max_listings: æœ€å¤§åˆ—è¡¨æ•°

    Returns:
        ç§Ÿæˆ¿åˆ—è¡¨
    """
    app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
    listings = []

    for url in urls:
        try:
            # çˆ¬å–åˆ—è¡¨é¡µ
            result = app.scrape(
                url=url,
                formats=["markdown"],
                only_main_content=True,
                max_age=3600000  # 1 å°æ—¶ç¼“å­˜ï¼ˆæ›´æ–°å¿«ï¼‰
            )

            # æå–ç§Ÿæˆ¿é“¾æ¥
            listing_urls = extract_craigslist_links(
                result.markdown,
                limit=max_listings
            )

            # æ‰¹é‡çˆ¬å–è¯¦æƒ…
            details = app.batch_scrape(
                urls=listing_urls,
                formats=["markdown"]
            )

            # è§£æç§Ÿæˆ¿ä¿¡æ¯
            for detail in details:
                if hasattr(detail, "markdown"):
                    info = parse_housing_listing(detail.markdown)
                    if info:
                        listings.append(info)

        except Exception as e:
            logging.error(f"çˆ¬å–ç§Ÿæˆ¿ä¿¡æ¯å¤±è´¥: {e}")

    return listings

def parse_housing_listing(markdown: str) -> Dict | None:
    """è§£æç§Ÿæˆ¿åˆ—è¡¨ Markdown"""
    # TODO: å®ç°è§£æé€»è¾‘
    # æå–ï¼šæ ‡é¢˜ã€ä»·æ ¼ã€åœ°å€ã€å§å®¤æ•°ã€é¢ç§¯ã€è”ç³»æ–¹å¼ç­‰
    return {
        "title": "",
        "price": 0,
        "address": "",
        "bedrooms": 0,
        "bathrooms": 0,
        "sqft": 0,
        "contact": "",
        "posted_date": ""
    }
```

---

## ğŸ§¹ æ•°æ®æ¸…æ´—è§„èŒƒ

### æ–°é—»å†…å®¹æ¸…æ´—

```python
import re

def clean_news_content(content: str) -> str:
    """
    æ¸…æ´—æ–°é—»å†…å®¹

    ç§»é™¤ï¼š
    - å¹¿å‘Š
    - å¯¼èˆªæ 
    - ç›¸å…³æ–‡ç« æ¨è
    - ç¤¾äº¤åª’ä½“æŒ‰é’®
    - Cookie é€šçŸ¥
    """
    # Firecrawl çš„ only_main_content å·²ç»åšäº†åŸºç¡€æ¸…æ´—
    # è¿™é‡Œåªéœ€è¦é¢å¤–å¤„ç†ç‰¹å®šæ¨¡å¼

    # ç§»é™¤å¹¿å‘Šæ¨¡å¼
    ad_patterns = [
        r"Subscribe to.*newsletter",
        r"Advertisement",
        r"ADVERTISEMENT",
        r"Sponsored Content",
        r"Related Articles?",
        r"You May Also Like",
        r"Recommended For You",
        r"Share on (Facebook|Twitter|LinkedIn|Email)",
        r"Follow us on.*",
        r"Sign up for.*",
        r"Cookie Policy",
        r"Accept Cookies"
    ]

    for pattern in ad_patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    # è§„èŒƒåŒ–ç©ºç™½
    content = re.sub(r"\n{3,}", "\n\n", content)  # å¤šä¸ªæ¢è¡Œ â†’ 2 ä¸ªæ¢è¡Œ
    content = re.sub(r" {2,}", " ", content)      # å¤šä¸ªç©ºæ ¼ â†’ 1 ä¸ªç©ºæ ¼

    # ç§»é™¤é¦–å°¾ç©ºç™½
    content = content.strip()

    return content
```

### ä»·æ ¼æ ¼å¼åŒ–

```python
def parse_price(price_text: str) -> float | None:
    """
    è§£æä»·æ ¼æ–‡æœ¬

    æ”¯æŒæ ¼å¼ï¼š
    - $1,500
    - $1500/mo
    - 1500 per month
    """
    if not price_text:
        return None

    # ç§»é™¤éæ•°å­—å­—ç¬¦ï¼ˆä¿ç•™é€—å·å’Œç‚¹ï¼‰
    price_str = re.sub(r"[^\d,.]", "", price_text)

    # ç§»é™¤é€—å·
    price_str = price_str.replace(",", "")

    try:
        return float(price_str)
    except:
        return None

# ä½¿ç”¨
price = parse_price("$1,500/mo")  # 1500.0
```

### æ—¥æœŸè§£æ

```python
from dateutil.parser import parse as parse_date

def normalize_date(date_text: str) -> str | None:
    """
    è§„èŒƒåŒ–æ—¥æœŸæ ¼å¼

    æ”¯æŒï¼š
    - "2 hours ago"
    - "Yesterday"
    - "Oct 27, 2025"
    - "10/27/2025"

    Returns:
        ISO 8601 æ ¼å¼ï¼šYYYY-MM-DD
    """
    if not date_text:
        return None

    try:
        # å¤„ç†ç›¸å¯¹æ—¶é—´
        if "ago" in date_text.lower():
            return datetime.now().date().isoformat()
        elif "yesterday" in date_text.lower():
            return (datetime.now() - timedelta(days=1)).date().isoformat()
        elif "today" in date_text.lower():
            return datetime.now().date().isoformat()
        else:
            # è§£æç»å¯¹æ—¥æœŸ
            dt = parse_date(date_text)
            return dt.date().isoformat()
    except:
        return None

# ä½¿ç”¨
date = normalize_date("2 hours ago")  # "2025-10-27"
```

---

## ğŸ“Š æ•°æ®å­˜å‚¨æ¨¡æ¿

### Pydantic æ•°æ®æ¨¡å‹

```python
from pydantic import BaseModel, HttpUrl, Field
from typing import Optional
from datetime import date

class NewsArticle(BaseModel):
    """æ–°é—»æ–‡ç« æ¨¡å‹"""
    source_id: str
    source_name: str
    url: HttpUrl
    title: str = Field(..., min_length=1, max_length=500)
    content: str
    author: Optional[str] = None
    published_date: Optional[date] = None
    scraped_at: str

    class Config:
        extra = "allow"

class Restaurant(BaseModel):
    """é¤å…ä¿¡æ¯æ¨¡å‹"""
    name: str = Field(..., min_length=1)
    url: HttpUrl
    rating: float = Field(..., ge=0, le=5)
    address: str
    phone: Optional[str] = None
    cuisine: List[str] = []
    price_range: Optional[str] = None

    class Config:
        extra = "allow"

class HousingListing(BaseModel):
    """ç§Ÿæˆ¿åˆ—è¡¨æ¨¡å‹"""
    title: str
    url: HttpUrl
    price: float = Field(..., gt=0)
    address: str
    bedrooms: int = Field(..., ge=0)
    bathrooms: float = Field(..., ge=0)
    sqft: Optional[int] = None
    posted_date: Optional[date] = None

    class Config:
        extra = "allow"
```

### ä¿å­˜åˆ°æ•°æ®åº“

```python
import json
from datetime import datetime

def save_articles(articles: List[Dict], format: str = "all"):
    """
    ä¿å­˜æ–‡ç« æ•°æ®

    Args:
        articles: æ–‡ç« åˆ—è¡¨
        format: ä¿å­˜æ ¼å¼ (json|csv|all)
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if format in ["json", "all"]:
        # ä¿å­˜ JSON
        filename = f"hawaii_news_{timestamp}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSON å·²ä¿å­˜: {filename}")

    if format in ["csv", "all"]:
        # ä¿å­˜ CSV
        filename = f"hawaii_news_{timestamp}.csv"
        import pandas as pd
        df = pd.DataFrame(articles)
        df.to_csv(filename, index=False, encoding="utf-8")
        print(f"âœ… CSV å·²ä¿å­˜: {filename}")
```

---

## ğŸ¯ å®šæ—¶ä»»åŠ¡æ¨¡æ¿

### æ¯æ—¥æ–°é—»èšåˆ

```python
import schedule
import time

def daily_news_job():
    """æ¯æ—¥æ–°é—»èšåˆä»»åŠ¡"""
    logging.info("å¼€å§‹æ¯æ—¥æ–°é—»èšåˆ")

    # 1. çˆ¬å–æ–°é—»
    articles = scrape_hawaii_news(HAWAII_NEWS_SOURCES)

    # 2. æ¸…æ´—æ•°æ®
    for article in articles:
        article["content"] = clean_news_content(article["content"])

    # 3. ä¿å­˜æ•°æ®
    save_articles(articles, format="all")

    # 4. å‘é€æ‘˜è¦
    send_daily_summary(articles)

    logging.info(f"å®Œæˆæ¯æ—¥æ–°é—»èšåˆ: {len(articles)} ç¯‡æ–‡ç« ")

# æ¯å¤©æ—©ä¸Š 6:00 æ‰§è¡Œ
schedule.every().day.at("06:00").do(daily_news_job)

# è¿è¡Œè°ƒåº¦å™¨
while True:
    schedule.run_pending()
    time.sleep(60)
```

---

_æœ€åæ›´æ–°: 2025-10-27_
_é€‚ç”¨åœºæ™¯: HawaiiHub æœ¬åœ°å†…å®¹é‡‡é›†_
_ç»´æŠ¤è€…: HawaiiHub AI Team_
