# Firecrawl å®Œæ•´å­¦ä¹ æ‰‹å†Œ

> **åˆ›å»ºæ—¶é—´**: 2025-10-27
> **æ–‡æ¡£æ¥æº**: å®˜æ–¹ä¸­æ–‡æ–‡æ¡£ + å®æˆ˜ç»éªŒ
> **é€‚ç”¨ç‰ˆæœ¬**: v2.4.0
> **ç›®æ ‡è¯»è€…**: HawaiiHub é¡¹ç›®å›¢é˜Ÿ

---

## ğŸ“– ç›®å½•

- [ç¬¬ä¸€ç« ï¼šFirecrawl åŸºç¡€](#chapter-1)
- [ç¬¬äºŒç« ï¼šæ ¸å¿ƒåŠŸèƒ½è¯¦è§£](#chapter-2)
- [ç¬¬ä¸‰ç« ï¼šPython SDK å®Œå…¨æŒ‡å—](#chapter-3)
- [ç¬¬å››ç« ï¼šé«˜çº§ç‰¹æ€§](#chapter-4)
- [ç¬¬äº”ç« ï¼šHawaiiHub å®æˆ˜åº”ç”¨](#chapter-5)
- [ç¬¬å…­ç« ï¼šæœ€ä½³å®è·µä¸ä¼˜åŒ–](#chapter-6)
- [é™„å½•](#appendix)

---

<div id="chapter-1">

## ç¬¬ä¸€ç« ï¼šFirecrawl åŸºç¡€

</div>

### 1.1 ä»€ä¹ˆæ˜¯ Firecrawlï¼Ÿ

**Firecrawl** æ˜¯ä¸€ä¸ªå°†ç½‘ç«™è½¬æ¢ä¸º LLM-readyï¼ˆé€‚é…å¤§è¯­è¨€æ¨¡å‹ï¼‰æ•°æ®çš„å¼€æº API æœåŠ¡ã€‚

**æ ¸å¿ƒä»·å€¼**:

- âœ… **é›¶é…ç½®**: æ— éœ€ sitemapï¼Œè‡ªåŠ¨å‘ç°æ‰€æœ‰é¡µé¢
- âœ… **LLM-ready**: è¾“å‡ºå¹²å‡€çš„ Markdownã€ç»“æ„åŒ– JSON
- âœ… **è§£å†³æ£˜æ‰‹é—®é¢˜**: ä»£ç†ã€åçˆ¬ã€JS æ¸²æŸ“ã€é€Ÿç‡é™åˆ¶
- âœ… **æé€Ÿ**: æ•°ç§’å†…è¿”å›ç»“æœï¼Œæ”¯æŒé«˜åååœºæ™¯
- âœ… **é«˜åº¦å¯å®šåˆ¶**: æ’é™¤æ ‡ç­¾ã€è‡ªå®šä¹‰ headersã€è®¾ç½®æ·±åº¦

### 1.2 å¼€æº vs äº‘ç«¯

| åŠŸèƒ½                        | å¼€æºç‰ˆ | äº‘ç«¯ç‰ˆ         |
| --------------------------- | ------ | -------------- |
| åŸºç¡€çˆ¬å–                    | âœ…     | âœ…             |
| é«˜çº§åçˆ¬ç»•è¿‡                | âŒ     | âœ…             |
| å…¨çƒä»£ç†æ±                   | âŒ     | âœ…             |
| Actionsï¼ˆé¡µé¢äº¤äº’ï¼‰         | âŒ     | âœ…             |
| Change Trackingï¼ˆå˜æ›´ç›‘æ§ï¼‰ | âŒ     | âœ…             |
| 99.9% å¯ç”¨æ€§                | âŒ     | âœ…             |
| å¹¶å‘ä¼˜åŒ–                    | æœ‰é™   | âœ… é«˜æ€§èƒ½      |
| å…è´¹é¢åº¦                    | -      | 500 credits/æœˆ |

**HawaiiHub é€‰æ‹©**: âœ… äº‘ç«¯ç‰ˆï¼ˆé«˜çº§åŠŸèƒ½ + ç¨³å®šæ€§ä¿è¯ï¼‰

### 1.3 5 å¤§æ ¸å¿ƒåŠŸèƒ½

#### 1. **Scrapeï¼ˆå•é¡µçˆ¬å–ï¼‰**

- è¾“å…¥ï¼šå•ä¸ª URL
- è¾“å‡ºï¼šMarkdownã€HTMLã€JSONã€æˆªå›¾ã€æ‘˜è¦
- é€‚ç”¨ï¼šå•ç¯‡æ–‡ç« ã€äº§å“é¡µé¢

#### 2. **Crawlï¼ˆæ•´ç«™çˆ¬å–ï¼‰**

- è¾“å…¥ï¼šèµ·å§‹ URL
- è¾“å‡ºï¼šæ‰€æœ‰å­é¡µé¢çš„å†…å®¹
- é€‚ç”¨ï¼šæ•´ç«™æ•°æ®é‡‡é›†ã€æ–‡æ¡£çˆ¬å–

#### 3. **Mapï¼ˆç«™ç‚¹åœ°å›¾ï¼‰**

- è¾“å…¥ï¼šç½‘ç«™ URL
- è¾“å‡ºï¼šæ‰€æœ‰ URL åˆ—è¡¨
- é€‚ç”¨ï¼šå¿«é€Ÿå‘ç°ç½‘ç«™ç»“æ„

#### 4. **Searchï¼ˆæ™ºèƒ½æœç´¢ï¼‰**

- è¾“å…¥ï¼šæœç´¢å…³é”®è¯
- è¾“å‡ºï¼šæœç´¢ç»“æœ + å®Œæ•´å†…å®¹
- é€‚ç”¨ï¼šå†…å®¹å‘ç°ã€ç«å“ç›‘æ§

#### 5. **Extractï¼ˆæ•°æ®æå–ï¼‰**

- è¾“å…¥ï¼šURL + Schema/Prompt
- è¾“å‡ºï¼šç»“æ„åŒ–æ•°æ®
- é€‚ç”¨ï¼šè¡¨å•æ•°æ®ã€äº§å“ä¿¡æ¯

### 1.4 å¿«é€Ÿä¸Šæ‰‹ï¼ˆ5 åˆ†é’Ÿï¼‰

#### æ­¥éª¤ 1: å®‰è£… SDK

```bash
pip install firecrawl-py python-dotenv
```

#### æ­¥éª¤ 2: é…ç½® API Key

```bash
# .env
FIRECRAWL_API_KEY=fc-your-key-here
```

#### æ­¥éª¤ 3: ç¬¬ä¸€ä¸ªçˆ¬å–

```python
from firecrawl import FirecrawlApp
import os
from dotenv import load_dotenv

load_dotenv()
app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))

# çˆ¬å–å•é¡µ
result = app.scrape(
    url="https://www.hawaiinewsnow.com/",
    formats=["markdown"],
    only_main_content=True
)

print(result.markdown[:500])  # å‰ 500 å­—ç¬¦
```

**é¢„æœŸè¾“å‡º**:

```
âœ… æˆåŠŸè¿”å›å¹²å‡€çš„ Markdown
ğŸ“„ æ ‡é¢˜: Hawaii News Now - Breaking News...
ğŸ“ å†…å®¹é•¿åº¦: ~36,000 å­—ç¬¦
â±ï¸ è€—æ—¶: ~1 ç§’
```

---

<div id="chapter-2">

## ç¬¬äºŒç« ï¼šæ ¸å¿ƒåŠŸèƒ½è¯¦è§£

</div>

### 2.1 Scrapeï¼ˆå•é¡µçˆ¬å–ï¼‰

#### åŸºæœ¬ç”¨æ³•

```python
from firecrawl import FirecrawlApp
app = FirecrawlApp(api_key="fc-your-key")

result = app.scrape(
    url="https://example.com",
    formats=["markdown"],         # è¾“å‡ºæ ¼å¼
    only_main_content=True       # åªè¦ä¸»è¦å†…å®¹
)

# è®¿é—®ç»“æœï¼ˆSDK v2ï¼‰
content = result.markdown        # âœ… å±æ€§è®¿é—®
title = result.metadata.title
url = result.url
```

#### æ”¯æŒçš„è¾“å‡ºæ ¼å¼

| æ ¼å¼         | ç”¨é€”          | ç¤ºä¾‹           |
| ------------ | ------------- | -------------- |
| `markdown`   | LLM è®­ç»ƒã€RAG | åšå®¢æ–‡ç« ã€æ–°é—» |
| `html`       | ä¿ç•™ç»“æ„      | å¤æ‚å¸ƒå±€       |
| `rawHtml`    | åŸå§‹ HTML     | è°ƒè¯•ã€åˆ†æ     |
| `screenshot` | å¯è§†åŒ–        | é¡µé¢å¿«ç…§       |
| `links`      | é“¾æ¥æå–      | SEO åˆ†æ       |
| `json`       | ç»“æ„åŒ–æ•°æ®    | äº§å“ä¿¡æ¯       |
| `summary`    | æ‘˜è¦          | å¿«é€Ÿé¢„è§ˆ       |
| `images`     | å›¾ç‰‡ URL      | å›¾åƒé‡‡é›†       |

#### å¤šæ ¼å¼åŒæ—¶è·å–

```python
result = app.scrape(
    url="https://example.com",
    formats=["markdown", "html", "links", "screenshot"]
)

# è®¿é—®ä¸åŒæ ¼å¼
print(result.markdown)     # Markdown å†…å®¹
print(result.html)         # HTML å†…å®¹
print(result.links)        # é“¾æ¥åˆ—è¡¨
print(result.screenshot)   # Base64 ç¼–ç çš„æˆªå›¾
```

#### JSON æ¨¡å¼ï¼ˆç»“æ„åŒ–æå–ï¼‰

##### æ–¹å¼ 1: ä½¿ç”¨ Schema

```python
from pydantic import BaseModel

class Article(BaseModel):
    title: str
    author: str
    date: str
    summary: str

result = app.scrape(
    url="https://news.example.com/article",
    formats=[{
        "type": "json",
        "schema": Article.model_json_schema()
    }]
)

print(result.json)  # ç»“æ„åŒ–æ•°æ®
```

##### æ–¹å¼ 2: ä½¿ç”¨ Promptï¼ˆæ—  Schemaï¼‰

```python
result = app.scrape(
    url="https://restaurant.com",
    formats=[{
        "type": "json",
        "prompt": "æå–é¤å…åç§°ã€åœ°å€ã€ç”µè¯ã€è¥ä¸šæ—¶é—´"
    }]
)

print(result.json)  # LLM è‡ªåŠ¨å†³å®šç»“æ„
```

#### Actionsï¼ˆé¡µé¢äº¤äº’ï¼‰

```python
result = app.scrape(
    url="https://google.com",
    formats=["markdown", "screenshot"],
    actions=[
        {"type": "wait", "milliseconds": 2000},
        {"type": "write", "text": "Firecrawl", "selector": "textarea[name='q']"},
        {"type": "press", "key": "Enter"},
        {"type": "wait", "milliseconds": 3000},
        {"type": "screenshot", "fullPage": False}
    ]
)
```

**æ”¯æŒçš„ Actions**:

- `wait` - ç­‰å¾…æŒ‡å®šæ—¶é—´
- `click` - ç‚¹å‡»å…ƒç´ 
- `write` - è¾“å…¥æ–‡æœ¬
- `press` - æŒ‰é”®
- `scroll` - æ»šåŠ¨é¡µé¢
- `screenshot` - æˆªå›¾

### 2.2 Crawlï¼ˆæ•´ç«™çˆ¬å–ï¼‰

#### åŸºæœ¬ç”¨æ³•

```python
# é˜»å¡å¼ï¼ˆç­‰å¾…å®Œæˆï¼‰
crawl_result = app.crawl(
    url="https://example.com/blog/",
    limit=100,                    # æœ€å¤šçˆ¬å– 100 é¡µ
    scrape_options={
        "formats": ["markdown"],
        "only_main_content": True
    }
)

print(f"çˆ¬å–äº† {len(crawl_result.data)} ä¸ªé¡µé¢")
for page in crawl_result.data:
    print(f"- {page.url}: {len(page.markdown)} å­—ç¬¦")
```

#### éé˜»å¡å¼ï¼ˆå¯åŠ¨åè½®è¯¢ï¼‰

```python
# å¯åŠ¨çˆ¬å–ä»»åŠ¡
crawl_job = app.start_crawl(
    url="https://example.com",
    limit=100
)

print(f"ä»»åŠ¡ ID: {crawl_job.id}")

# æ£€æŸ¥çŠ¶æ€
status = app.get_crawl_status(crawl_job.id)
print(f"çŠ¶æ€: {status.status}")
print(f"å·²å®Œæˆ: {status.completed}/{status.total}")
```

#### é«˜çº§é…ç½®

```python
crawl_result = app.crawl(
    url="https://example.com",
    limit=200,
    max_depth=3,                  # æœ€å¤§çˆ¬å–æ·±åº¦
    allow_subdomains=False,        # æ˜¯å¦åŒ…å«å­åŸŸå
    crawl_entire_domain=False,     # æ˜¯å¦çˆ¬å–æ•´ä¸ªåŸŸå
    scrape_options={
        "formats": ["markdown", {"type": "json", "prompt": "æå–æ ‡é¢˜å’Œæ—¥æœŸ"}],
        "only_main_content": True,
        "exclude_tags": ["nav", "footer", "aside"],
        "max_age": 3600000,        # ç¼“å­˜ 1 å°æ—¶
    }
)
```

#### Crawl å‚æ•°è¯¦è§£

| å‚æ•°                  | ç±»å‹   | é»˜è®¤å€¼ | è¯´æ˜             |
| --------------------- | ------ | ------ | ---------------- |
| `url`                 | string | -      | èµ·å§‹ URLï¼ˆå¿…éœ€ï¼‰ |
| `limit`               | int    | 10000  | æœ€å¤§çˆ¬å–é¡µé¢æ•°   |
| `max_depth`           | int    | -      | æœ€å¤§çˆ¬å–æ·±åº¦     |
| `allow_subdomains`    | bool   | False  | æ˜¯å¦åŒ…å«å­åŸŸå   |
| `crawl_entire_domain` | bool   | False  | æ˜¯å¦çˆ¬å–æ•´ä¸ªåŸŸå |
| `scrape_options`      | object | {}     | Scrape é€‰é¡¹      |
| `webhook`             | string | -      | Webhook URL      |

#### WebSocket å®æ—¶ç›‘æ§

```python
from firecrawl.utils import create_watcher

# å¯åŠ¨çˆ¬å–ä»»åŠ¡
crawl_job = app.start_crawl(
    url="https://example.com",
    limit=50
)

# åˆ›å»ºç›‘æ§å™¨
watcher = create_watcher(crawl_job.id, app)

# æ³¨å†Œäº‹ä»¶å¤„ç†å™¨
@watcher.on("page")
def on_page(page):
    print(f"âœ… çˆ¬å–: {page.url} ({len(page.markdown)} å­—ç¬¦)")

@watcher.on("completed")
def on_completed(data):
    print(f"ğŸ‰ å®Œæˆ! å…± {len(data)} é¡µ")

@watcher.on("failed")
def on_failed(error):
    print(f"âŒ å¤±è´¥: {error}")

# å¼€å§‹ç›‘æ§
watcher.start()
```

### 2.3 Mapï¼ˆç«™ç‚¹åœ°å›¾ï¼‰

#### åŸºæœ¬ç”¨æ³•

```python
map_result = app.map(
    url="https://example.com"
)

print(f"å‘ç° {len(map_result.links)} ä¸ª URL")
for url in map_result.links[:10]:
    print(f"- {url}")
```

#### é«˜çº§ç”¨æ³•

```python
map_result = app.map(
    url="https://example.com",
    search="blog",              # åªåŒ…å« "blog" çš„ URL
    ignore_sitemap=False,       # ä½¿ç”¨ sitemap
    include_subdomains=False,   # ä¸åŒ…å«å­åŸŸå
    limit=5000                  # æœ€å¤šè¿”å› 5000 ä¸ª URL
)
```

**åº”ç”¨åœºæ™¯**:

1. **å…ˆ Map å Crawl**: å…ˆå‘ç°æ‰€æœ‰ URLï¼Œå†æ‰¹é‡çˆ¬å–
2. **é€‰æ‹©æ€§çˆ¬å–**: åªçˆ¬å–ç‰¹å®šè·¯å¾„
3. **ç«™ç‚¹åˆ†æ**: äº†è§£ç½‘ç«™ç»“æ„

### 2.4 Searchï¼ˆæ™ºèƒ½æœç´¢ï¼‰

#### åŸºæœ¬ç”¨æ³•

```python
search_result = app.search(
    query="å¤å¨å¤· åäºº é¤å…",
    sources=[{"type": "web"}],  # æœç´¢æ¥æºï¼šwebã€newsã€images
    limit=10
)

for item in search_result:
    print(f"æ ‡é¢˜: {item.title}")
    print(f"URL: {item.url}")
```

#### æœç´¢ + çˆ¬å–

```python
search_result = app.search(
    query="Firecrawl Python tutorial",
    sources=[{"type": "web"}],
    limit=5,
    scrape_options={
        "formats": ["markdown"],
        "only_main_content": True
    }
)

# ç›´æ¥è·å–æœç´¢ç»“æœçš„å®Œæ•´å†…å®¹
for item in search_result:
    print(f"\n{item.title}")
    print(f"{item.markdown[:200]}...")
```

#### æœç´¢åˆ†ç±»ï¼ˆv2.1.0+ï¼‰

```python
# GitHub æœç´¢
github_results = app.search(
    query="firecrawl python",
    categories=["github"]  # GitHub ä»“åº“ã€ä»£ç ã€Issuesã€æ–‡æ¡£
)

# å­¦æœ¯æœç´¢
research_results = app.search(
    query="AI machine learning",
    categories=["research"]  # arXivã€Natureã€IEEEã€PubMed
)

# PDF æœç´¢ï¼ˆv2.4.0+ï¼‰
pdf_results = app.search(
    query="ç ”ç©¶æŠ¥å‘Š",
    categories=["pdf"]
)
```

### 2.5 Batch Scrapeï¼ˆæ‰¹é‡çˆ¬å–ï¼‰

#### åŸºæœ¬ç”¨æ³•

```python
urls = [
    "https://example.com/page1",
    "https://example.com/page2",
    "https://example.com/page3"
]

# é˜»å¡å¼ï¼ˆç­‰å¾…å®Œæˆï¼‰
batch_result = app.batch_scrape(
    urls=urls,
    formats=["markdown"],
    only_main_content=True
)

for page in batch_result:
    print(f"{page.url}: {len(page.markdown)} å­—ç¬¦")
```

#### éé˜»å¡å¼

```python
# å¯åŠ¨æ‰¹é‡ä»»åŠ¡
batch_job = app.start_batch_scrape(
    urls=urls,
    formats=["markdown"]
)

# æ£€æŸ¥çŠ¶æ€
status = app.get_batch_scrape_status(batch_job.id)
print(f"è¿›åº¦: {status.completed}/{status.total}")
```

---

<div id="chapter-3">

## ç¬¬ä¸‰ç« ï¼šPython SDK å®Œå…¨æŒ‡å—

</div>

### 3.1 å®‰è£…ä¸åˆå§‹åŒ–

#### å®‰è£…

```bash
pip install firecrawl-py python-dotenv
```

#### åˆå§‹åŒ–

```python
from firecrawl import FirecrawlApp
import os
from dotenv import load_dotenv

load_dotenv()

# æ–¹å¼ 1: ç¯å¢ƒå˜é‡
app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))

# æ–¹å¼ 2: ç›´æ¥ä¼ å‚
app = FirecrawlApp(api_key="fc-your-key-here")
```

### 3.2 SDK v2 é‡è¦å˜åŒ–

#### å‘½åçº¦å®šå˜åŒ–

```python
# âœ… v2 æ­£ç¡®ï¼ˆä¸‹åˆ’çº¿ï¼‰
result = app.scrape(
    url="...",
    only_main_content=True,    # âœ… ä¸‹åˆ’çº¿
    max_age=172800000,          # âœ… ä¸‹åˆ’çº¿
    block_ads=True,             # âœ… ä¸‹åˆ’çº¿
    skip_tls_verification=True  # âœ… ä¸‹åˆ’çº¿
)

# âŒ v1 é”™è¯¯ï¼ˆé©¼å³°å¼ï¼‰
result = app.scrape(
    url="...",
    onlyMainContent=True,       # âŒ ä¼šæŠ¥é”™
    maxAge=172800000            # âŒ ä¼šæŠ¥é”™
)
```

#### è¿”å›å€¼ç±»å‹å˜åŒ–

```python
# âœ… v2 æ­£ç¡®ï¼ˆDocument å¯¹è±¡ï¼‰
result = app.scrape(url="...", formats=["markdown"])

content = result.markdown       # âœ… å±æ€§è®¿é—®
title = result.metadata.title   # âœ… å…ƒæ•°æ®è®¿é—®
url = result.url                # âœ… URL è®¿é—®

# âŒ v1 é”™è¯¯ï¼ˆå­—å…¸è®¿é—®ï¼‰
content = result.get("markdown")      # âŒ æŠ¥é”™
content = result["markdown"]          # âŒ æŠ¥é”™
```

### 3.3 å®Œæ•´ API å‚è€ƒ

#### Scrape å‚æ•°

```python
result = app.scrape(
    url="https://example.com",              # å¿…éœ€ï¼šURL
    formats=["markdown", "html", "links"],  # è¾“å‡ºæ ¼å¼
    only_main_content=True,                 # åªè¦ä¸»è¦å†…å®¹
    include_tags=["article", "main"],       # åŒ…å«çš„æ ‡ç­¾
    exclude_tags=["nav", "footer"],         # æ’é™¤çš„æ ‡ç­¾
    wait_for=5000,                          # ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    timeout=30000,                          # è¶…æ—¶æ—¶é—´
    max_age=3600000,                        # ç¼“å­˜æ—¶é—´ï¼ˆ1å°æ—¶ï¼‰
    actions=[...],                          # é¡µé¢äº¤äº’
    location={                              # åœ°ç†ä½ç½®
        "country": "US",
        "languages": ["en-US"]
    },
    mobile=False,                           # ç§»åŠ¨ç«¯æ¨¡å¼
    skip_tls_verification=False,            # è·³è¿‡ TLS éªŒè¯
    remove_base64_images=True,              # ç§»é™¤ Base64 å›¾ç‰‡
    block_ads=True                          # å±è”½å¹¿å‘Š
)
```

#### Crawl å‚æ•°

```python
crawl_result = app.crawl(
    url="https://example.com",
    limit=100,                    # æœ€å¤§é¡µé¢æ•°
    max_depth=3,                  # æœ€å¤§æ·±åº¦
    allow_subdomains=False,       # å­åŸŸå
    crawl_entire_domain=False,    # æ•´ä¸ªåŸŸå
    ignore_sitemap=False,         # å¿½ç•¥ sitemap
    include_paths=["/blog/"],     # åŒ…å«è·¯å¾„
    exclude_paths=["/admin/"],    # æ’é™¤è·¯å¾„
    scrape_options={...}          # Scrape é€‰é¡¹
)
```

### 3.4 å¼‚æ­¥æ”¯æŒ

```python
from firecrawl import AsyncFirecrawlApp
import asyncio

async def main():
    app = AsyncFirecrawlApp(api_key="fc-your-key")

    # å¼‚æ­¥çˆ¬å–
    result = await app.scrape(
        url="https://example.com",
        formats=["markdown"]
    )

    print(result.markdown)

asyncio.run(main())
```

### 3.5 é”™è¯¯å¤„ç†

```python
from firecrawl import FirecrawlApp
from firecrawl.exceptions import (
    FirecrawlError,
    RequestTimeoutError,
    RateLimitError
)

app = FirecrawlApp(api_key="fc-your-key")

try:
    result = app.scrape(url="https://example.com")
except RequestTimeoutError as e:
    print(f"è¶…æ—¶: {e}")
except RateLimitError as e:
    print(f"é€Ÿç‡é™åˆ¶: {e}")
except FirecrawlError as e:
    print(f"Firecrawl é”™è¯¯: {e}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

---

<div id="chapter-4">

## ç¬¬å››ç« ï¼šé«˜çº§ç‰¹æ€§

</div>

### 4.1 ç¼“å­˜ç­–ç•¥

#### é»˜è®¤ç¼“å­˜ï¼ˆ2 å¤©ï¼‰

```python
# é»˜è®¤ maxAge = 172800000 æ¯«ç§’ï¼ˆ2 å¤©ï¼‰
result = app.scrape(
    url="https://example.com",
    formats=["markdown"]
)
# å¦‚æœ 2 å¤©å†…æœ‰ç¼“å­˜ï¼Œç›´æ¥è¿”å›
```

#### è‡ªå®šä¹‰ç¼“å­˜æ—¶é—´

```python
# 10 åˆ†é’Ÿç¼“å­˜
result = app.scrape(
    url="https://example.com",
    max_age=600000,  # 10 åˆ†é’Ÿ
    formats=["markdown"]
)

# å§‹ç»ˆè·å–æœ€æ–°
result = app.scrape(
    url="https://example.com",
    max_age=0,  # ä¸ä½¿ç”¨ç¼“å­˜
    formats=["markdown"]
)

# ä¸å­˜å‚¨åˆ°ç¼“å­˜
result = app.scrape(
    url="https://example.com",
    store_in_cache=False,
    formats=["markdown"]
)
```

**æˆæœ¬ä¼˜åŒ–**:

- é»˜è®¤ 2 å¤©ç¼“å­˜å¯èŠ‚çœ **50%+ æˆæœ¬**
- æ–°é—»ç½‘ç«™ï¼š1 å°æ—¶ç¼“å­˜
- é™æ€æ–‡æ¡£ï¼š7 å¤©ç¼“å­˜
- å®æ—¶æ•°æ®ï¼š`max_age=0`

### 4.2 åœ°ç†ä½ç½®ä¸è¯­è¨€

```python
result = app.scrape(
    url="https://example.com",
    formats=["markdown"],
    location={
        "country": "JP",              # æ—¥æœ¬
        "languages": ["ja-JP", "en"]  # æ—¥è¯­ä¼˜å…ˆï¼Œè‹±è¯­æ¬¡ä¹‹
    }
)
```

**æ”¯æŒçš„å›½å®¶ä»£ç **ï¼ˆéƒ¨åˆ†ï¼‰:

- `US` - ç¾å›½
- `GB` - è‹±å›½
- `AU` - æ¾³å¤§åˆ©äºš
- `JP` - æ—¥æœ¬
- `CN` - ä¸­å›½
- `DE` - å¾·å›½
- `FR` - æ³•å›½

### 4.3 Stealth Modeï¼ˆéšèº«æ¨¡å¼ï¼‰

é’ˆå¯¹é«˜çº§åçˆ¬ç½‘ç«™ï¼š

```python
result = app.scrape(
    url="https://protected-site.com",
    formats=["markdown"],
    proxy="stealth"  # ä½¿ç”¨éšèº«ä»£ç†
)
```

**é€‚ç”¨åœºæ™¯**:

- å¤§å‹ç”µå•†ç½‘ç«™ï¼ˆAmazonã€eBayï¼‰
- ç¤¾äº¤åª’ä½“ï¼ˆLinkedInï¼‰
- å—ä¿æŠ¤çš„æ–°é—»ç«™ç‚¹

### 4.4 Change Trackingï¼ˆå˜æ›´ç›‘æ§ï¼‰

```python
# é¦–æ¬¡çˆ¬å–
result1 = app.scrape(
    url="https://example.com/product",
    formats=["markdown"]
)

# å®šæœŸæ£€æŸ¥å˜æ›´
import time
time.sleep(3600)  # 1 å°æ—¶å

result2 = app.scrape(
    url="https://example.com/product",
    formats=["markdown", "changeTracking"]
)

# æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´
if result2.change_tracking.changed:
    print("é¡µé¢å·²æ›´æ–°ï¼")
    print(f"å˜æ›´å†…å®¹: {result2.change_tracking.diff}")
```

### 4.5 Extractï¼ˆæ™ºèƒ½æ•°æ®æå–ï¼‰

#### å•é¡µæå–

```python
from pydantic import BaseModel

class Product(BaseModel):
    name: str
    price: float
    description: str
    rating: float

result = app.scrape(
    url="https://shop.example.com/product",
    formats=[{
        "type": "json",
        "schema": Product.model_json_schema()
    }]
)

product = result.json
print(f"{product['name']}: ${product['price']}")
```

#### æ•´ç«™æå–ï¼ˆFIRE-1 Betaï¼‰

```python
extract_result = app.extract(
    urls=["https://shop.com/category1", "https://shop.com/category2"],
    prompt="æå–æ‰€æœ‰äº§å“çš„åç§°ã€ä»·æ ¼ã€åº“å­˜çŠ¶æ€",
    schema={
        "type": "object",
        "properties": {
            "products": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "price": {"type": "number"},
                        "in_stock": {"type": "boolean"}
                    }
                }
            }
        }
    }
)
```

---

<div id="chapter-5">

## ç¬¬äº”ç« ï¼šHawaiiHub å®æˆ˜åº”ç”¨

</div>

### 5.1 å¤å¨å¤·æ–°é—»çˆ¬å–ç³»ç»Ÿ

#### ç›®æ ‡ç½‘ç«™

1. **Hawaii News Now** - `https://www.hawaiinewsnow.com/`
2. **Star Advertiser** - `https://www.staradvertiser.com/`
3. **Civil Beat** - `https://www.civilbeat.org/`

#### å®ç°ä»£ç 

```python
from firecrawl import FirecrawlApp
from datetime import datetime
import json
import os
from dotenv import load_dotenv

load_dotenv()
app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))

class HawaiiNewsScrap:
    """å¤å¨å¤·æ–°é—»çˆ¬è™«"""

    def __init__(self):
        self.sources = [
            "https://www.hawaiinewsnow.com/",
            "https://www.staradvertiser.com/",
            "https://www.civilbeat.org/"
        ]

    def scrape_homepage(self, url: str) -> dict:
        """çˆ¬å–æ–°é—»é¦–é¡µ"""
        try:
            result = app.scrape(
                url=url,
                formats=["markdown", "links"],
                only_main_content=True,
                max_age=3600000  # 1 å°æ—¶ç¼“å­˜
            )

            return {
                "url": url,
                "success": True,
                "content": result.markdown,
                "links": result.links,
                "scraped_at": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "url": url,
                "success": False,
                "error": str(e)
            }

    def extract_article_links(self, links: list) -> list:
        """æå–æ–‡ç« é“¾æ¥"""
        article_links = []
        for link in links:
            # è¿‡æ»¤æ¡ä»¶
            if any(keyword in link.lower() for keyword in ['article', 'news', 'story']):
                if not any(exclude in link.lower() for exclude in ['video', 'weather', 'sports']):
                    article_links.append(link)
        return article_links[:10]  # é™åˆ¶ 10 ç¯‡

    def scrape_articles(self, article_links: list) -> list:
        """æ‰¹é‡çˆ¬å–æ–‡ç« """
        try:
            batch_result = app.batch_scrape(
                urls=article_links,
                formats=["markdown", {"type": "json", "prompt": "æå–æ ‡é¢˜ã€ä½œè€…ã€æ—¥æœŸã€æ‘˜è¦"}],
                only_main_content=True
            )

            articles = []
            for page in batch_result:
                articles.append({
                    "url": page.url,
                    "markdown": page.markdown,
                    "metadata": page.json if hasattr(page, 'json') else {}
                })

            return articles
        except Exception as e:
            print(f"æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
            return []

    def run(self):
        """æ‰§è¡Œå®Œæ•´çˆ¬å–æµç¨‹"""
        all_articles = []

        for source in self.sources:
            print(f"\nğŸ“° çˆ¬å–: {source}")

            # 1. çˆ¬å–é¦–é¡µ
            homepage = self.scrape_homepage(source)
            if not homepage["success"]:
                print(f"âŒ å¤±è´¥: {homepage['error']}")
                continue

            # 2. æå–æ–‡ç« é“¾æ¥
            article_links = self.extract_article_links(homepage["links"])
            print(f"ğŸ“Š å‘ç° {len(article_links)} ç¯‡æ–‡ç« ")

            # 3. æ‰¹é‡çˆ¬å–æ–‡ç« 
            articles = self.scrape_articles(article_links)
            all_articles.extend(articles)

            print(f"âœ… æˆåŠŸçˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")

        # ä¿å­˜ç»“æœ
        self.save_results(all_articles)
        return all_articles

    def save_results(self, articles: list):
        """ä¿å­˜çˆ¬å–ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSON æ ¼å¼
        with open(f"data/hawaii_news_{timestamp}.json", "w", encoding="utf-8") as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)

        # Markdown æ ¼å¼
        with open(f"data/hawaii_news_{timestamp}.md", "w", encoding="utf-8") as f:
            f.write(f"# å¤å¨å¤·æ–°é—»æ±‡æ€»\n\n")
            f.write(f"> çˆ¬å–æ—¶é—´: {datetime.now()}\n\n")
            for i, article in enumerate(articles, 1):
                f.write(f"## {i}. {article.get('url', 'N/A')}\n\n")
                f.write(f"{article['markdown'][:500]}...\n\n")
                f.write("---\n\n")

# ä½¿ç”¨
scraper = HawaiiNewsScrap()
articles = scraper.run()
print(f"\nğŸ‰ å®Œæˆï¼å…±é‡‡é›† {len(articles)} ç¯‡æ–‡ç« ")
```

### 5.2 ä¸ NewsAPI é›†æˆ

```python
from firecrawl import FirecrawlApp
from newsapi import NewsApiClient
import os

# åˆå§‹åŒ–
firecrawl = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
newsapi = NewsApiClient(api_key=os.getenv("NEWSAPI_KEY"))

def fetch_hawaii_news():
    """NewsAPI å‘ç° + Firecrawl é‡‡é›†"""

    # 1. ä½¿ç”¨ NewsAPI æœç´¢
    articles = newsapi.get_everything(
        q="Hawaii Chinese community",
        language="en",
        sort_by="publishedAt",
        page_size=20
    )

    # 2. æå– URL
    urls = [article["url"] for article in articles["articles"]]

    # 3. ä½¿ç”¨ Firecrawl æ‰¹é‡çˆ¬å–å®Œæ•´å†…å®¹
    full_articles = firecrawl.batch_scrape(
        urls=urls,
        formats=["markdown"],
        only_main_content=True
    )

    # 4. åˆå¹¶æ•°æ®
    results = []
    for i, full_article in enumerate(full_articles):
        results.append({
            **articles["articles"][i],  # NewsAPI å…ƒæ•°æ®
            "full_content": full_article.markdown  # Firecrawl å®Œæ•´å†…å®¹
        })

    return results
```

### 5.3 æˆæœ¬ç›‘æ§ä¸ä¼˜åŒ–

```python
class CostMonitor:
    """æˆæœ¬ç›‘æ§å™¨"""

    def __init__(self, daily_budget: float = 10.0):
        self.daily_budget = daily_budget
        self.request_count = 0
        self.total_cost = 0.0

    def track_request(self, cost: float = 0.01):
        """è®°å½•è¯·æ±‚æˆæœ¬"""
        self.request_count += 1
        self.total_cost += cost

        if self.total_cost >= self.daily_budget:
            raise Exception(f"è¶…å‡ºæ¯æ—¥é¢„ç®—: ${self.daily_budget}")

        print(f"è¯·æ±‚ #{self.request_count} | æˆæœ¬: ${cost:.4f} | ç´¯è®¡: ${self.total_cost:.2f}/{self.daily_budget}")

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_requests": self.request_count,
            "total_cost": self.total_cost,
            "average_cost": self.total_cost / self.request_count if self.request_count > 0 else 0,
            "budget_remaining": self.daily_budget - self.total_cost
        }

# ä½¿ç”¨
monitor = CostMonitor(daily_budget=10.0)

def scrape_with_monitoring(url: str) -> dict:
    """å¸¦æˆæœ¬ç›‘æ§çš„çˆ¬å–"""
    monitor.track_request(cost=0.01)
    result = app.scrape(url=url, formats=["markdown"])
    return result

# ç»Ÿè®¡ä¿¡æ¯
stats = monitor.get_stats()
print(f"ğŸ“Š ä»Šæ—¥ç»Ÿè®¡: {stats}")
```

---

<div id="chapter-6">

## ç¬¬å…­ç« ï¼šæœ€ä½³å®è·µä¸ä¼˜åŒ–

</div>

### 6.1 æ€§èƒ½ä¼˜åŒ–

#### 1. ä½¿ç”¨ç¼“å­˜

```python
# âŒ é”™è¯¯ï¼šæ¯æ¬¡éƒ½é‡æ–°çˆ¬å–
for i in range(10):
    result = app.scrape(url="https://example.com", max_age=0)

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ç¼“å­˜
for i in range(10):
    result = app.scrape(url="https://example.com", max_age=3600000)  # 1 å°æ—¶
```

#### 2. æ‰¹é‡ä¼˜äºå•ä¸ª

```python
# âŒ é”™è¯¯ï¼šé€ä¸ªçˆ¬å–
urls = ["url1", "url2", "url3"]
for url in urls:
    result = app.scrape(url)

# âœ… æ­£ç¡®ï¼šæ‰¹é‡çˆ¬å–
results = app.batch_scrape(urls)
```

#### 3. åªè¦ä¸»è¦å†…å®¹

```python
# âœ… æ­£ç¡®ï¼šç§»é™¤å¯¼èˆªã€å¹¿å‘Šã€é¡µè„š
result = app.scrape(
    url="...",
    only_main_content=True,
    exclude_tags=["nav", "footer", "aside", "script"],
    block_ads=True
)
```

### 6.2 é”™è¯¯å¤„ç†ä¸é‡è¯•

```python
import time

def safe_scrape(url: str, max_retries: int = 3) -> dict | None:
    """å®‰å…¨çˆ¬å–ï¼Œå¸¦é‡è¯•"""
    for attempt in range(max_retries):
        try:
            result = app.scrape(
                url=url,
                formats=["markdown"],
                only_main_content=True
            )

            if not result or not hasattr(result, "markdown"):
                raise ValueError("æ— æ•ˆç»“æœ")

            print(f"âœ… æˆåŠŸçˆ¬å–: {url}")
            return result

        except RequestTimeoutError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                print(f"â³ è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... ({attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                print(f"âŒ å¤±è´¥ï¼ˆ{max_retries}æ¬¡é‡è¯•åï¼‰: {url}")
                return None

        except Exception as e:
            print(f"âŒ é”™è¯¯: {url} - {e}")
            return None
```

### 6.3 æ•°æ®éªŒè¯

```python
from pydantic import BaseModel, HttpUrl, Field
from typing import Optional

class Article(BaseModel):
    """æ–‡ç« æ•°æ®æ¨¡å‹"""
    title: str = Field(..., min_length=1, max_length=200)
    url: HttpUrl
    author: str
    date: str
    content: Optional[str] = None

# éªŒè¯
try:
    article = Article(
        title="æµ‹è¯•æ–‡ç« ",
        url="https://example.com",
        author="å¼ ä¸‰",
        date="2025-10-27",
        content="..."
    )
except ValidationError as e:
    print(f"éªŒè¯å¤±è´¥: {e}")
```

### 6.4 æ—¥å¿—è®°å½•

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/firecrawl.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger("HawaiiNews")

# ä½¿ç”¨æ—¥å¿—
def scrape_with_logging(url: str):
    logger.info(f"å¼€å§‹çˆ¬å–: {url}")
    try:
        result = app.scrape(url=url)
        logger.info(f"æˆåŠŸ: {len(result.markdown)} å­—ç¬¦")
        return result
    except Exception as e:
        logger.error(f"å¤±è´¥: {url} - {e}")
        return None
```

### 6.5 æ•°æ®æ¸…æ´—

```python
import re

def clean_markdown(content: str) -> str:
    """æ¸…æ´— Markdown å†…å®¹"""
    # ç§»é™¤å¤šä½™ç©ºè¡Œ
    content = re.sub(r"\n{3,}", "\n\n", content)

    # ç§»é™¤ç‰¹å®šæ¨¡å¼
    patterns = [
        r"Subscribe to.*newsletter",
        r"Advertisement",
        r"Related Articles",
        r"Share on.*"
    ]

    for pattern in patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    return content.strip()
```

---

<div id="appendix">

## é™„å½•

</div>

### A. æœ¯è¯­è¡¨

| æœ¯è¯­         | ä¸­æ–‡      | è¯´æ˜                     |
| ------------ | --------- | ------------------------ |
| Scrape       | çˆ¬å–/æŠ“å– | å•é¡µå†…å®¹é‡‡é›†             |
| Crawl        | æ•´ç«™çˆ¬å–  | é€’å½’çˆ¬å–æ‰€æœ‰å­é¡µé¢       |
| Map          | ç«™ç‚¹åœ°å›¾  | å¿«é€Ÿå‘ç°æ‰€æœ‰ URL         |
| Extract      | æå–      | ç»“æ„åŒ–æ•°æ®æå–           |
| LLM-ready    | é€‚é… LLM  | é€‚åˆå¤§è¯­è¨€æ¨¡å‹ä½¿ç”¨çš„æ ¼å¼ |
| Schema       | æ¶æ„      | æ•°æ®ç»“æ„å®šä¹‰             |
| Actions      | æ“ä½œ/åŠ¨ä½œ | é¡µé¢äº¤äº’ï¼ˆç‚¹å‡»ã€è¾“å…¥ç­‰ï¼‰ |
| Stealth Mode | éšèº«æ¨¡å¼  | ç»•è¿‡é«˜çº§åçˆ¬æœºåˆ¶         |

### B. é”™è¯¯ç å¯¹ç…§

| é”™è¯¯ç  | è¯´æ˜         | è§£å†³æ–¹æ¡ˆ               |
| ------ | ------------ | ---------------------- |
| 401    | API Key æ— æ•ˆ | æ£€æŸ¥ç¯å¢ƒå˜é‡           |
| 429    | é€Ÿç‡é™åˆ¶     | é™ä½è¯·æ±‚é¢‘ç‡æˆ–å‡çº§è®¡åˆ’ |
| 500    | æœåŠ¡å™¨é”™è¯¯   | ç¨åé‡è¯•               |
| 504    | è¶…æ—¶         | å¢åŠ  timeout å‚æ•°      |

### C. é€ŸæŸ¥è¡¨

#### å¸¸ç”¨å‚æ•°

```python
# Scrape
result = app.scrape(
    url="...",
    formats=["markdown"],        # è¾“å‡ºæ ¼å¼
    only_main_content=True,      # ä¸»è¦å†…å®¹
    max_age=3600000,             # ç¼“å­˜ 1 å°æ—¶
    exclude_tags=["nav"]         # æ’é™¤æ ‡ç­¾
)

# Crawl
crawl_result = app.crawl(
    url="...",
    limit=100,                   # æœ€å¤š 100 é¡µ
    max_depth=3,                 # æ·±åº¦ 3 å±‚
    scrape_options={...}         # Scrape é€‰é¡¹
)

# Map
map_result = app.map(
    url="...",
    limit=5000                   # æœ€å¤š 5000 URL
)

# Search
search_result = app.search(
    query="...",
    sources=[{"type": "web"}],
    limit=10
)
```

### D. èµ„æºé“¾æ¥

#### å®˜æ–¹èµ„æº

- ğŸ“– åœ¨çº¿æ–‡æ¡£: https://docs.firecrawl.dev/
- ğŸ™ GitHub: https://github.com/firecrawl/firecrawl
- ğŸ’¬ Discord: https://discord.gg/firecrawl
- ğŸ“ Changelog: https://www.firecrawl.dev/changelog
- ğŸ® Playground: https://firecrawl.dev/playground

#### æœ¬åœ°èµ„æº

- ğŸ“‚ å®˜æ–¹ä¸­æ–‡æ–‡æ¡£: `firecrawl-docs/zh/`
- ğŸ“‹ é¡¹ç›®è§„åˆ™: `.cursorrules`
- ğŸ“Š æ›´æ–°æ—¥å¿—: `Firecrawlæ›´æ–°æ—¥å¿—æ±‡æ€».md`
- âš™ï¸ SDK é…ç½®: `SDK_CONFIGURATION_COMPLETE.md`

---

**æœ€åæ›´æ–°**: 2025-10-27
**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**ä½œè€…**: HawaiiHub AI Team
**æ€»å­—æ•°**: çº¦ 15,000 å­—

ğŸ‰ **æ­å–œå®Œæˆå­¦ä¹ ï¼ç°åœ¨ä½ å·²ç»æŒæ¡ Firecrawl çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼**
