# Firecrawl Python SDK å®Œæ•´å­¦ä¹ æŒ‡å—

**ç‰ˆæœ¬**: SDK v4.5.0
**å®˜æ–¹æ–‡æ¡£**: https://docs.firecrawl.dev/sdks/python
**æ›´æ–°æ—¶é—´**: 2025-10-28
**ç»´æŠ¤è€…**: HawaiiHub AI Team

---

## ğŸ“š ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½)
3. [é«˜çº§ç‰¹æ€§](#é«˜çº§ç‰¹æ€§)
4. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
5. [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
6. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
7. [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# åŸºç¡€å®‰è£…
pip install firecrawl-py

# åŒ…å«å¼€å‘å·¥å…·
pip install firecrawl-py python-dotenv pydantic
```

### åŸºæœ¬ç”¨æ³•

```python
from firecrawl import FirecrawlApp

# åˆå§‹åŒ–ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å– API å¯†é’¥ï¼‰
app = FirecrawlApp()

# æˆ–è€…æ˜¾å¼ä¼ å…¥ API å¯†é’¥
app = FirecrawlApp(api_key="fc-YOUR-API-KEY")

# Scrape å•ä¸ª URL
result = app.scrape(
    url="https://firecrawl.dev",
    formats=["markdown", "html"]
)

print(result.markdown)  # è®¿é—® Markdown å†…å®¹
print(result.metadata.title)  # è®¿é—®å…ƒæ•°æ®
```

### ç¯å¢ƒé…ç½®

**æ¨èåšæ³•**ï¼šä½¿ç”¨ `.env` æ–‡ä»¶ç®¡ç† API å¯†é’¥

```bash
# .env æ–‡ä»¶
FIRECRAWL_API_KEY=fc-YOUR-API-KEY
```

```python
import os
from dotenv import load_dotenv
from firecrawl import FirecrawlApp

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è‡ªåŠ¨è¯»å– FIRECRAWL_API_KEY
app = FirecrawlApp()
```

---

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### 1. Scrape - å•é¡µé‡‡é›†

**åŸºç¡€ç”¨æ³•**ï¼š

```python
# æœ€ç®€å•çš„ç”¨æ³•
result = app.scrape("https://example.com")
print(result.markdown)

# æŒ‡å®šè¿”å›æ ¼å¼
result = app.scrape(
    url="https://example.com",
    formats=["markdown", "html", "links", "screenshot"]
)

# è®¿é—®ä¸åŒæ ¼å¼çš„å†…å®¹
print(result.markdown)    # Markdown å†…å®¹
print(result.html)        # HTML å†…å®¹
print(result.links)       # é¡µé¢é“¾æ¥åˆ—è¡¨
print(result.screenshot)  # Base64 æˆªå›¾
```

**é«˜çº§é€‰é¡¹**ï¼š

```python
result = app.scrape(
    url="https://example.com",
    formats=["markdown"],

    # åªæå–ä¸»è¦å†…å®¹ï¼ˆå»é™¤å¯¼èˆªã€å¹¿å‘Šç­‰ï¼‰
    only_main_content=True,

    # åŒ…å«çš„ HTML æ ‡ç­¾
    include_tags=["article", "main", "p"],

    # æ’é™¤çš„ HTML æ ‡ç­¾
    exclude_tags=["nav", "footer", "aside"],

    # ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    wait_for=2000,

    # ç§»é™¤ Base64 å›¾ç‰‡ï¼ˆå‡å°‘å“åº”å¤§å°ï¼‰
    remove_base64_images=True,

    # ç¼“å­˜æ§åˆ¶ï¼ˆ2å¤©ï¼‰
    max_age=172800000
)
```

### 2. Crawl - æ·±åº¦çˆ¬å–

**é˜»å¡å¼çˆ¬å–**ï¼ˆç­‰å¾…å®Œæˆï¼‰ï¼š

```python
# ç®€å•çˆ¬å–ï¼ˆè‡ªåŠ¨åˆ†é¡µï¼Œç­‰å¾…å®Œæˆï¼‰
job = app.crawl(
    url="https://docs.firecrawl.dev",
    limit=5,              # é™åˆ¶é¡µé¢æ•°
    poll_interval=1,      # è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
    timeout=120           # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
)

# è®¿é—®ç»“æœ
print(f"çŠ¶æ€: {job.status}")
print(f"å®Œæˆ: {job.completed}/{job.total}")
for doc in job.data:
    print(f"- {doc.metadata.source_url}")
```

**éé˜»å¡å¼çˆ¬å–**ï¼ˆå¯åŠ¨åæ£€æŸ¥ï¼‰ï¼š

```python
# å¯åŠ¨çˆ¬å–ä»»åŠ¡
job = app.start_crawl(
    url="https://docs.firecrawl.dev",
    limit=10,
    scrape_options={
        "formats": ["markdown"],
        "only_main_content": True
    }
)

print(f"ä»»åŠ¡ ID: {job.id}")

# ç¨åæ£€æŸ¥çŠ¶æ€
status = app.get_crawl_status(job.id)
print(f"è¿›åº¦: {status.completed}/{status.total}")

# å–æ¶ˆä»»åŠ¡
ok = app.cancel_crawl(job.id)
print(f"å·²å–æ¶ˆ: {ok}")
```

**é«˜çº§çˆ¬å–é€‰é¡¹**ï¼š

```python
job = app.crawl(
    url="https://example.com",
    limit=100,

    # å…è®¸çš„åŸŸå
    allow_subdomains=True,
    allow_external_links=False,

    # è·¯å¾„è¿‡æ»¤
    include_paths=["/blog/*", "/docs/*"],
    exclude_paths=["/admin/*", "/api/*"],

    # å»é‡
    deduplicate_similar_urls=True,

    # å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼Œé¿å…é€Ÿç‡é™åˆ¶ï¼‰
    delay=1000,

    # æœ€å¤§å‘ç°æ·±åº¦
    max_discovery_depth=3,

    # æœ€å¤§å¹¶å‘æ•°
    max_concurrency=5,

    # ç«™ç‚¹åœ°å›¾
    sitemap="include",  # "skip", "include", "only"

    # Scrape é€‰é¡¹
    scrape_options={
        "formats": ["markdown"],
        "only_main_content": True,
        "remove_base64_images": True
    }
)
```

### 3. Map - ç«™ç‚¹åœ°å›¾

**å‘ç°æ‰€æœ‰ URL**ï¼š

```python
# ç”Ÿæˆç«™ç‚¹åœ°å›¾
result = app.map(
    url="https://firecrawl.dev",
    limit=100,

    # åŒ…å«å­åŸŸå
    include_subdomains=False,

    # å¿½ç•¥æŸ¥è¯¢å‚æ•°
    ignore_query_parameters=True,

    # ç«™ç‚¹åœ°å›¾ç­–ç•¥
    sitemap="include",  # "skip", "include", "only"

    # æœç´¢è¿‡æ»¤
    search="docs"  # åªåŒ…å«åŒ¹é…çš„ URL
)

# è®¿é—® URL åˆ—è¡¨
print(f"å‘ç° {len(result.links)} ä¸ª URL")
for link in result.links:
    print(f"- {link}")
```

### 4. Batch Scrape - æ‰¹é‡é‡‡é›†

**é˜»å¡å¼æ‰¹é‡**ï¼ˆç­‰å¾…å®Œæˆï¼‰ï¼š

```python
urls = [
    "https://firecrawl.dev",
    "https://docs.firecrawl.dev",
    "https://blog.firecrawl.dev"
]

job = app.batch_scrape(
    urls=urls,
    formats=["markdown"],
    poll_interval=1,
    timeout=60
)

# è®¿é—®ç»“æœ
for doc in job.data:
    print(f"URL: {doc.metadata.source_url}")
    print(f"å†…å®¹: {doc.markdown[:100]}...")
```

**éé˜»å¡å¼æ‰¹é‡**ï¼š

```python
# å¯åŠ¨æ‰¹é‡ä»»åŠ¡
job = app.start_batch_scrape(urls)

# æ£€æŸ¥çŠ¶æ€
status = app.get_batch_scrape_status(job.id)
print(f"è¿›åº¦: {status.completed}/{status.total}")
```

### 5. Search - æ™ºèƒ½æœç´¢

```python
# æœç´¢ + çˆ¬å–ä¸€ä½“åŒ–
results = app.search(
    query="å¤å¨å¤· åäºº é¤å…",
    limit=10,

    # æœç´¢æ¥æº
    sources=[{"type": "web"}],  # "web", "images", "news"

    # åœ°ç†ä½ç½®
    location="us",

    # è¿‡æ»¤æ¡ä»¶
    filter="site:yelp.com OR site:tripadvisor.com",

    # Scrape é€‰é¡¹ï¼ˆå¯é€‰ï¼‰
    scrape_options={
        "formats": ["markdown"],
        "only_main_content": True
    }
)

# è®¿é—®æœç´¢ç»“æœ
for item in results.get("web", []):
    print(f"æ ‡é¢˜: {item.get('title')}")
    print(f"URL: {item.get('url')}")
    if "markdown" in item:
        print(f"å†…å®¹: {item['markdown'][:100]}...")
```

---

## ğŸš€ é«˜çº§ç‰¹æ€§

### 1. WebSocket å®æ—¶ç›‘æ§

```python
import asyncio
from firecrawl import AsyncFirecrawl

async def main():
    app = AsyncFirecrawl()

    # å¯åŠ¨çˆ¬å–
    job = await app.start_crawl("https://firecrawl.dev", limit=5)

    # å®æ—¶ç›‘æ§è¿›åº¦
    async for snapshot in app.watcher(
        job.id,
        kind="crawl",
        poll_interval=2,
        timeout=120
    ):
        if snapshot.status == "completed":
            print("âœ… å®Œæˆ")
            for doc in snapshot.data:
                print(f"- {doc.metadata.source_url}")
            break
        elif snapshot.status == "failed":
            print("âŒ å¤±è´¥")
            break
        else:
            print(f"â³ è¿›åº¦: {snapshot.completed}/{snapshot.total}")

asyncio.run(main())
```

### 2. åˆ†é¡µæ§åˆ¶

**æ‰‹åŠ¨åˆ†é¡µ**ï¼ˆå•é¡µï¼‰ï¼š

```python
from firecrawl import PaginationConfig

# å¯åŠ¨ä»»åŠ¡
job = app.start_crawl("https://example.com", limit=100)

# è·å–ç¬¬ä¸€é¡µï¼ˆä¸è‡ªåŠ¨åˆ†é¡µï¼‰
status = app.get_crawl_status(
    job.id,
    pagination_config=PaginationConfig(auto_paginate=False)
)

print(f"æœ¬é¡µæ–‡æ¡£æ•°: {len(status.data)}")
print(f"ä¸‹ä¸€é¡µ URL: {status.next}")

# å¦‚æœéœ€è¦ï¼Œæ‰‹åŠ¨è·å–ä¸‹ä¸€é¡µ
if status.next:
    next_status = app.get_crawl_status(
        job.id,
        pagination_config=PaginationConfig(auto_paginate=False, next=status.next)
    )
```

**é™åˆ¶è‡ªåŠ¨åˆ†é¡µ**ï¼š

```python
# è‡ªåŠ¨åˆ†é¡µï¼Œä½†æå‰åœæ­¢
status = app.get_crawl_status(
    job.id,
    pagination_config=PaginationConfig(
        max_pages=2,        # æœ€å¤š 2 é¡µ
        max_results=50,     # æœ€å¤š 50 ä¸ªç»“æœ
        max_wait_time=15    # æœ€å¤šç­‰å¾… 15 ç§’
    )
)
```

### 3. å¼‚æ­¥æ“ä½œ

```python
import asyncio
from firecrawl import AsyncFirecrawl

async def main():
    app = AsyncFirecrawl()

    # å¼‚æ­¥ Scrape
    doc = await app.scrape("https://firecrawl.dev", formats=["markdown"])
    print(doc.markdown)

    # å¼‚æ­¥ Search
    results = await app.search("firecrawl", limit=2)
    print(results.get("web", []))

    # å¼‚æ­¥ Crawl
    job = await app.start_crawl("https://docs.firecrawl.dev", limit=3)
    status = await app.get_crawl_status(job.id)
    print(status.status)

    # å¼‚æ­¥ Batch Scrape
    batch_job = await app.batch_scrape(
        ["https://firecrawl.dev", "https://docs.firecrawl.dev"],
        formats=["markdown"],
        poll_interval=1,
        timeout=60
    )
    print(f"å®Œæˆ: {batch_job.completed}/{batch_job.total}")

asyncio.run(main())
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. API å¯†é’¥ç®¡ç†

```python
import os
from dotenv import load_dotenv
from firecrawl import FirecrawlApp

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡
load_dotenv()
app = FirecrawlApp()  # è‡ªåŠ¨è¯»å– FIRECRAWL_API_KEY

# âŒ é”™è¯¯ï¼šç¡¬ç¼–ç 
app = FirecrawlApp(api_key="fc-hardcoded-key")  # æ°¸è¿œä¸è¦è¿™æ ·åšï¼
```

### 2. é”™è¯¯å¤„ç†

```python
from firecrawl import FirecrawlApp
from firecrawl.exceptions import (
    RequestTimeoutError,
    RateLimitError,
    AuthenticationError,
)
import time
import logging

def safe_scrape(url: str, max_retries: int = 3) -> dict | None:
    """å®‰å…¨çˆ¬å–ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    app = FirecrawlApp()

    for attempt in range(max_retries):
        try:
            result = app.scrape(
                url=url,
                formats=["markdown"],
                only_main_content=True
            )

            # éªŒè¯ç»“æœ
            if not result or not hasattr(result, "markdown"):
                raise ValueError("è¿”å›ç»“æœæ— æ•ˆ")

            logging.info(f"âœ… æˆåŠŸçˆ¬å–: {url}")
            return result

        except AuthenticationError as e:
            logging.error(f"âŒ è®¤è¯å¤±è´¥: {e}")
            return None  # å¯†é’¥é—®é¢˜ï¼Œä¸é‡è¯•

        except RateLimitError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                logging.warning(f"â³ é€Ÿç‡é™åˆ¶ï¼Œ{wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                logging.error(f"âŒ è¾¾åˆ°é€Ÿç‡é™åˆ¶: {url}")
                return None

        except RequestTimeoutError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                logging.warning(f"â³ è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... ({attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                logging.error(f"âŒ è¶…æ—¶å¤±è´¥: {url}")
                return None

        except Exception as e:
            logging.error(f"âŒ æœªçŸ¥é”™è¯¯: {url} - {e}")
            return None

    return None
```

### 3. ç¼“å­˜ä¼˜åŒ–

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=100)
def cached_scrape(url: str, max_age: int = 172800000) -> str:
    """å¸¦ç¼“å­˜çš„çˆ¬å–ï¼ˆä½¿ç”¨ Firecrawl å†…ç½®ç¼“å­˜ + Python LRUï¼‰"""
    app = FirecrawlApp()

    result = app.scrape(
        url=url,
        formats=["markdown"],
        only_main_content=True,
        max_age=max_age  # 2å¤©ç¼“å­˜
    )

    return result.markdown

# ä½¿ç”¨
content1 = cached_scrape("https://firecrawl.dev")
content2 = cached_scrape("https://firecrawl.dev")  # å‘½ä¸­ Python LRU ç¼“å­˜
```

### 4. æ‰¹é‡å¤„ç†æ¨¡å¼

```python
def scrape_batch_smart(urls: list[str], batch_size: int = 10) -> list[dict]:
    """æ™ºèƒ½æ‰¹é‡çˆ¬å–ï¼ˆåˆ†æ‰¹å¤„ç† + é”™è¯¯å¤„ç†ï¼‰"""
    app = FirecrawlApp()
    all_results = []

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(urls), batch_size):
        batch = urls[i:i+batch_size]

        try:
            job = app.batch_scrape(
                urls=batch,
                formats=["markdown"],
                poll_interval=2,
                timeout=120
            )

            all_results.extend(job.data)
            print(f"âœ… æ‰¹æ¬¡ {i//batch_size + 1}: {len(job.data)} ä¸ªé¡µé¢")

        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} å¤±è´¥: {e}")
            # é€ä¸ªé‡è¯•
            for url in batch:
                try:
                    result = app.scrape(url, formats=["markdown"])
                    all_results.append(result)
                except Exception as e2:
                    print(f"  âŒ {url}: {e2}")

        time.sleep(1)  # é¿å…é€Ÿç‡é™åˆ¶

    return all_results
```

---

## ğŸ’° æˆæœ¬æ§åˆ¶

### è¯·æ±‚è®¡æ•°å™¨

```python
class FirecrawlClient:
    """å¸¦æˆæœ¬æ§åˆ¶çš„ Firecrawl å®¢æˆ·ç«¯"""

    def __init__(self, daily_budget: float = 10.0):
        self.app = FirecrawlApp()
        self.daily_budget = daily_budget
        self.request_count = 0
        self.total_cost = 0.0
        self.cost_per_request = 0.01  # å‡è®¾ $0.01/è¯·æ±‚

    def scrape(self, url: str, **kwargs) -> dict:
        """çˆ¬å–å¹¶è®°å½•æˆæœ¬"""
        # æ£€æŸ¥é¢„ç®—
        if self.total_cost >= self.daily_budget:
            raise Exception(f"âŒ è¶…å‡ºæ¯æ—¥é¢„ç®—: ${self.daily_budget}")

        # æ‰§è¡Œçˆ¬å–
        result = self.app.scrape(url=url, **kwargs)

        # è®°å½•æˆæœ¬
        self.request_count += 1
        self.total_cost += self.cost_per_request

        logging.info(
            f"ğŸ“Š è¯·æ±‚ #{self.request_count} | "
            f"æˆæœ¬: ${self.cost_per_request:.4f} | "
            f"ç´¯è®¡: ${self.total_cost:.2f}/{self.daily_budget}"
        )

        return result

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "request_count": self.request_count,
            "total_cost": self.total_cost,
            "budget": self.daily_budget,
            "remaining": self.daily_budget - self.total_cost
        }

# ä½¿ç”¨
client = FirecrawlClient(daily_budget=10.0)
result = client.scrape("https://example.com", formats=["markdown"])
print(client.get_stats())
```

---

## ğŸ“Š å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šçˆ¬å– Firecrawl åšå®¢

```python
from firecrawl import FirecrawlApp
import json
from datetime import datetime

def scrape_firecrawl_blog():
    """çˆ¬å– Firecrawl åšå®¢æ‰€æœ‰æ–‡ç« """
    app = FirecrawlApp()

    # 1. å‘ç°æ‰€æœ‰ URL
    print("ğŸ” å‘ç°åšå®¢ URL...")
    map_result = app.map(
        url="https://firecrawl.dev/blog",
        limit=100,
        search="blog"
    )

    blog_urls = [
        link for link in map_result.links
        if "/blog/" in link and link != "https://firecrawl.dev/blog"
    ]

    print(f"âœ… å‘ç° {len(blog_urls)} ç¯‡æ–‡ç« ")

    # 2. æ‰¹é‡çˆ¬å–
    print("ğŸ“¥ æ‰¹é‡çˆ¬å–æ–‡ç« ...")
    job = app.batch_scrape(
        urls=blog_urls,
        formats=["markdown"],
        poll_interval=2,
        timeout=180
    )

    # 3. ä¿å­˜ç»“æœ
    articles = []
    for doc in job.data:
        articles.append({
            "url": doc.metadata.source_url,
            "title": doc.metadata.title,
            "description": doc.metadata.description,
            "content": doc.markdown,
            "scraped_at": datetime.now().isoformat()
        })

    # ä¿å­˜ JSON
    with open("firecrawl_blog.json", "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

    # ä¿å­˜ Markdown
    with open("firecrawl_blog.md", "w", encoding="utf-8") as f:
        f.write(f"# Firecrawl åšå®¢æ–‡ç« æ±‡æ€»\n\n")
        f.write(f"> çˆ¬å–æ—¶é—´: {datetime.now()}\n\n")
        f.write(f"> æ–‡ç« æ•°é‡: {len(articles)}\n\n")

        for article in articles:
            f.write(f"## {article['title']}\n\n")
            f.write(f"**URL**: {article['url']}\n\n")
            f.write(f"{article['content']}\n\n")
            f.write("---\n\n")

    print(f"âœ… å®Œæˆï¼ä¿å­˜äº† {len(articles)} ç¯‡æ–‡ç« ")

if __name__ == "__main__":
    scrape_firecrawl_blog()
```

### æ¡ˆä¾‹ 2ï¼šå¤å¨å¤·æ–°é—»ç›‘æ§

```python
from firecrawl import FirecrawlApp
import json
from datetime import datetime
from typing import List, Dict

HAWAII_NEWS_SOURCES = [
    "https://www.hawaiinewsnow.com/",
    "https://www.staradvertiser.com/",
    "https://www.civilbeat.org/",
]

def scrape_hawaii_news() -> List[Dict]:
    """çˆ¬å–å¤å¨å¤·æ–°é—»"""
    app = FirecrawlApp()
    all_articles = []

    for source in HAWAII_NEWS_SOURCES:
        print(f"ğŸ“° çˆ¬å– {source}...")

        try:
            # 1. çˆ¬å–é¦–é¡µ
            result = app.scrape(
                url=source,
                formats=["markdown", "links"],
                only_main_content=True
            )

            # 2. æå–æ–‡ç« é“¾æ¥
            article_links = [
                link for link in result.links
                if "article" in link or "news" in link
            ][:10]  # é™åˆ¶ 10 ç¯‡

            # 3. æ‰¹é‡çˆ¬å–æ–‡ç« 
            if article_links:
                job = app.batch_scrape(
                    urls=article_links,
                    formats=["markdown"],
                    poll_interval=1,
                    timeout=60
                )

                for doc in job.data:
                    all_articles.append({
                        "source": source,
                        "url": doc.metadata.source_url,
                        "title": doc.metadata.title,
                        "content": doc.markdown,
                        "scraped_at": datetime.now().isoformat()
                    })

            print(f"âœ… {source}: {len(article_links)} ç¯‡æ–‡ç« ")

        except Exception as e:
            print(f"âŒ {source} å¤±è´¥: {e}")

    # ä¿å­˜ç»“æœ
    output_file = f"hawaii_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å®Œæˆï¼å…±é‡‡é›† {len(all_articles)} ç¯‡æ–°é—»")
    print(f"ğŸ“„ ä¿å­˜åˆ°: {output_file}")

    return all_articles

if __name__ == "__main__":
    scrape_hawaii_news()
```

### æ¡ˆä¾‹ 3ï¼šç«å“ç›‘æ§

```python
from firecrawl import FirecrawlApp
import json
from datetime import datetime
from typing import Dict, List

def monitor_competitor_changes(url: str, previous_content: str = None) -> Dict:
    """ç›‘æ§ç«å“ç½‘ç«™å˜æ›´"""
    app = FirecrawlApp()

    # çˆ¬å–å½“å‰å†…å®¹
    result = app.scrape(
        url=url,
        formats=["markdown", "html"],
        only_main_content=True
    )

    current_content = result.markdown

    # æ£€æµ‹å˜åŒ–
    has_changed = previous_content and current_content != previous_content

    return {
        "url": url,
        "title": result.metadata.title,
        "has_changed": has_changed,
        "content": current_content,
        "content_length": len(current_content),
        "checked_at": datetime.now().isoformat()
    }

def monitor_multiple_competitors(urls: List[str]) -> None:
    """æ‰¹é‡ç›‘æ§å¤šä¸ªç«å“"""
    results = []

    for url in urls:
        try:
            result = monitor_competitor_changes(url)
            results.append(result)

            if result["has_changed"]:
                print(f"âš ï¸  å˜æ›´æ£€æµ‹: {url}")
            else:
                print(f"âœ… æ— å˜æ›´: {url}")

        except Exception as e:
            print(f"âŒ å¤±è´¥: {url} - {e}")

    # ä¿å­˜æŠ¥å‘Š
    report_file = f"competitor_monitor_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

# ä½¿ç”¨
competitors = [
    "https://competitor1.com/pricing",
    "https://competitor2.com/features",
    "https://competitor3.com/products",
]

monitor_multiple_competitors(competitors)
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- **Python SDK**: https://docs.firecrawl.dev/sdks/python
- **API å‚è€ƒ**: https://docs.firecrawl.dev/api-reference/v2-introduction
- **æ›´æ–°æ—¥å¿—**: https://firecrawl.dev/changelog

### é¡¹ç›®æ–‡æ¡£
- **å¿«é€Ÿå‚è€ƒ**: `QUICK_REFERENCE.md`
- **é…ç½®æ€»ç»“**: `SDK_CONFIGURATION_COMPLETE.md`
- **æœ€ä½³å®è·µ**: `FIRECRAWL_CLOUD_API_RULES.md`

### ç¤¾åŒºèµ„æº
- **Discord**: https://discord.gg/gSmWdAkdwd
- **GitHub**: https://github.com/mendableai/firecrawl
- **åšå®¢**: https://firecrawl.dev/blog

---

## ğŸ“ ä¸‹ä¸€æ­¥

1. **å®è·µç»ƒä¹ **: è¿è¡Œä¸Šé¢çš„å®æˆ˜æ¡ˆä¾‹
2. **æ¢ç´¢é«˜çº§åŠŸèƒ½**: WebSocketã€å¼‚æ­¥æ“ä½œã€åˆ†é¡µæ§åˆ¶
3. **æ€§èƒ½ä¼˜åŒ–**: ç¼“å­˜ã€æ‰¹é‡å¤„ç†ã€æˆæœ¬æ§åˆ¶
4. **é›†æˆåº”ç”¨**: å°† Firecrawl é›†æˆåˆ°ä½ çš„é¡¹ç›®ä¸­

---

**ç»´æŠ¤è€…**: HawaiiHub AI Team
**ç‰ˆæœ¬**: v1.0.0
**æœ€åæ›´æ–°**: 2025-10-28
