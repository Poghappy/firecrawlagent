#!/usr/bin/env python3
"""åˆ†æ Firecrawl åšå®¢å†…å®¹

æå–æ‰€æœ‰æ–‡ç« çš„æ ‡é¢˜ã€ä½œè€…ã€æ—¥æœŸã€URL ç­‰ç»“æ„åŒ–æ•°æ®
"""

import json
import re
from collections import defaultdict
from datetime import datetime


# ä» MCP å·¥å…·è¿”å›çš„ markdown å†…å®¹
MARKDOWN_CONTENT = """We just raised our Series A and shipped Firecrawl /v2 ğŸ‰. [Read the blog.](https://www.firecrawl.dev/blog/firecrawl-v2-series-a-announcement)

Blog

All Posts

Updates

Customers

Example Apps

Web Extraction

AI Engineering

Low Code

[![placeholder](https://www.firecrawl.dev/_next/image?url=%2Fimages%2Fblog%2F15-python-projects-2025%2Ftop_15_python_projects_to_build_in_2025.webp&w=3840&q=75&dpl=dpl_EHyrqtTiFDFWEn279mFtbDKkaoDJ)\\\\
\\\\
Top 15 Python Projects to Build in 2025: From Beginner to Production\\\\
\\\\
Explore a range of Python projects tailored for beginners to advanced developers, empowering you to progressively improve your Python skills, AI automation skills, and build and deploy real-world applications.\\\\
\\\\
![placeholder](https://www.firecrawl.dev/_next/image?url=%2Fblog%2Fauthors%2Fabid.jpg&w=48&q=75&dpl=dpl_EHyrqtTiFDFWEn279mFtbDKkaoDJ)\\\\
\\\\
Abid Ali Awan\\\\
\\\\
Oct 16, 2025](https://www.firecrawl.dev/blog/15-python-projects-2025)"""


def extract_articles(markdown):
    """ä» Markdown å†…å®¹ä¸­æå–æ–‡ç« ä¿¡æ¯"""
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ–‡ç« æ¨¡å¼
    # æ ¼å¼: [æ ‡é¢˜\\\\æè¿°\\\\ä½œè€…\\\\æ—¥æœŸ](URL)
    pattern = r"\[(.*?)\\\\\n\\\\\n(.*?)\\\\\n\\\\\n.*?\\\\\n\\\\\n(.*?)\\\\\n\\\\\n(.*?)\]\((https://www\.firecrawl\.dev/blog/.*?)\)"

    articles = []
    matches = re.findall(pattern, markdown, re.DOTALL)

    for match in matches:
        title = match[0].strip()
        description = match[1].strip()
        author = match[2].strip()
        date = match[3].strip()
        url = match[4].strip()

        # æå– slug
        slug = url.split("/")[-1]

        articles.append(
            {
                "title": title,
                "description": description,
                "author": author,
                "date": date,
                "url": url,
                "slug": slug,
            }
        )

    return articles


def analyze_articles(articles):
    """åˆ†ææ–‡ç« æ•°æ®"""
    print("=" * 80)
    print("ğŸ“Š Firecrawl åšå®¢åˆ†ææŠ¥å‘Š")
    print("=" * 80)

    print(f"\nğŸ“ æ€»æ–‡ç« æ•°: {len(articles)}")

    # æŒ‰ä½œè€…ç»Ÿè®¡
    authors = defaultdict(int)
    for article in articles:
        authors[article["author"]] += 1

    print("\nğŸ‘¥ ä½œè€…ç»Ÿè®¡ (å‰ 10 å):")
    for i, (author, count) in enumerate(
        sorted(authors.items(), key=lambda x: x[1], reverse=True)[:10], 1
    ):
        print(f"{i:2d}. {author:30s} - {count:3d} ç¯‡")

    # æŒ‰æœˆä»½ç»Ÿè®¡
    months = defaultdict(int)
    for article in articles:
        date_str = article["date"]
        try:
            # è§£ææ—¥æœŸï¼ˆæ ¼å¼: "Oct 16, 2025"ï¼‰
            parts = date_str.split()
            if len(parts) >= 2:
                month_year = f"{parts[0]} {parts[2]}" if len(parts) >= 3 else parts[0]
                months[month_year] += 1
        except:
            pass

    print("\nğŸ“… å‘å¸ƒæ—¶é—´ç»Ÿè®¡ (å‰ 12 ä¸ªæœˆ):")
    for i, (month, count) in enumerate(sorted(months.items(), reverse=True)[:12], 1):
        print(f"{i:2d}. {month:15s} - {count:3d} ç¯‡")

    # çƒ­é—¨ä¸»é¢˜ï¼ˆä»æ ‡é¢˜ä¸­æå–å…³é”®è¯ï¼‰
    keywords = defaultdict(int)
    stop_words = {
        "the",
        "a",
        "an",
        "and",
        "or",
        "but",
        "in",
        "on",
        "at",
        "to",
        "for",
        "of",
        "with",
        "how",
        "using",
        "guide",
        "tutorial",
        "introduction",
        "complete",
    }

    for article in articles:
        title = article["title"].lower()
        words = re.findall(r"\b\w+\b", title)
        for word in words:
            if word not in stop_words and len(word) > 3:
                keywords[word] += 1

    print("\nğŸ”¥ çƒ­é—¨ä¸»é¢˜å…³é”®è¯ (å‰ 20 ä¸ª):")
    for i, (keyword, count) in enumerate(
        sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:20], 1
    ):
        print(f"{i:2d}. {keyword:20s} - {count:3d} æ¬¡")

    # æœ€æ–°æ–‡ç« 
    print("\nğŸ“Œ æœ€æ–°æ–‡ç«  (å‰ 10 ç¯‡):")
    for i, article in enumerate(articles[:10], 1):
        print(f"{i:2d}. {article['date']:15s} - {article['title'][:60]}")

    return {
        "total_articles": len(articles),
        "authors": dict(authors),
        "months": dict(months),
        "keywords": dict(
            sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:50]
        ),
    }


def main():
    """ä¸»å‡½æ•°"""
    # è¯»å–å®Œæ•´çš„ Markdown æ–‡ä»¶
    try:
        with open("firecrawl_blog.md", "r", encoding="utf-8") as f:
            content = f.read()
    except:
        print("âŒ æ— æ³•è¯»å– firecrawl_blog.md æ–‡ä»¶")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ Firecrawl MCP å·¥å…·çˆ¬å–åšå®¢å†…å®¹")
        return

    # æå–æ–‡ç« 
    print("ğŸ” æ­£åœ¨æå–æ–‡ç« ...")
    articles = extract_articles(content)

    if not articles:
        print("âš ï¸ æœªæ‰¾åˆ°æ–‡ç« ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨è§£ææ–¹æ³•...")

        # å¤‡ç”¨æ–¹æ³•ï¼šç®€å•æŒ‰è¡Œè§£æ
        lines = content.split("\n")
        articles = []
        current_title = None

        for line in lines:
            # æŸ¥æ‰¾åŒ…å«æ—¥æœŸçš„é“¾æ¥è¡Œ
            if "https://www.firecrawl.dev/blog/" in line and (
                "2025" in line or "2024" in line
            ):
                # æå– URL
                url_match = re.search(r"https://www\.firecrawl\.dev/blog/[^\)]+", line)
                if url_match:
                    url = url_match.group(0)
                    slug = url.split("/")[-1]

                    # æå–æ ‡é¢˜ï¼ˆURL å‰çš„æ–‡æœ¬ï¼‰
                    title_match = re.search(r"\[(.*?)\]", line)
                    title = title_match.group(1) if title_match else "Unknown"

                    # æå–æ—¥æœŸ
                    date_match = re.search(
                        r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d+,\s+\d{4}",
                        line,
                    )
                    date = date_match.group(0) if date_match else "Unknown"

                    articles.append(
                        {
                            "title": title.split("\\\\")[
                                0
                            ].strip(),  # å–ç¬¬ä¸€éƒ¨åˆ†ä½œä¸ºæ ‡é¢˜
                            "description": "",
                            "author": "",
                            "date": date,
                            "url": url,
                            "slug": slug,
                        }
                    )

    print(f"âœ… æˆåŠŸæå– {len(articles)} ç¯‡æ–‡ç« \n")

    # åˆ†ææ–‡ç« 
    stats = analyze_articles(articles)

    # ä¿å­˜ç»“æ„åŒ–æ•°æ®
    output = {
        "scraped_at": datetime.now().isoformat(),
        "source_url": "https://www.firecrawl.dev/blog",
        "total_articles": len(articles),
        "statistics": stats,
        "articles": articles,
    }

    output_file = "firecrawl_blog_articles.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ç»“æ„åŒ–æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

    # ç”Ÿæˆ CSV
    csv_file = "firecrawl_blog_articles.csv"
    with open(csv_file, "w", encoding="utf-8") as f:
        f.write("æ ‡é¢˜,ä½œè€…,æ—¥æœŸ,URL,Slug\n")
        for article in articles:
            title = article["title"].replace(",", ";")  # å¤„ç†é€—å·
            f.write(
                f'"{title}","{article["author"]}","{article["date"]}","{article["url"]}","{article["slug"]}"\n'
            )

    print(f"ğŸ’¾ CSV æ•°æ®å·²ä¿å­˜åˆ°: {csv_file}")

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  1. firecrawl_blog.md - å®Œæ•´ Markdown å†…å®¹")
    print("  2. firecrawl_blog_articles.json - ç»“æ„åŒ– JSON æ•°æ®")
    print("  3. firecrawl_blog_articles.csv - CSV æ ¼å¼æ•°æ®")


if __name__ == "__main__":
    main()
