# 🚀 Firecrawl 云 API 快速配置指南

> **专为 HawaiiHub 团队打造**
> **配置时间**: 10 分钟
> **难度**: ⭐⭐☆☆☆

---

## 📋 核心文档

| 文档 | 用途 | 适合人群 |
|------|------|---------|
| **FIRECRAWL_CLOUD_API_RULES.md** | 完整使用规范（本规范） | 所有开发者 ⭐⭐⭐⭐⭐ |
| FIRECRAWL_README.md | 项目入口 | 新手 |
| FIRECRAWL_QUICK_INDEX.md | 快速索引 | 查找项目 |
| FIRECRAWL_ECOSYSTEM_GUIDE.md | 生态系统 | 深入学习 |

---

## ⚡ 5 分钟快速配置

### 步骤 1: 环境变量配置

创建 `.env` 文件：

```bash
# 复制模板
cp .env.example .env

# 编辑配置
vim .env
```

添加以下内容：

```bash
# ========================================
# Firecrawl 云 API 配置
# ========================================

# 🔑 必需：API 密钥
FIRECRAWL_API_KEY=fc-your-api-key-here

# 🌐 API 端点（通常不需要修改）
FIRECRAWL_API_URL=https://api.firecrawl.dev

# ⏱️ 超时设置（秒）
FIRECRAWL_TIMEOUT=60

# 🔄 重试次数
FIRECRAWL_MAX_RETRIES=3

# 💾 缓存配置
FIRECRAWL_ENABLE_CACHE=true
FIRECRAWL_CACHE_TTL=3600

# 💰 成本限制
FIRECRAWL_DAILY_BUDGET=10.0
FIRECRAWL_MONTHLY_BUDGET=200.0

# 🚨 告警（可选）
FIRECRAWL_ALERT_WEBHOOK=https://hooks.slack.com/your-webhook

# 🔒 备用密钥（可选）
FIRECRAWL_API_KEY_BACKUP_1=fc-backup-key-1
FIRECRAWL_API_KEY_BACKUP_2=fc-backup-key-2
```

### 步骤 2: 安装依赖

```bash
pip install firecrawl-py python-dotenv requests
```

### 步骤 3: 测试连接

创建 `test_firecrawl.py`:

```python
import os
from dotenv import load_dotenv
from firecrawl import FirecrawlApp

# 加载环境变量
load_dotenv()

# 初始化客户端
app = FirecrawlApp(api_key=os.getenv('FIRECRAWL_API_KEY'))

# 测试爬取
result = app.scrape(
    url="https://www.hawaiinewsnow.com/",
    formats=['markdown'],
    onlyMainContent=True
)

print("✅ 连接成功！")
print(f"标题: {result.get('title', 'N/A')}")
print(f"内容长度: {len(result.get('markdown', ''))} 字符")
```

运行测试：

```bash
python test_firecrawl.py
```

---

## 🎯 核心功能速览

### 1. 基础爬取

```python
from firecrawl import FirecrawlApp
import os

app = FirecrawlApp(api_key=os.getenv('FIRECRAWL_API_KEY'))

# 单页爬取
result = app.scrape(
    url="https://example.com",
    formats=['markdown'],
    onlyMainContent=True,      # ✅ 只要主要内容
    maxAge=3600000             # ✅ 使用 1 小时缓存
)

# 批量爬取
results = app.batch_scrape(
    urls=["https://example.com/1", "https://example.com/2"],
    formats=['markdown']
)
```

### 2. 搜索功能

```python
# 搜索夏威夷新闻
results = app.search(
    query="Hawaii news 夏威夷 华人社区",
    sources=[{"type": "web"}],
    limit=10
)
```

### 3. 变更监控

```python
# 监控网站变化
result = app.scrape(
    url="https://example.com",
    formats=['markdown', {'type': 'changeTracking'}]
)
```

---

## 💡 最佳实践（必读）

### ✅ DO - 推荐做法

#### 1. 使用批量接口
```python
# ✅ 好 - 批量处理
results = app.batch_scrape(urls)

# ❌ 差 - 循环调用
for url in urls:
    result = app.scrape(url)
```

#### 2. 启用缓存
```python
# ✅ 好 - 使用缓存
result = app.scrape(url, maxAge=3600000)

# ❌ 差 - 不使用缓存
result = app.scrape(url)
```

#### 3. 过滤内容
```python
# ✅ 好 - 只要主要内容
result = app.scrape(
    url,
    onlyMainContent=True,
    removeTags=['script', 'style']
)

# ❌ 差 - 获取所有内容（浪费）
result = app.scrape(url)
```

#### 4. 错误处理
```python
# ✅ 好 - 完整的错误处理
try:
    result = app.scrape(url)
except Exception as e:
    logger.error(f"爬取失败: {url} - {e}")
    # 降级处理
    result = fallback_scrape(url)

# ❌ 差 - 不处理错误
result = app.scrape(url)
```

#### 5. 成本监控
```python
# ✅ 好 - 追踪成本
@track_cost('scrape')
def scrape_urls(urls):
    return app.batch_scrape(urls)

# 定期检查
print(f"今日成本: ${cost_tracker.get_cost()}")
```

---

### ❌ DON'T - 禁止行为

#### 1. 硬编码密钥
```python
# ❌ 严禁
app = FirecrawlApp(api_key="fc-xxxxxxxx")

# ✅ 正确
app = FirecrawlApp(api_key=os.getenv('FIRECRAWL_API_KEY'))
```

#### 2. 不验证 URL
```python
# ❌ 危险
result = app.scrape(user_input_url)

# ✅ 安全
if URLValidator.validate(user_input_url):
    result = app.scrape(user_input_url)
```

#### 3. 忽略速率限制
```python
# ❌ 会被限流
for url in urls:
    app.scrape(url)

# ✅ 速率控制
rate_limiter.wait_if_needed()
app.scrape(url)
```

#### 4. 不控制成本
```python
# ❌ 可能超支
app.crawl(url, limit=1000)

# ✅ 控制预算
if budget.check_budget(estimated_cost):
    app.crawl(url, limit=100)
```

---

## 📊 成本预算参考

### 定价（实际以官方为准）

| 操作 | 单价 | 说明 |
|------|------|------|
| Scrape | ~$0.005/页 | 单页爬取 |
| Crawl | ~$0.004/页 | 批量爬取（有折扣） |
| Search | ~$0.01/次 | 搜索功能 |
| Extract | ~$0.008/次 | 结构化提取 |

### HawaiiHub 预算建议

| 阶段 | 每日预算 | 每月预算 | 预估用量 |
|------|---------|---------|---------|
| MVP | $10 | $200 | 2K 页/天 |
| 增长期 | $20 | $500 | 5K 页/天 |
| 成熟期 | $50 | $1000 | 12K 页/天 |

### 省钱技巧

1. **充分利用缓存**：相同内容 1 小时内不重复爬取
2. **批量处理**：使用 `batch_scrape` 获得折扣
3. **过滤内容**：只爬取需要的部分
4. **定时任务**：错峰执行（夜间）
5. **去重**：避免重复爬取相同 URL

---

## 🚨 关键配置

### 1. 成本告警

```python
# config/alerts.py
from firecrawl_utils import cost_tracker, alert_system

def check_daily_budget():
    """每小时检查一次预算"""
    cost = cost_tracker.get_cost()

    if cost > 8.0:  # 80% 预算
        alert_system.alert(
            level=AlertLevel.WARNING,
            title="成本告警",
            message=f"今日成本 ${cost:.2f}，接近预算上限"
        )
```

### 2. 自动重试

```python
# config/retry.py
from firecrawl_utils import retry_with_exponential_backoff

@retry_with_exponential_backoff(max_retries=5)
def scrape_with_retry(url):
    return app.scrape(url)
```

### 3. 速率限制

```python
# config/rate_limit.py
from firecrawl_utils import RateLimiter

rate_limiter = RateLimiter(
    max_requests=100,  # 每分钟 100 次
    time_window=60
)

def rate_limited_scrape(url):
    rate_limiter.wait_if_needed()
    return app.scrape(url)
```

---

## 📈 监控仪表板

### 实时指标

```python
# 打印实时指标
from firecrawl_utils import dashboard

dashboard.print_dashboard()
```

输出示例：

```
================================================================================
📊 Firecrawl API 实时监控仪表板
================================================================================

SCRAPE:
  总请求: 150
  成功: 145
  失败: 5
  成功率: 96.7%
  平均耗时: 1.85s

SEARCH:
  总请求: 20
  成功: 20
  失败: 0
  成功率: 100.0%
  平均耗时: 3.12s

每小时请求趋势:
  2025-10-27 08:00: ████████ (80)
  2025-10-27 09:00: ██████ (60)
  2025-10-27 10:00: ██████████ (100)
================================================================================
```

---

## 🛠️ 常用代码片段

### 新闻采集

```python
# scripts/scrape_news.py
from firecrawl_utils import firecrawl_client

def scrape_hawaii_news():
    """爬取夏威夷新闻"""
    news_sites = [
        "https://www.hawaiinewsnow.com/",
        "https://www.staradvertiser.com/",
        "https://www.civilbeat.org/"
    ]

    results = firecrawl_client.batch_scrape(
        urls=news_sites,
        formats=['markdown'],
        onlyMainContent=True
    )

    # 保存结果
    for result in results:
        save_to_database(result)

    # 打印指标
    metrics = firecrawl_client.get_metrics()
    print(f"✅ 爬取完成！成本: ${metrics['cost']['total_cost']}")

    return results

if __name__ == "__main__":
    scrape_hawaii_news()
```

### 定时任务

```bash
# crontab 配置
# 每小时爬取一次新闻
0 * * * * cd /path/to/project && python scripts/scrape_news.py

# 每日成本报告（早上 9 点）
0 9 * * * cd /path/to/project && python scripts/daily_report.py

# 每周清理缓存（周日凌晨 2 点）
0 2 * * 0 cd /path/to/project && python scripts/clear_cache.py
```

---

## 📚 完整代码示例

### HawaiiHub 新闻爬虫

```python
# hawaiihub/news_scraper.py
import os
from typing import List, Dict
from dotenv import load_dotenv
from firecrawl import FirecrawlApp

# 加载环境变量
load_dotenv()

class HawaiiNewsScraper:
    """夏威夷新闻爬虫"""

    # 新闻源配置
    NEWS_SOURCES = {
        'hawaii_news_now': 'https://www.hawaiinewsnow.com/',
        'star_advertiser': 'https://www.staradvertiser.com/',
        'civil_beat': 'https://www.civilbeat.org/',
    }

    def __init__(self):
        self.app = FirecrawlApp(api_key=os.getenv('FIRECRAWL_API_KEY'))

    def scrape_all_news(self) -> List[Dict]:
        """爬取所有新闻源"""
        results = []

        for source_name, url in self.NEWS_SOURCES.items():
            try:
                print(f"🔄 爬取 {source_name}...")

                # 使用 Search 发现新闻
                search_results = self.app.search(
                    query=f"site:{url} Hawaii news",
                    limit=10
                )

                # 批量爬取完整内容
                urls = [r['url'] for r in search_results]
                articles = self.app.batch_scrape(
                    urls=urls,
                    formats=['markdown'],
                    onlyMainContent=True
                )

                # 添加来源标记
                for article in articles:
                    article['source'] = source_name

                results.extend(articles)
                print(f"✅ {source_name}: {len(articles)} 篇文章")

            except Exception as e:
                print(f"❌ {source_name} 失败: {e}")

        return results

    def save_to_database(self, articles: List[Dict]):
        """保存到数据库"""
        # TODO: 实现数据库保存逻辑
        pass

# 使用示例
if __name__ == "__main__":
    scraper = HawaiiNewsScraper()
    articles = scraper.scrape_all_news()

    print(f"\n📊 总计爬取 {len(articles)} 篇文章")

    # 保存到数据库
    scraper.save_to_database(articles)
```

---

## 🔍 故障排查

### 常见问题

#### Q1: `Authentication failed` 错误

**原因**: API 密钥无效或过期

**解决**:
```bash
# 检查环境变量
echo $FIRECRAWL_API_KEY

# 重新设置
export FIRECRAWL_API_KEY=fc-your-new-key

# 验证
python test_firecrawl.py
```

#### Q2: `Rate limit exceeded` 错误

**原因**: 超过速率限制

**解决**:
```python
# 添加速率限制
from firecrawl_utils import rate_limiter

rate_limiter.wait_if_needed()
result = app.scrape(url)
```

#### Q3: `Request timeout` 错误

**原因**: 请求超时

**解决**:
```bash
# 增加超时时间
export FIRECRAWL_TIMEOUT=120
```

#### Q4: 成本过高

**原因**: 未优化请求

**解决**:
1. 启用缓存
2. 使用批量接口
3. 过滤不需要的内容
4. 去重 URL

---

## ✅ 上线检查清单

### 配置检查

- [ ] ✅ `.env` 文件已配置
- [ ] ✅ API 密钥已验证
- [ ] ✅ 备用密钥已设置
- [ ] ✅ 预算限制已配置
- [ ] ✅ 告警 Webhook 已测试

### 功能检查

- [ ] ✅ 基础爬取功能正常
- [ ] ✅ 批量爬取功能正常
- [ ] ✅ 缓存功能启用
- [ ] ✅ 错误处理完整
- [ ] ✅ 日志记录正常

### 监控检查

- [ ] ✅ 成本监控已启用
- [ ] ✅ 性能监控已启用
- [ ] ✅ 告警系统已测试
- [ ] ✅ 仪表板可访问

### 安全检查

- [ ] ✅ API 密钥安全存储
- [ ] ✅ URL 验证已实现
- [ ] ✅ 数据脱敏已配置
- [ ] ✅ `.env` 已加入 `.gitignore`

---

## 📞 获取帮助

### 官方资源

- 📚 [官方文档](https://docs.firecrawl.dev)
- 💬 [Discord 社区](https://discord.gg/firecrawl)
- 📧 [技术支持](mailto:hello@firecrawl.dev)

### 团队资源

- 📖 **完整规范**: `FIRECRAWL_CLOUD_API_RULES.md`
- 🔍 **快速索引**: `FIRECRAWL_QUICK_INDEX.md`
- 📚 **生态系统**: `FIRECRAWL_ECOSYSTEM_GUIDE.md`
- 🚀 **团队规范**: `/Users/zhiledeng/Documents/augment-projects/AGENTS.md`

---

## 🎯 下一步

1. **立即**: 完成环境配置（10 分钟）
2. **今天**: 运行第一个爬虫测试
3. **本周**: 部署新闻采集系统
4. **本月**: 优化成本和性能

---

**🔥 开始使用 Firecrawl 云 API，为 HawaiiHub 提供强大的数据采集能力！🌴**

---

_创建时间: 2025-10-27_
_维护者: HawaiiHub AI Team_
_版本: v1.0.0_
