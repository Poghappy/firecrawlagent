# HawaiiHub å®æˆ˜æ¡ˆä¾‹æ‰‹å†Œ

> **åˆ›å»ºæ—¶é—´**: 2025-10-27
> **é¡¹ç›®**: HawaiiHub - å¤å¨å¤·åäººç¤¾åŒºå¹³å°
> **ç›®æ ‡**: ä½¿ç”¨ Firecrawl æ„å»ºå®Œæ•´çš„æ•°æ®é‡‡é›†å’Œåˆ†æç³»ç»Ÿ
> **éš¾åº¦**: åˆçº§ â†’ é«˜çº§ï¼ˆå¾ªåºæ¸è¿›ï¼‰

---

## ğŸ“– æ‰‹å†Œç®€ä»‹

æœ¬æ‰‹å†Œä»¥ **HawaiiHub** é¡¹ç›®ä¸ºä¾‹ï¼Œé€šè¿‡ **15 ä¸ªå®æˆ˜æ¡ˆä¾‹**å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Firecrawl æ„å»ºç”Ÿäº§çº§æ•°æ®é‡‡é›†ç³»ç»Ÿã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ‰‹å†Œï¼Œä½ å°†å­¦ä¼šï¼š

- âœ… çˆ¬å–å¤å¨å¤·æœ¬åœ°æ–°é—»ç½‘ç«™
- âœ… é‡‡é›†åäººå•†å®¶ä¿¡æ¯ï¼ˆé¤å…ã€è¶…å¸‚ã€æœåŠ¡ï¼‰
- âœ… ç›‘æ§ç§Ÿæˆ¿ä»·æ ¼å˜åŒ–
- âœ… åˆ†æç”¨æˆ·è¯„è®ºå’Œåé¦ˆ
- âœ… æå–æ‹›è˜ä¿¡æ¯
- âœ… æ„å»º AI é©±åŠ¨çš„æ™ºèƒ½æ¨èç³»ç»Ÿ

### ğŸ“Š æ¡ˆä¾‹ç»Ÿè®¡

| ç±»åˆ«               | æ¡ˆä¾‹æ•° | æŠ€æœ¯æ ˆ                    |
| ------------------ | ------ | ------------------------- |
| æ•°æ®é‡‡é›†           | 5      | Scrapeã€Crawlã€Map        |
| æ•°æ®åˆ†æ           | 3      | Extractã€Pandasã€Charts   |
| å®æ—¶ç›‘æ§           | 2      | Change Trackingã€å®šæ—¶ä»»åŠ¡ |
| AI åº”ç”¨            | 3      | LLMã€RAGã€æ¨èç³»ç»Ÿ        |
| å®Œæ•´ç³»ç»Ÿ           | 2      | ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆ            |
| **æ€»è®¡**           | **15** | **Python + Next.js**      |

### ğŸ—‚ï¸ æ¡ˆä¾‹ç»„ç»‡

```
HawaiiHubå®æˆ˜æ¡ˆä¾‹æ‰‹å†Œ.md        â† æœ¬æ–‡æ¡£ï¼ˆæ€»è§ˆï¼‰
â”‚
â”œâ”€â”€ æ¡ˆä¾‹ 01-05: æ•°æ®é‡‡é›†åŸºç¡€    â† çˆ¬å–å„ç±»æ•°æ®
â”œâ”€â”€ æ¡ˆä¾‹ 06-08: æ•°æ®åˆ†æ        â† æå–å’Œåˆ†æ
â”œâ”€â”€ æ¡ˆä¾‹ 09-10: å®æ—¶ç›‘æ§        â† å˜æ›´ç›‘æ§
â”œâ”€â”€ æ¡ˆä¾‹ 11-13: AI åº”ç”¨         â† æ™ºèƒ½æ¨è
â””â”€â”€ æ¡ˆä¾‹ 14-15: å®Œæ•´ç³»ç»Ÿ        â† ç”Ÿäº§çº§ç³»ç»Ÿ
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®æ¡ä»¶

```bash
# 1. Python ç¯å¢ƒ
Python 3.11+

# 2. å®‰è£…ä¾èµ–
pip3 install firecrawl-py python-dotenv requests pandas pydantic

# 3. é…ç½® API å¯†é’¥
cp env.template .env
# ç¼–è¾‘ .envï¼Œæ·»åŠ  FIRECRAWL_API_KEY=fc-xxx

# 4. éªŒè¯ç¯å¢ƒ
python3 -c "from firecrawl import FirecrawlApp; print('âœ… ç¯å¢ƒå°±ç»ª')"
```

### ç›®å½•ç»“æ„

```bash
hawaiihub-firecrawl-cases/
â”œâ”€â”€ case-01-news-scraper/           # æ¡ˆä¾‹ 01
â”œâ”€â”€ case-02-restaurant-collector/   # æ¡ˆä¾‹ 02
â”œâ”€â”€ case-03-housing-monitor/        # æ¡ˆä¾‹ 03
â”œâ”€â”€ ...
â”œâ”€â”€ data/                           # æ•°æ®å­˜å‚¨
â”œâ”€â”€ utils/                          # å·¥å…·å‡½æ•°
â””â”€â”€ README.md
```

---

## ğŸ“š æ¡ˆä¾‹ç›®å½•

### ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®é‡‡é›†åŸºç¡€ï¼ˆæ¡ˆä¾‹ 01-05ï¼‰

#### æ¡ˆä¾‹ 01: å¤å¨å¤·æ–°é—»é‡‡é›†å™¨ â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: è‡ªåŠ¨é‡‡é›†å¤å¨å¤·æœ¬åœ°æ–°é—»ï¼Œä¸ºç¤¾åŒºæä¾›æœ€æ–°èµ„è®¯

**æ¶‰åŠç½‘ç«™**:
- Hawaii News Now: https://www.hawaiinewsnow.com/
- Honolulu Star-Advertiser: https://www.staradvertiser.com/
- Civil Beat: https://www.civilbeat.org/

**æ ¸å¿ƒåŠŸèƒ½**:
- ä½¿ç”¨ Scrape API é‡‡é›†å•ç¯‡æ–°é—»
- ä½¿ç”¨ Crawl API çˆ¬å–å®Œæ•´æ–°é—»åˆ—è¡¨
- æå–æ ‡é¢˜ã€ä½œè€…ã€æ—¶é—´ã€å†…å®¹
- ä¿å­˜ä¸º JSON å’Œ Markdown

**æŠ€æœ¯æ ˆ**: Python + Firecrawl Scrape/Crawl API

**éš¾åº¦**: â­â­â­ï¼ˆåˆçº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 2 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `blog-articles` (examples/blog-articles)
- `hacker_news_scraper` (examples/hacker_news_scraper)

---

#### æ¡ˆä¾‹ 02: åäººå•†å®¶ä¿¡æ¯é‡‡é›† â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: å»ºç«‹å®Œæ•´çš„å¤å¨å¤·åäººå•†å®¶æ•°æ®åº“

**ç›®æ ‡å•†å®¶ç±»å‹**:
- ä¸­é¤å…
- åäººè¶…å¸‚
- ä¸­æ–‡å­¦æ ¡
- ç§»æ°‘æœåŠ¡
- æˆ¿åœ°äº§ä¸­ä»‹

**é‡‡é›†æ¥æº**:
- Yelp: https://www.yelp.com/search?find_desc=Chinese&find_loc=Honolulu
- Google Maps
- æœ¬åœ°åˆ†ç±»ä¿¡æ¯ç½‘ç«™

**æ ¸å¿ƒåŠŸèƒ½**:
- ä½¿ç”¨ Extract API ç»“æ„åŒ–æå–
- æå–ï¼šåç§°ã€åœ°å€ã€ç”µè¯ã€è¥ä¸šæ—¶é—´ã€è¯„åˆ†
- éªŒè¯æ•°æ®å®Œæ•´æ€§
- å»é‡å’Œæ•°æ®æ¸…æ´—

**æŠ€æœ¯æ ˆ**: Python + Firecrawl Extract API + Pydantic

**éš¾åº¦**: â­â­â­â­ï¼ˆä¸­çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 4 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `company-data-scraper` (firecrawl-app-examples)
- `crm_lead_enrichment` (examples/crm_lead_enrichment)
- `gpt-4.1-company-researcher` (examples/gpt-4.1-company-researcher)

---

#### æ¡ˆä¾‹ 03: ç§Ÿæˆ¿ä¿¡æ¯ç›‘æ§ç³»ç»Ÿ â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: å®æ—¶ç›‘æ§å¤å¨å¤·ç§Ÿæˆ¿å¸‚åœºï¼Œæä¾›ä»·æ ¼é¢„è­¦

**é‡‡é›†æ¥æº**:
- Craigslist Honolulu: https://honolulu.craigslist.org/search/apa
- Zillow
- Apartments.com

**æ ¸å¿ƒåŠŸèƒ½**:
- ä½¿ç”¨ Crawl API çˆ¬å–æˆ¿æºåˆ—è¡¨
- æå–ä»·æ ¼ã€é¢ç§¯ã€ä½ç½®ã€è®¾æ–½
- ä½¿ç”¨ Change Tracking ç›‘æ§ä»·æ ¼å˜åŒ–
- ä»·æ ¼ä½äºé¢„æœŸæ—¶å‘é€é€šçŸ¥

**æŠ€æœ¯æ ˆ**: Python + Firecrawl Crawl + Change Tracking + Email

**éš¾åº¦**: â­â­â­â­ï¼ˆä¸­çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 4 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `automated_price_tracking` (firecrawl-app-examples)
- `deep-research-apartment-finder` (examples/deep-research-apartment-finder)
- `o3-mini-deal-finder` (examples/o3-mini-deal-finder)

---

#### æ¡ˆä¾‹ 04: æ‹›è˜ä¿¡æ¯èšåˆ â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: ä¸ºåäººç¤¾åŒºæä¾›æœ¬åœ°æ‹›è˜ä¿¡æ¯

**é‡‡é›†æ¥æº**:
- Indeed Honolulu
- LinkedIn Jobs
- æœ¬åœ°åäººè®ºå›æ‹›è˜æ¿å—

**æ ¸å¿ƒåŠŸèƒ½**:
- ä½¿ç”¨ Search API æœç´¢ç›¸å…³èŒä½
- æå–èŒä½åç§°ã€å…¬å¸ã€è–ªèµ„ã€è¦æ±‚
- æŒ‰ç±»åˆ«åˆ†ç±»ï¼ˆé¤é¥®ã€é›¶å”®ã€æŠ€æœ¯ç­‰ï¼‰
- ç”Ÿæˆæ¯æ—¥æ‹›è˜ç®€æŠ¥

**æŠ€æœ¯æ ˆ**: Python + Firecrawl Search + Extract API

**éš¾åº¦**: â­â­â­ï¼ˆåˆçº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 3 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `ai-resume-job-matching` (firecrawl-app-examples)
- `job-resource-analyzer` (examples/job-resource-analyzer)
- `o1_job_recommender` (examples/o1_job_recommender)

---

#### æ¡ˆä¾‹ 05: æ´»åŠ¨ä¿¡æ¯çˆ¬å– â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: æ±‡æ€»å¤å¨å¤·æœ¬åœ°æ´»åŠ¨ï¼Œæ–¹ä¾¿åäººå‚ä¸

**é‡‡é›†æ¥æº**:
- Eventbrite Honolulu
- Facebook Events
- æœ¬åœ°ç¤¾åŒºç½‘ç«™

**æ ¸å¿ƒåŠŸèƒ½**:
- ä½¿ç”¨ Map API å‘ç°æ‰€æœ‰æ´»åŠ¨é¡µé¢
- æ‰¹é‡çˆ¬å–æ´»åŠ¨è¯¦æƒ…
- æå–æ—¶é—´ã€åœ°ç‚¹ã€ä»·æ ¼ã€ä¸»åŠæ–¹
- æŒ‰ç±»å‹åˆ†ç±»ï¼ˆæ–‡åŒ–ã€è¿åŠ¨ã€éŸ³ä¹ç­‰ï¼‰

**æŠ€æœ¯æ ˆ**: Python + Firecrawl Map + Batch Scrape

**éš¾åº¦**: â­â­â­ï¼ˆåˆçº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 2 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `blog-articles` (examples/blog-articles)

---

### ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®åˆ†æï¼ˆæ¡ˆä¾‹ 06-08ï¼‰

#### æ¡ˆä¾‹ 06: å•†å®¶è¯„è®ºæƒ…æ„Ÿåˆ†æ â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: åˆ†æåäººå•†å®¶çš„ç”¨æˆ·è¯„è®ºï¼Œæå–å…³é”®é—®é¢˜

**åˆ†æç»´åº¦**:
- æƒ…æ„Ÿåˆ†æï¼ˆæ­£é¢ã€è´Ÿé¢ã€ä¸­ç«‹ï¼‰
- å…³é”®è¯æå–ï¼ˆæœåŠ¡ã€ä»·æ ¼ã€ç¯å¢ƒç­‰ï¼‰
- è¯„åˆ†è¶‹åŠ¿åˆ†æ
- æ”¹è¿›å»ºè®®ç”Ÿæˆ

**æ ¸å¿ƒåŠŸèƒ½**:
- çˆ¬å– Yelp è¯„è®º
- ä½¿ç”¨ LLM è¿›è¡Œæƒ…æ„Ÿåˆ†æ
- ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
- æä¾›æ”¹è¿›å»ºè®®

**æŠ€æœ¯æ ˆ**: Python + Firecrawl + Claude/GPT + Pandas

**éš¾åº¦**: â­â­â­â­ï¼ˆä¸­çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 5 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `review-analyzer` (firecrawl-app-examples)
- `claude-3.7-stock-analyzer` (examples/claude-3.7-stock-analyzer)

---

#### æ¡ˆä¾‹ 07: ç§Ÿæˆ¿å¸‚åœºæ•°æ®åˆ†æ â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: åˆ†æç§Ÿæˆ¿å¸‚åœºè¶‹åŠ¿ï¼Œæä¾›å®šä»·å»ºè®®

**åˆ†æå†…å®¹**:
- ä¸åŒåŒºåŸŸä»·æ ¼å¯¹æ¯”
- æˆ¿å‹ä»·æ ¼åˆ†å¸ƒ
- ä»·æ ¼éšæ—¶é—´å˜åŒ–è¶‹åŠ¿
- æ€§ä»·æ¯”åˆ†æ

**æ ¸å¿ƒåŠŸèƒ½**:
- çˆ¬å–å†å²ç§Ÿæˆ¿æ•°æ®
- æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
- ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ–
- ç”Ÿæˆå¸‚åœºæŠ¥å‘Š

**æŠ€æœ¯æ ˆ**: Python + Firecrawl + Pandas + Matplotlib

**éš¾åº¦**: â­â­â­â­ï¼ˆä¸­çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 4 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `scrape_and_analyze_airbnb_data_e2b` (examples/scrape_and_analyze_airbnb_data_e2b)
- `visualize_website_topics_e2b` (examples/visualize_website_topics_e2b)

---

#### æ¡ˆä¾‹ 08: ç«å“åˆ†æç³»ç»Ÿ â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: ç›‘æ§ç«å“åŠ¨æ€ï¼Œç”Ÿæˆåˆ†ææŠ¥å‘Š

**åˆ†æå¯¹è±¡**:
- å…¶ä»–åäººç¤¾åŒºå¹³å°
- æœ¬åœ°åˆ†ç±»ä¿¡æ¯ç½‘ç«™
- ç¤¾äº¤åª’ä½“ç¾¤ç»„

**æ ¸å¿ƒåŠŸèƒ½**:
- å®šæœŸçˆ¬å–ç«å“ç½‘ç«™
- å¯¹æ¯”åŠŸèƒ½å’Œå†…å®¹
- åˆ†æç”¨æˆ·åé¦ˆ
- ç”Ÿæˆç«å“åˆ†ææŠ¥å‘Š

**æŠ€æœ¯æ ˆ**: Python + Firecrawl + LLM

**éš¾åº¦**: â­â­â­â­ï¼ˆä¸­çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 5 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `search-competitor-analysis` (firecrawl-app-examples)
- `gemini-github-analyzer` (examples/gemini-github-analyzer)

---

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®æ—¶ç›‘æ§ï¼ˆæ¡ˆä¾‹ 09-10ï¼‰

#### æ¡ˆä¾‹ 09: ä»·æ ¼å˜åŠ¨ç›‘æ§ â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: å®æ—¶ç›‘æ§å…³é”®å•†å“å’ŒæœåŠ¡ä»·æ ¼

**ç›‘æ§å¯¹è±¡**:
- ç§Ÿæˆ¿ä»·æ ¼
- äºŒæ‰‹è½¦ä»·æ ¼
- å•†å®¶ä¿ƒé”€ä¿¡æ¯

**æ ¸å¿ƒåŠŸèƒ½**:
- ä½¿ç”¨ Change Tracking ç›‘æ§å˜åŒ–
- è®¾ç½®ä»·æ ¼é˜ˆå€¼
- è‡ªåŠ¨å‘é€é€šçŸ¥ï¼ˆEmail/SMS/å¾®ä¿¡ï¼‰
- ä¿å­˜å†å²ä»·æ ¼æ•°æ®

**æŠ€æœ¯æ ˆ**: Python + Firecrawl Change Tracking + é€šçŸ¥æœåŠ¡

**éš¾åº¦**: â­â­â­â­ï¼ˆä¸­çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 3 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `automated_price_tracking` (firecrawl-app-examples)
- `change-detection-tutorial` (firecrawl-app-examples)

---

#### æ¡ˆä¾‹ 10: å†…å®¹æ›´æ–°ç›‘æ§ â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: ç›‘æ§é‡è¦ç½‘ç«™å†…å®¹æ›´æ–°

**ç›‘æ§å¯¹è±¡**:
- æ”¿åºœå…¬å‘Š
- ç­¾è¯æ”¿ç­–
- æœ¬åœ°æ–°é—»
- ç¤¾åŒºæ´»åŠ¨

**æ ¸å¿ƒåŠŸèƒ½**:
- å®šæ—¶æ£€æŸ¥ç½‘ç«™å˜åŒ–
- æå–æ–°å¢å†…å®¹
- ç”Ÿæˆæ›´æ–°æ‘˜è¦
- æ¨é€åˆ°ç¤¾åŒº

**æŠ€æœ¯æ ˆ**: Python + Firecrawl + å®šæ—¶ä»»åŠ¡

**éš¾åº¦**: â­â­â­ï¼ˆåˆçº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 2 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `change-detection-tutorial` (firecrawl-app-examples)

---

### ç¬¬å››éƒ¨åˆ†ï¼šAI åº”ç”¨ï¼ˆæ¡ˆä¾‹ 11-13ï¼‰

#### æ¡ˆä¾‹ 11: æ™ºèƒ½é—®ç­”ç³»ç»Ÿ â­â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: æ„å»º HawaiiHub çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ

**çŸ¥è¯†æ¥æº**:
- ç½‘ç«™æ‰€æœ‰æ–‡ç« 
- ç”¨æˆ·å¸¸è§é—®é¢˜
- æœ¬åœ°æœåŠ¡æŒ‡å—

**æ ¸å¿ƒåŠŸèƒ½**:
- çˆ¬å–æ‰€æœ‰å†…å®¹æ„å»ºçŸ¥è¯†åº“
- ä½¿ç”¨ RAG æŠ€æœ¯å®ç°æ™ºèƒ½é—®ç­”
- æ”¯æŒä¸­è‹±æ–‡é—®ç­”
- æŒç»­æ›´æ–°çŸ¥è¯†åº“

**æŠ€æœ¯æ ˆ**: Python + Firecrawl + LangChain + Vector DB

**éš¾åº¦**: â­â­â­â­â­ï¼ˆé«˜çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 8 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `local-website-chatbot` (firecrawl-app-examples)
- `deepseek-rag` (firecrawl-app-examples)
- `web_data_rag_with_llama3` (examples/web_data_rag_with_llama3)
- `website_qa_with_gemini_caching` (examples/website_qa_with_gemini_caching)

---

#### æ¡ˆä¾‹ 12: ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿ â­â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: ä¸ºç”¨æˆ·æ¨èåˆé€‚çš„å•†å®¶å’ŒæœåŠ¡

**æ¨èå†…å®¹**:
- é¤å…æ¨è
- ç§Ÿæˆ¿æ¨è
- æ´»åŠ¨æ¨è
- æœåŠ¡æ¨è

**æ ¸å¿ƒåŠŸèƒ½**:
- çˆ¬å–ç”¨æˆ·æµè§ˆå†å²
- åˆ†æç”¨æˆ·åå¥½
- ä½¿ç”¨ååŒè¿‡æ»¤æ¨è
- å®æ—¶æ›´æ–°æ¨èç»“æœ

**æŠ€æœ¯æ ˆ**: Python + Firecrawl + ML æ¨èç®—æ³•

**éš¾åº¦**: â­â­â­â­â­ï¼ˆé«˜çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 10 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `ai-resume-job-matching` (firecrawl-app-examples)
- `o1_job_recommender` (examples/o1_job_recommender)

---

#### æ¡ˆä¾‹ 13: AI å†…å®¹ç”Ÿæˆ â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: è‡ªåŠ¨ç”Ÿæˆç¤¾åŒºç®€æŠ¥å’Œå†…å®¹

**ç”Ÿæˆå†…å®¹**:
- æ¯å‘¨æ–°é—»ç®€æŠ¥
- å•†å®¶æ¨èæ–‡ç« 
- æ´»åŠ¨é¢„å‘Š
- ç”Ÿæ´»æŒ‡å—

**æ ¸å¿ƒåŠŸèƒ½**:
- çˆ¬å–ç›¸å…³å†…å®¹
- ä½¿ç”¨ LLM ç”Ÿæˆæ–‡ç« 
- è‡ªåŠ¨é…å›¾
- å®šæ—¶å‘å¸ƒ

**æŠ€æœ¯æ ˆ**: Python + Firecrawl + GPT/Claude + CMS

**éš¾åº¦**: â­â­â­â­ï¼ˆä¸­çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 6 å°æ—¶

**å‚è€ƒé¡¹ç›®**:
- `aginews-ai-newsletter` (examples/aginews-ai-newsletter)
- `deepseek-v3-agi-newsletter` (firecrawl-app-examples)
- `ai-podcast-generator` (examples/ai-podcast-generator)

---

### ç¬¬äº”éƒ¨åˆ†ï¼šå®Œæ•´ç³»ç»Ÿï¼ˆæ¡ˆä¾‹ 14-15ï¼‰

#### æ¡ˆä¾‹ 14: ç«¯åˆ°ç«¯å•†å®¶ç®¡ç†ç³»ç»Ÿ â­â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: å®Œæ•´çš„å•†å®¶ä¿¡æ¯ç®¡ç†ç³»ç»Ÿ

**ç³»ç»ŸåŠŸèƒ½**:
1. **æ•°æ®é‡‡é›†**
   - è‡ªåŠ¨çˆ¬å–å•†å®¶ä¿¡æ¯
   - æ›´æ–°è¥ä¸šæ—¶é—´ã€èœå•ã€ä»·æ ¼

2. **æ•°æ®ç®¡ç†**
   - å•†å®¶ä¿¡æ¯å­˜å‚¨
   - è¯„è®ºç®¡ç†
   - å›¾ç‰‡ç®¡ç†

3. **æ•°æ®å±•ç¤º**
   - å•†å®¶åˆ—è¡¨é¡µé¢
   - è¯¦æƒ…é¡µé¢
   - æœç´¢å’Œç­›é€‰

4. **ç›‘æ§å‘Šè­¦**
   - ä»·æ ¼å˜åŠ¨æé†’
   - æ–°è¯„è®ºé€šçŸ¥
   - æ•°æ®è´¨é‡æ£€æŸ¥

**æŠ€æœ¯æ ˆ**:
- åç«¯: Python + Firecrawl + FastAPI + PostgreSQL
- å‰ç«¯: Next.js + React + Tailwind CSS
- éƒ¨ç½²: Docker + Vercel

**éš¾åº¦**: â­â­â­â­â­ï¼ˆé«˜çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 2-3 å¤©

**å‚è€ƒé¡¹ç›®**:
- `company-data-scraper` (firecrawl-app-examples)
- `full_example_apps` (examples/full_example_apps)

---

#### æ¡ˆä¾‹ 15: HawaiiHub å®Œæ•´æ•°æ®å¹³å° â­â­â­â­â­

**ä¸šåŠ¡éœ€æ±‚**: HawaiiHub ç”Ÿäº§çº§æ•°æ®é‡‡é›†å’Œåˆ†æå¹³å°

**å¹³å°æ¶æ„**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           HawaiiHub æ•°æ®å¹³å°                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ æ•°æ®é‡‡é›†å±‚   â”‚  â”‚ æ•°æ®å­˜å‚¨å±‚   â”‚        â”‚
â”‚  â”‚              â”‚  â”‚              â”‚        â”‚
â”‚  â”‚ - Firecrawl  â”‚â”€â”€â”‚ - PostgreSQL â”‚        â”‚
â”‚  â”‚ - å®šæ—¶ä»»åŠ¡   â”‚  â”‚ - Redis      â”‚        â”‚
â”‚  â”‚ - ç›‘æ§å‘Šè­¦   â”‚  â”‚ - S3/OSS     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                  â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ æ•°æ®å¤„ç†å±‚   â”‚  â”‚ åº”ç”¨æœåŠ¡å±‚   â”‚        â”‚
â”‚  â”‚              â”‚  â”‚              â”‚        â”‚
â”‚  â”‚ - ETL        â”‚â”€â”€â”‚ - API        â”‚        â”‚
â”‚  â”‚ - æ¸…æ´—å»é‡   â”‚  â”‚ - æœç´¢       â”‚        â”‚
â”‚  â”‚ - AI åˆ†æ    â”‚  â”‚ - æ¨è       â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                  â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ å‰ç«¯å±•ç¤ºå±‚   â”‚  â”‚ è¿è¥ç®¡ç†å±‚   â”‚        â”‚
â”‚  â”‚              â”‚  â”‚              â”‚        â”‚
â”‚  â”‚ - Next.js    â”‚  â”‚ - Admin      â”‚        â”‚
â”‚  â”‚ - Mobile App â”‚  â”‚ - Dashboard  â”‚        â”‚
â”‚  â”‚ - å°ç¨‹åº     â”‚  â”‚ - Analytics  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒæ¨¡å—**:

1. **æ•°æ®é‡‡é›†æ¨¡å—**
   - æ–°é—»é‡‡é›†
   - å•†å®¶é‡‡é›†
   - ç§Ÿæˆ¿é‡‡é›†
   - æ‹›è˜é‡‡é›†
   - æ´»åŠ¨é‡‡é›†

2. **æ•°æ®å¤„ç†æ¨¡å—**
   - æ•°æ®æ¸…æ´—
   - å»é‡éªŒè¯
   - æ ¼å¼æ ‡å‡†åŒ–
   - AI å†…å®¹åˆ†æ

3. **åº”ç”¨æœåŠ¡æ¨¡å—**
   - RESTful API
   - GraphQL API
   - WebSocket å®æ—¶æ¨é€
   - æœç´¢æœåŠ¡

4. **æ™ºèƒ½åŠŸèƒ½æ¨¡å—**
   - æ™ºèƒ½æ¨è
   - æ™ºèƒ½é—®ç­”
   - å†…å®¹ç”Ÿæˆ
   - è¶‹åŠ¿åˆ†æ

5. **è¿è¥ç®¡ç†æ¨¡å—**
   - æ•°æ®è´¨é‡ç›‘æ§
   - çˆ¬å–ä»»åŠ¡ç®¡ç†
   - ç”¨æˆ·è¡Œä¸ºåˆ†æ
   - æˆæœ¬æ§åˆ¶

**æŠ€æœ¯æ ˆ**:
- **åç«¯**: Python + FastAPI + Celery + Redis
- **å‰ç«¯**: Next.js 14 + TypeScript + Tailwind CSS
- **æ•°æ®åº“**: PostgreSQL + Redis + Elasticsearch
- **çˆ¬è™«**: Firecrawl Cloud API
- **AI**: LangChain + OpenAI/Claude
- **éƒ¨ç½²**: Docker + Kubernetes + Vercel
- **ç›‘æ§**: Sentry + Grafana + Prometheus

**éš¾åº¦**: â­â­â­â­â­ï¼ˆä¸“å®¶çº§ï¼‰

**å­¦ä¹ æ—¶é—´**: 1-2 å‘¨

**å‚è€ƒé¡¹ç›®**:
- æ‰€æœ‰å‰é¢çš„æ¡ˆä¾‹
- `full_example_apps` (examples/full_example_apps)
- `kubernetes` (examples/kubernetes)

---

## ğŸ“– æ¡ˆä¾‹è¯¦è§£

### æ¡ˆä¾‹ 01 è¯¦è§£ï¼šå¤å¨å¤·æ–°é—»é‡‡é›†å™¨

#### æ­¥éª¤ 1: åˆ†æç›®æ ‡ç½‘ç«™

```python
# 1.1 åˆ†æç½‘ç«™ç»“æ„
# Hawaii News Now: https://www.hawaiinewsnow.com/

# é¦–é¡µç»“æ„:
# - å¤´æ¡æ–°é—»ï¼šclass="lead-story"
# - æ–°é—»åˆ—è¡¨ï¼šclass="article-list"
# - æ¯ç¯‡æ–°é—»ï¼š<article>æ ‡ç­¾

# æ–°é—»è¯¦æƒ…é¡µç»“æ„:
# - æ ‡é¢˜ï¼š<h1 class="headline">
# - ä½œè€…ï¼š<div class="byline">
# - æ—¶é—´ï¼š<time datetime="">
# - å†…å®¹ï¼š<div class="article-body">
```

#### æ­¥éª¤ 2: çˆ¬å–å•ç¯‡æ–°é—»

```python
from firecrawl import FirecrawlApp
import os
from datetime import datetime
import json

# åˆå§‹åŒ–å®¢æˆ·ç«¯
app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))

def scrape_single_news(url: str) -> dict:
    """
    çˆ¬å–å•ç¯‡æ–°é—»

    Args:
        url: æ–°é—» URL

    Returns:
        æ–°é—»æ•°æ®å­—å…¸
    """
    try:
        # ä½¿ç”¨ Scrape API
        result = app.scrape(
            url=url,
            formats=["markdown", "html"],
            only_main_content=True,  # åªæå–ä¸»è¦å†…å®¹
            remove_base64_images=True,  # ç§»é™¤ base64 å›¾ç‰‡
        )

        # æå–æ•°æ®
        news_data = {
            "url": url,
            "title": extract_title(result.markdown),
            "author": extract_author(result.markdown),
            "published_date": extract_date(result.markdown),
            "content": result.markdown,
            "html": result.html,
            "scraped_at": datetime.now().isoformat(),
        }

        return news_data

    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {url} - {e}")
        return None

def extract_title(markdown: str) -> str:
    """ä» Markdown ä¸­æå–æ ‡é¢˜"""
    lines = markdown.split("\n")
    for line in lines:
        if line.startswith("# "):
            return line.replace("# ", "").strip()
    return "æœªçŸ¥æ ‡é¢˜"

def extract_author(markdown: str) -> str:
    """ä» Markdown ä¸­æå–ä½œè€…"""
    # å®ç°æå–é€»è¾‘
    # é€šå¸¸åœ¨ "By " æˆ– "ä½œè€…ï¼š" ä¹‹å
    import re
    match = re.search(r"By\s+([A-Za-z\s]+)", markdown)
    if match:
        return match.group(1).strip()
    return "æœªçŸ¥ä½œè€…"

def extract_date(markdown: str) -> str:
    """ä» Markdown ä¸­æå–å‘å¸ƒæ—¶é—´"""
    # å®ç°æå–é€»è¾‘
    import re
    # æŸ¥æ‰¾æ—¥æœŸæ¨¡å¼
    match = re.search(r"(\d{4}-\d{2}-\d{2}|\w+ \d{1,2}, \d{4})", markdown)
    if match:
        return match.group(1)
    return datetime.now().strftime("%Y-%m-%d")

# æµ‹è¯•
if __name__ == "__main__":
    test_url = "https://www.hawaiinewsnow.com/2025/01/27/some-news/"
    news = scrape_single_news(test_url)

    if news:
        print(f"âœ… æˆåŠŸçˆ¬å–: {news['title']}")
        print(f"ä½œè€…: {news['author']}")
        print(f"æ—¶é—´: {news['published_date']}")

        # ä¿å­˜ä¸º JSON
        with open(f"news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", "w", encoding="utf-8") as f:
            json.dump(news, f, ensure_ascii=False, indent=2)
```

#### æ­¥éª¤ 3: çˆ¬å–æ–°é—»åˆ—è¡¨

```python
def scrape_news_list(homepage_url: str, limit: int = 20) -> list:
    """
    çˆ¬å–æ–°é—»åˆ—è¡¨

    Args:
        homepage_url: é¦–é¡µ URL
        limit: çˆ¬å–æ•°é‡é™åˆ¶

    Returns:
        æ–°é—»åˆ—è¡¨
    """
    try:
        # ä½¿ç”¨ Crawl API
        result = app.crawl(
            url=homepage_url,
            limit=limit,
            scrape_options={
                "formats": ["markdown"],
                "only_main_content": True,
            },
            include_paths=["/2025/", "/news/"],  # åªçˆ¬å–æ–°é—»è·¯å¾„
            exclude_paths=["/weather/", "/sports/"],  # æ’é™¤ä¸éœ€è¦çš„è·¯å¾„
        )

        news_list = []
        for item in result:
            if isinstance(item, tuple):
                success, data = item
                if success:
                    news_list.append({
                        "url": data.url,
                        "title": extract_title(data.markdown),
                        "author": extract_author(data.markdown),
                        "published_date": extract_date(data.markdown),
                        "content": data.markdown,
                    })
            else:
                news_list.append({
                    "url": item.url,
                    "title": extract_title(item.markdown),
                    "author": extract_author(item.markdown),
                    "published_date": extract_date(item.markdown),
                    "content": item.markdown,
                })

        return news_list

    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        return []

# æµ‹è¯•
if __name__ == "__main__":
    homepage = "https://www.hawaiinewsnow.com/"
    news_list = scrape_news_list(homepage, limit=10)

    print(f"âœ… æˆåŠŸçˆ¬å– {len(news_list)} ç¯‡æ–°é—»")

    # ä¿å­˜ä¸º JSON
    with open(f"news_list_{datetime.now().strftime('%Y%m%d')}.json", "w", encoding="utf-8") as f:
        json.dump(news_list, f, ensure_ascii=False, indent=2)
```

#### æ­¥éª¤ 4: ä½¿ç”¨ Extract API ç»“æ„åŒ–æå–

```python
from pydantic import BaseModel, Field
from typing import List, Optional

class NewsArticle(BaseModel):
    """æ–°é—»æ–‡ç« æ•°æ®æ¨¡å‹"""
    title: str = Field(..., description="æ–°é—»æ ‡é¢˜")
    author: str = Field(..., description="ä½œè€…å§“å")
    published_date: str = Field(..., description="å‘å¸ƒæ—¥æœŸ")
    category: str = Field(..., description="æ–°é—»ç±»åˆ«")
    summary: str = Field(..., description="æ–°é—»æ‘˜è¦")
    content: str = Field(..., description="æ–°é—»æ­£æ–‡")
    tags: List[str] = Field(default_factory=list, description="æ ‡ç­¾åˆ—è¡¨")

def scrape_with_extract(url: str) -> Optional[NewsArticle]:
    """
    ä½¿ç”¨ Extract API ç»“æ„åŒ–æå–æ–°é—»

    Args:
        url: æ–°é—» URL

    Returns:
        ç»“æ„åŒ–æ–°é—»æ•°æ®
    """
    try:
        result = app.scrape(
            url=url,
            formats=[{
                "type": "json",
                "schema": NewsArticle.model_json_schema(),
            }],
        )

        # å°† JSON è½¬æ¢ä¸º Pydantic æ¨¡å‹
        news = NewsArticle(**result.json)
        return news

    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {url} - {e}")
        return None

# æµ‹è¯•
if __name__ == "__main__":
    test_url = "https://www.hawaiinewsnow.com/2025/01/27/some-news/"
    news = scrape_with_extract(test_url)

    if news:
        print(f"âœ… æˆåŠŸæå–:")
        print(f"æ ‡é¢˜: {news.title}")
        print(f"ä½œè€…: {news.author}")
        print(f"ç±»åˆ«: {news.category}")
        print(f"æ‘˜è¦: {news.summary}")
        print(f"æ ‡ç­¾: {', '.join(news.tags)}")
```

#### æ­¥éª¤ 5: å®šæ—¶ä»»åŠ¡

```python
import schedule
import time

def daily_news_scraper():
    """
    æ¯æ—¥æ–°é—»çˆ¬å–ä»»åŠ¡
    """
    print(f"ğŸ• {datetime.now()}: å¼€å§‹çˆ¬å–æ¯æ—¥æ–°é—»...")

    # çˆ¬å–å¤šä¸ªæ–°é—»æº
    sources = [
        "https://www.hawaiinewsnow.com/",
        "https://www.staradvertiser.com/",
        "https://www.civilbeat.org/",
    ]

    all_news = []
    for source in sources:
        print(f"ğŸ“° çˆ¬å–: {source}")
        news_list = scrape_news_list(source, limit=10)
        all_news.extend(news_list)

    # ä¿å­˜æ•°æ®
    filename = f"daily_news_{datetime.now().strftime('%Y%m%d')}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(all_news, f, ensure_ascii=False, indent=2)

    print(f"âœ… å®Œæˆï¼å…±çˆ¬å– {len(all_news)} ç¯‡æ–°é—»ï¼Œä¿å­˜åˆ° {filename}")

# è®¾ç½®å®šæ—¶ä»»åŠ¡
schedule.every().day.at("08:00").do(daily_news_scraper)  # æ¯å¤©æ—©ä¸Š 8 ç‚¹æ‰§è¡Œ

print("ğŸš€ æ–°é—»çˆ¬å–å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨...")
print("æ¯å¤© 08:00 è‡ªåŠ¨çˆ¬å–å¤å¨å¤·æ–°é—»")

# è¿è¡Œå®šæ—¶ä»»åŠ¡
while True:
    schedule.run_pending()
    time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
```

---

## ğŸ“ å­¦ä¹ å»ºè®®

### å¾ªåºæ¸è¿›å­¦ä¹ è·¯å¾„

**ç¬¬ 1 å‘¨ï¼šåŸºç¡€æ¡ˆä¾‹**
- Day 1-2: æ¡ˆä¾‹ 01ï¼ˆæ–°é—»é‡‡é›†ï¼‰
- Day 3-4: æ¡ˆä¾‹ 04ï¼ˆæ‹›è˜ä¿¡æ¯ï¼‰
- Day 5-6: æ¡ˆä¾‹ 05ï¼ˆæ´»åŠ¨ä¿¡æ¯ï¼‰
- Day 7: æ€»ç»“å’Œå¤ä¹ 

**ç¬¬ 2 å‘¨ï¼šè¿›é˜¶æ¡ˆä¾‹**
- Day 1-2: æ¡ˆä¾‹ 02ï¼ˆå•†å®¶é‡‡é›†ï¼‰
- Day 3-4: æ¡ˆä¾‹ 03ï¼ˆç§Ÿæˆ¿ç›‘æ§ï¼‰
- Day 5-6: æ¡ˆä¾‹ 06ï¼ˆè¯„è®ºåˆ†æï¼‰
- Day 7: å®æˆ˜é¡¹ç›®

**ç¬¬ 3 å‘¨ï¼šé«˜çº§æ¡ˆä¾‹**
- Day 1-2: æ¡ˆä¾‹ 11ï¼ˆæ™ºèƒ½é—®ç­”ï¼‰
- Day 3-4: æ¡ˆä¾‹ 13ï¼ˆå†…å®¹ç”Ÿæˆï¼‰
- Day 5-7: æ¡ˆä¾‹ 14ï¼ˆå®Œæ•´ç³»ç»Ÿï¼‰

**ç¬¬ 4 å‘¨ï¼šç”Ÿäº§éƒ¨ç½²**
- Day 1-5: æ¡ˆä¾‹ 15ï¼ˆæ•°æ®å¹³å°ï¼‰
- Day 6-7: ä¼˜åŒ–å’Œéƒ¨ç½²

### å®æˆ˜æŠ€å·§

1. **ä»ç®€å•å¼€å§‹** - å…ˆæŒæ¡ Scrape APIï¼Œå†å­¦ä¹  Crawl
2. **å¤šç»ƒä¹ ** - æ¯ä¸ªæ¡ˆä¾‹è‡³å°‘è¿è¡Œ 3 æ¬¡
3. **ä¿®æ”¹å‚æ•°** - å°è¯•ä¸åŒçš„é…ç½®é€‰é¡¹
4. **é”™è¯¯å¤„ç†** - æ·»åŠ å®Œæ•´çš„å¼‚å¸¸å¤„ç†
5. **æ•°æ®éªŒè¯** - ä½¿ç”¨ Pydantic éªŒè¯æ•°æ®
6. **æ€§èƒ½ä¼˜åŒ–** - ä½¿ç”¨ç¼“å­˜å’Œæ‰¹é‡å¤„ç†
7. **æˆæœ¬æ§åˆ¶** - ç›‘æ§ API ä½¿ç”¨é‡

---

## ğŸ“Š é¡¹ç›®å‚è€ƒ

### Firecrawl å®˜æ–¹ç¤ºä¾‹ï¼ˆ56 ä¸ªï¼‰

ä½ç½®: `/Users/zhiledeng/Downloads/FireShot/Firecrawlå­¦ä¹ æ‰‹å†Œ/05-å®æˆ˜æ¡ˆä¾‹/firecrawl-main-repo/examples/`

**æ¨èå­¦ä¹ é¡ºåº**:

1. **çˆ¬è™«åŸºç¡€** (å­¦ä¹  Crawl API)
   - `gpt-4.1-web-crawler`
   - `claude3.7-web-crawler`
   - `gemini-2.5-crawler`
   - `deepseek-v3-crawler`

2. **æ•°æ®æå–** (å­¦ä¹  Extract API)
   - `simple_web_data_extraction_with_claude`
   - `web_data_extraction`
   - `attributes-extraction-python-sdk.py`

3. **å…¬å¸ç ”ç©¶** (å•†å®¶é‡‡é›†å‚è€ƒ)
   - `gpt-4.1-company-researcher`
   - `deepseek-v3-company-researcher`
   - `R1_company_researcher`

4. **å®æˆ˜é¡¹ç›®**
   - `hacker_news_scraper`
   - `blog-articles`
   - `scrape_and_analyze_airbnb_data_e2b`
   - `deep-research-apartment-finder`

5. **AI é›†æˆ**
   - `web_data_rag_with_llama3`
   - `website_qa_with_gemini_caching`
   - `openai_swarm_firecrawl`

### Firecrawl App Examplesï¼ˆ40 ä¸ªï¼‰

ä½ç½®: `/Users/zhiledeng/Downloads/FireShot/Firecrawlå­¦ä¹ æ‰‹å†Œ/05-å®æˆ˜æ¡ˆä¾‹/ç¤ºä¾‹åº”ç”¨/firecrawl-app-examples/`

**HawaiiHub æ¨è**:

1. `company-data-scraper` - å•†å®¶æ•°æ®é‡‡é›†
2. `automated_price_tracking` - ä»·æ ¼ç›‘æ§
3. `review-analyzer` - è¯„è®ºåˆ†æ
4. `local-website-chatbot` - æ™ºèƒ½é—®ç­”
5. `ai-resume-job-matching` - æ‹›è˜åŒ¹é…

---

## ğŸ”— ç›¸å…³èµ„æº

### æ–‡æ¡£

- å®Œæ•´å­¦ä¹ æ‰‹å†Œ: `01-åŸºç¡€å…¥é—¨/Firecrawlå®Œæ•´å­¦ä¹ æ‰‹å†Œ.md`
- API è§„èŒƒ: `03-APIå‚è€ƒ/äº‘ç«¯APIè§„èŒƒ.md`
- é…ç½®æŒ‡å—: `04-é…ç½®æŒ‡å—/äº‘ç«¯é…ç½®æŒ‡å—.md`
- é¡¹ç›®ç´¢å¼•: `05-å®æˆ˜æ¡ˆä¾‹/ç¤ºä¾‹é¡¹ç›®å®Œæ•´ç´¢å¼•.md`

### å®˜æ–¹èµ„æº

- å®˜æ–¹æ–‡æ¡£: https://docs.firecrawl.dev/
- GitHub ä¸»ä»“åº“: https://github.com/firecrawl/firecrawl
- ç¤ºä¾‹ä»“åº“: https://github.com/firecrawl/firecrawl-app-examples
- Discord ç¤¾åŒº: https://discord.gg/firecrawl

---

## ğŸ‰ ä¸‹ä¸€æ­¥

å‡†å¤‡å¥½å¼€å§‹äº†å—ï¼Ÿ

**æ¨èèµ·ç‚¹**:

```bash
# 1. åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p hawaiihub-firecrawl-cases
cd hawaiihub-firecrawl-cases

# 2. åˆ›å»ºç¬¬ä¸€ä¸ªæ¡ˆä¾‹
mkdir case-01-news-scraper
cd case-01-news-scraper

# 3. å¤åˆ¶ä¸Šé¢çš„ä»£ç ï¼Œå¼€å§‹å®è·µï¼
```

**å­¦ä¹ é¡ºåº**:

1. âœ… ä»æ¡ˆä¾‹ 01 å¼€å§‹
2. âœ… æ¯å®Œæˆä¸€ä¸ªæ¡ˆä¾‹ï¼Œè®°å½•ç¬”è®°
3. âœ… é‡åˆ°é—®é¢˜æŸ¥é˜…æ–‡æ¡£
4. âœ… å‚è€ƒå®˜æ–¹ç¤ºä¾‹é¡¹ç›®
5. âœ… æœ€ç»ˆæ„å»ºå®Œæ•´ç³»ç»Ÿ

---

**ç¥å­¦ä¹ æ„‰å¿«ï¼Œé¡¹ç›®æˆåŠŸï¼** ğŸš€ğŸŒº

---

**åˆ›å»ºæ—¶é—´**: 2025-10-27
**ç‰ˆæœ¬**: v1.0
**ç»´æŠ¤è€…**: HawaiiHub AI Team
**é€‚ç”¨é¡¹ç›®**: HawaiiHub - å¤å¨å¤·åäººç¤¾åŒºå¹³å°
